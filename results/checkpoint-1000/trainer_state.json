{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.4184397163120568,
  "eval_steps": 500,
  "global_step": 1000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0014184397163120568,
      "grad_norm": 4.663888931274414,
      "learning_rate": 0.0,
      "loss": 3.363,
      "step": 1
    },
    {
      "epoch": 0.0028368794326241137,
      "grad_norm": 4.326501846313477,
      "learning_rate": 4.000000000000001e-06,
      "loss": 3.1448,
      "step": 2
    },
    {
      "epoch": 0.00425531914893617,
      "grad_norm": 5.150259494781494,
      "learning_rate": 8.000000000000001e-06,
      "loss": 3.1434,
      "step": 3
    },
    {
      "epoch": 0.005673758865248227,
      "grad_norm": 5.390714645385742,
      "learning_rate": 1.2e-05,
      "loss": 3.3289,
      "step": 4
    },
    {
      "epoch": 0.0070921985815602835,
      "grad_norm": 4.865074634552002,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 3.0366,
      "step": 5
    },
    {
      "epoch": 0.00851063829787234,
      "grad_norm": 4.287224769592285,
      "learning_rate": 2e-05,
      "loss": 3.1406,
      "step": 6
    },
    {
      "epoch": 0.009929078014184398,
      "grad_norm": 4.218326568603516,
      "learning_rate": 2.4e-05,
      "loss": 3.1917,
      "step": 7
    },
    {
      "epoch": 0.011347517730496455,
      "grad_norm": 3.5314183235168457,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 2.882,
      "step": 8
    },
    {
      "epoch": 0.01276595744680851,
      "grad_norm": 3.696972370147705,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 2.8056,
      "step": 9
    },
    {
      "epoch": 0.014184397163120567,
      "grad_norm": 3.214474678039551,
      "learning_rate": 3.6e-05,
      "loss": 3.2135,
      "step": 10
    },
    {
      "epoch": 0.015602836879432624,
      "grad_norm": 3.480194330215454,
      "learning_rate": 4e-05,
      "loss": 2.6706,
      "step": 11
    },
    {
      "epoch": 0.01702127659574468,
      "grad_norm": 3.2293241024017334,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 2.577,
      "step": 12
    },
    {
      "epoch": 0.018439716312056736,
      "grad_norm": 3.4259519577026367,
      "learning_rate": 4.8e-05,
      "loss": 2.8119,
      "step": 13
    },
    {
      "epoch": 0.019858156028368795,
      "grad_norm": 2.966158151626587,
      "learning_rate": 5.2000000000000004e-05,
      "loss": 2.6788,
      "step": 14
    },
    {
      "epoch": 0.02127659574468085,
      "grad_norm": 3.072213888168335,
      "learning_rate": 5.6000000000000006e-05,
      "loss": 2.3474,
      "step": 15
    },
    {
      "epoch": 0.02269503546099291,
      "grad_norm": 3.135129690170288,
      "learning_rate": 6e-05,
      "loss": 2.2081,
      "step": 16
    },
    {
      "epoch": 0.024113475177304965,
      "grad_norm": 2.8877358436584473,
      "learning_rate": 6.400000000000001e-05,
      "loss": 2.486,
      "step": 17
    },
    {
      "epoch": 0.02553191489361702,
      "grad_norm": 1.3276922702789307,
      "learning_rate": 6.800000000000001e-05,
      "loss": 2.277,
      "step": 18
    },
    {
      "epoch": 0.02695035460992908,
      "grad_norm": 1.734344482421875,
      "learning_rate": 7.2e-05,
      "loss": 2.3077,
      "step": 19
    },
    {
      "epoch": 0.028368794326241134,
      "grad_norm": 1.927562952041626,
      "learning_rate": 7.6e-05,
      "loss": 2.2205,
      "step": 20
    },
    {
      "epoch": 0.029787234042553193,
      "grad_norm": 1.658927321434021,
      "learning_rate": 8e-05,
      "loss": 2.2476,
      "step": 21
    },
    {
      "epoch": 0.031205673758865248,
      "grad_norm": 1.7990249395370483,
      "learning_rate": 8.4e-05,
      "loss": 2.4956,
      "step": 22
    },
    {
      "epoch": 0.032624113475177303,
      "grad_norm": 1.2179522514343262,
      "learning_rate": 8.800000000000001e-05,
      "loss": 2.3486,
      "step": 23
    },
    {
      "epoch": 0.03404255319148936,
      "grad_norm": 1.2065980434417725,
      "learning_rate": 9.200000000000001e-05,
      "loss": 2.1772,
      "step": 24
    },
    {
      "epoch": 0.03546099290780142,
      "grad_norm": 1.3824200630187988,
      "learning_rate": 9.6e-05,
      "loss": 2.215,
      "step": 25
    },
    {
      "epoch": 0.03687943262411347,
      "grad_norm": 1.2284038066864014,
      "learning_rate": 0.0001,
      "loss": 2.1544,
      "step": 26
    },
    {
      "epoch": 0.03829787234042553,
      "grad_norm": 1.2294367551803589,
      "learning_rate": 0.00010400000000000001,
      "loss": 2.1664,
      "step": 27
    },
    {
      "epoch": 0.03971631205673759,
      "grad_norm": 1.2366228103637695,
      "learning_rate": 0.00010800000000000001,
      "loss": 2.06,
      "step": 28
    },
    {
      "epoch": 0.04113475177304964,
      "grad_norm": 1.154578685760498,
      "learning_rate": 0.00011200000000000001,
      "loss": 2.1996,
      "step": 29
    },
    {
      "epoch": 0.0425531914893617,
      "grad_norm": 1.2831920385360718,
      "learning_rate": 0.000116,
      "loss": 1.9245,
      "step": 30
    },
    {
      "epoch": 0.04397163120567376,
      "grad_norm": 1.2155778408050537,
      "learning_rate": 0.00012,
      "loss": 2.0609,
      "step": 31
    },
    {
      "epoch": 0.04539007092198582,
      "grad_norm": 1.2330687046051025,
      "learning_rate": 0.000124,
      "loss": 2.1224,
      "step": 32
    },
    {
      "epoch": 0.04680851063829787,
      "grad_norm": 1.2099401950836182,
      "learning_rate": 0.00012800000000000002,
      "loss": 2.122,
      "step": 33
    },
    {
      "epoch": 0.04822695035460993,
      "grad_norm": 1.0961740016937256,
      "learning_rate": 0.000132,
      "loss": 2.0189,
      "step": 34
    },
    {
      "epoch": 0.04964539007092199,
      "grad_norm": 1.2576420307159424,
      "learning_rate": 0.00013600000000000003,
      "loss": 2.0353,
      "step": 35
    },
    {
      "epoch": 0.05106382978723404,
      "grad_norm": 1.195496678352356,
      "learning_rate": 0.00014,
      "loss": 2.0332,
      "step": 36
    },
    {
      "epoch": 0.0524822695035461,
      "grad_norm": 1.218315839767456,
      "learning_rate": 0.000144,
      "loss": 2.0501,
      "step": 37
    },
    {
      "epoch": 0.05390070921985816,
      "grad_norm": 1.1647756099700928,
      "learning_rate": 0.000148,
      "loss": 2.2139,
      "step": 38
    },
    {
      "epoch": 0.05531914893617021,
      "grad_norm": 1.20754873752594,
      "learning_rate": 0.000152,
      "loss": 2.2591,
      "step": 39
    },
    {
      "epoch": 0.05673758865248227,
      "grad_norm": 1.0971544981002808,
      "learning_rate": 0.00015600000000000002,
      "loss": 2.273,
      "step": 40
    },
    {
      "epoch": 0.05815602836879433,
      "grad_norm": 1.2460731267929077,
      "learning_rate": 0.00016,
      "loss": 2.2102,
      "step": 41
    },
    {
      "epoch": 0.059574468085106386,
      "grad_norm": 1.1251200437545776,
      "learning_rate": 0.000164,
      "loss": 2.1836,
      "step": 42
    },
    {
      "epoch": 0.06099290780141844,
      "grad_norm": 1.149972677230835,
      "learning_rate": 0.000168,
      "loss": 1.8653,
      "step": 43
    },
    {
      "epoch": 0.062411347517730496,
      "grad_norm": 1.097387671470642,
      "learning_rate": 0.000172,
      "loss": 2.2198,
      "step": 44
    },
    {
      "epoch": 0.06382978723404255,
      "grad_norm": 1.1107244491577148,
      "learning_rate": 0.00017600000000000002,
      "loss": 2.1135,
      "step": 45
    },
    {
      "epoch": 0.06524822695035461,
      "grad_norm": 1.0508993864059448,
      "learning_rate": 0.00018,
      "loss": 1.9834,
      "step": 46
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 1.1061515808105469,
      "learning_rate": 0.00018400000000000003,
      "loss": 2.0214,
      "step": 47
    },
    {
      "epoch": 0.06808510638297872,
      "grad_norm": 1.055652141571045,
      "learning_rate": 0.000188,
      "loss": 2.0456,
      "step": 48
    },
    {
      "epoch": 0.06950354609929078,
      "grad_norm": 1.0564075708389282,
      "learning_rate": 0.000192,
      "loss": 1.8642,
      "step": 49
    },
    {
      "epoch": 0.07092198581560284,
      "grad_norm": 1.03203547000885,
      "learning_rate": 0.000196,
      "loss": 1.714,
      "step": 50
    },
    {
      "epoch": 0.07234042553191489,
      "grad_norm": 1.1104342937469482,
      "learning_rate": 0.0002,
      "loss": 2.0922,
      "step": 51
    },
    {
      "epoch": 0.07375886524822695,
      "grad_norm": 1.0160233974456787,
      "learning_rate": 0.0001999031476997579,
      "loss": 1.9706,
      "step": 52
    },
    {
      "epoch": 0.075177304964539,
      "grad_norm": 0.9867855310440063,
      "learning_rate": 0.00019980629539951574,
      "loss": 1.9446,
      "step": 53
    },
    {
      "epoch": 0.07659574468085106,
      "grad_norm": 1.0777437686920166,
      "learning_rate": 0.00019970944309927362,
      "loss": 1.8683,
      "step": 54
    },
    {
      "epoch": 0.07801418439716312,
      "grad_norm": 1.0760366916656494,
      "learning_rate": 0.0001996125907990315,
      "loss": 1.8404,
      "step": 55
    },
    {
      "epoch": 0.07943262411347518,
      "grad_norm": 1.1468721628189087,
      "learning_rate": 0.00019951573849878935,
      "loss": 1.9124,
      "step": 56
    },
    {
      "epoch": 0.08085106382978724,
      "grad_norm": 1.0046629905700684,
      "learning_rate": 0.00019941888619854722,
      "loss": 1.9508,
      "step": 57
    },
    {
      "epoch": 0.08226950354609928,
      "grad_norm": 1.1077072620391846,
      "learning_rate": 0.0001993220338983051,
      "loss": 1.966,
      "step": 58
    },
    {
      "epoch": 0.08368794326241134,
      "grad_norm": 1.161176085472107,
      "learning_rate": 0.00019922518159806295,
      "loss": 1.7889,
      "step": 59
    },
    {
      "epoch": 0.0851063829787234,
      "grad_norm": 1.0874756574630737,
      "learning_rate": 0.00019912832929782083,
      "loss": 1.9527,
      "step": 60
    },
    {
      "epoch": 0.08652482269503546,
      "grad_norm": 1.1886991262435913,
      "learning_rate": 0.00019903147699757868,
      "loss": 1.846,
      "step": 61
    },
    {
      "epoch": 0.08794326241134752,
      "grad_norm": 0.9369897246360779,
      "learning_rate": 0.00019893462469733656,
      "loss": 1.6584,
      "step": 62
    },
    {
      "epoch": 0.08936170212765958,
      "grad_norm": 1.0621529817581177,
      "learning_rate": 0.00019883777239709444,
      "loss": 1.8865,
      "step": 63
    },
    {
      "epoch": 0.09078014184397164,
      "grad_norm": 0.9763602614402771,
      "learning_rate": 0.0001987409200968523,
      "loss": 1.8334,
      "step": 64
    },
    {
      "epoch": 0.09219858156028368,
      "grad_norm": 1.0840765237808228,
      "learning_rate": 0.00019864406779661017,
      "loss": 2.0674,
      "step": 65
    },
    {
      "epoch": 0.09361702127659574,
      "grad_norm": 0.9588376879692078,
      "learning_rate": 0.00019854721549636805,
      "loss": 1.8821,
      "step": 66
    },
    {
      "epoch": 0.0950354609929078,
      "grad_norm": 0.9782182574272156,
      "learning_rate": 0.0001984503631961259,
      "loss": 1.8329,
      "step": 67
    },
    {
      "epoch": 0.09645390070921986,
      "grad_norm": 1.0978199243545532,
      "learning_rate": 0.00019835351089588377,
      "loss": 1.7789,
      "step": 68
    },
    {
      "epoch": 0.09787234042553192,
      "grad_norm": 1.2051382064819336,
      "learning_rate": 0.00019825665859564165,
      "loss": 1.8647,
      "step": 69
    },
    {
      "epoch": 0.09929078014184398,
      "grad_norm": 1.075136661529541,
      "learning_rate": 0.00019815980629539953,
      "loss": 1.8156,
      "step": 70
    },
    {
      "epoch": 0.10070921985815603,
      "grad_norm": 1.114687204360962,
      "learning_rate": 0.0001980629539951574,
      "loss": 2.0969,
      "step": 71
    },
    {
      "epoch": 0.10212765957446808,
      "grad_norm": 1.0886708498001099,
      "learning_rate": 0.00019796610169491526,
      "loss": 1.8735,
      "step": 72
    },
    {
      "epoch": 0.10354609929078014,
      "grad_norm": 1.0000802278518677,
      "learning_rate": 0.00019786924939467314,
      "loss": 1.7809,
      "step": 73
    },
    {
      "epoch": 0.1049645390070922,
      "grad_norm": 1.0024892091751099,
      "learning_rate": 0.00019777239709443102,
      "loss": 1.8654,
      "step": 74
    },
    {
      "epoch": 0.10638297872340426,
      "grad_norm": 1.0471751689910889,
      "learning_rate": 0.00019767554479418887,
      "loss": 1.8414,
      "step": 75
    },
    {
      "epoch": 0.10780141843971631,
      "grad_norm": 1.0259408950805664,
      "learning_rate": 0.00019757869249394675,
      "loss": 1.8887,
      "step": 76
    },
    {
      "epoch": 0.10921985815602837,
      "grad_norm": 1.0418591499328613,
      "learning_rate": 0.00019748184019370462,
      "loss": 1.9078,
      "step": 77
    },
    {
      "epoch": 0.11063829787234042,
      "grad_norm": 1.106501579284668,
      "learning_rate": 0.00019738498789346247,
      "loss": 1.9529,
      "step": 78
    },
    {
      "epoch": 0.11205673758865248,
      "grad_norm": 1.2001898288726807,
      "learning_rate": 0.00019728813559322035,
      "loss": 1.8873,
      "step": 79
    },
    {
      "epoch": 0.11347517730496454,
      "grad_norm": 1.1186670064926147,
      "learning_rate": 0.00019719128329297823,
      "loss": 1.9769,
      "step": 80
    },
    {
      "epoch": 0.1148936170212766,
      "grad_norm": 1.0681841373443604,
      "learning_rate": 0.00019709443099273608,
      "loss": 1.932,
      "step": 81
    },
    {
      "epoch": 0.11631205673758865,
      "grad_norm": 1.085942029953003,
      "learning_rate": 0.00019699757869249396,
      "loss": 1.9425,
      "step": 82
    },
    {
      "epoch": 0.11773049645390071,
      "grad_norm": 1.0608960390090942,
      "learning_rate": 0.00019690072639225184,
      "loss": 1.8147,
      "step": 83
    },
    {
      "epoch": 0.11914893617021277,
      "grad_norm": 0.9925984144210815,
      "learning_rate": 0.0001968038740920097,
      "loss": 1.6996,
      "step": 84
    },
    {
      "epoch": 0.12056737588652482,
      "grad_norm": 1.0561087131500244,
      "learning_rate": 0.00019670702179176757,
      "loss": 1.9734,
      "step": 85
    },
    {
      "epoch": 0.12198581560283688,
      "grad_norm": 0.9507638812065125,
      "learning_rate": 0.00019661016949152545,
      "loss": 1.8695,
      "step": 86
    },
    {
      "epoch": 0.12340425531914893,
      "grad_norm": 1.1066452264785767,
      "learning_rate": 0.0001965133171912833,
      "loss": 1.8306,
      "step": 87
    },
    {
      "epoch": 0.12482269503546099,
      "grad_norm": 1.409095287322998,
      "learning_rate": 0.00019641646489104117,
      "loss": 1.9217,
      "step": 88
    },
    {
      "epoch": 0.12624113475177304,
      "grad_norm": 1.0073235034942627,
      "learning_rate": 0.00019631961259079905,
      "loss": 1.871,
      "step": 89
    },
    {
      "epoch": 0.1276595744680851,
      "grad_norm": 1.1069413423538208,
      "learning_rate": 0.0001962227602905569,
      "loss": 2.2092,
      "step": 90
    },
    {
      "epoch": 0.12907801418439716,
      "grad_norm": 0.9206241369247437,
      "learning_rate": 0.00019612590799031478,
      "loss": 1.5993,
      "step": 91
    },
    {
      "epoch": 0.13049645390070921,
      "grad_norm": 0.971748948097229,
      "learning_rate": 0.00019602905569007263,
      "loss": 1.9343,
      "step": 92
    },
    {
      "epoch": 0.13191489361702127,
      "grad_norm": 0.9606374502182007,
      "learning_rate": 0.0001959322033898305,
      "loss": 1.988,
      "step": 93
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 0.992902934551239,
      "learning_rate": 0.0001958353510895884,
      "loss": 1.6357,
      "step": 94
    },
    {
      "epoch": 0.1347517730496454,
      "grad_norm": 0.991494357585907,
      "learning_rate": 0.00019573849878934624,
      "loss": 1.929,
      "step": 95
    },
    {
      "epoch": 0.13617021276595745,
      "grad_norm": 0.9323453903198242,
      "learning_rate": 0.00019564164648910412,
      "loss": 1.9671,
      "step": 96
    },
    {
      "epoch": 0.1375886524822695,
      "grad_norm": 1.1381146907806396,
      "learning_rate": 0.000195544794188862,
      "loss": 1.6889,
      "step": 97
    },
    {
      "epoch": 0.13900709219858157,
      "grad_norm": 0.942345380783081,
      "learning_rate": 0.00019544794188861985,
      "loss": 1.8009,
      "step": 98
    },
    {
      "epoch": 0.14042553191489363,
      "grad_norm": 1.1215462684631348,
      "learning_rate": 0.00019535108958837773,
      "loss": 1.9217,
      "step": 99
    },
    {
      "epoch": 0.14184397163120568,
      "grad_norm": 1.0809898376464844,
      "learning_rate": 0.0001952542372881356,
      "loss": 2.0826,
      "step": 100
    },
    {
      "epoch": 0.14326241134751774,
      "grad_norm": 0.9221402406692505,
      "learning_rate": 0.00019515738498789345,
      "loss": 1.8363,
      "step": 101
    },
    {
      "epoch": 0.14468085106382977,
      "grad_norm": 1.1425656080245972,
      "learning_rate": 0.00019506053268765133,
      "loss": 1.9013,
      "step": 102
    },
    {
      "epoch": 0.14609929078014183,
      "grad_norm": 0.9840394258499146,
      "learning_rate": 0.0001949636803874092,
      "loss": 1.8085,
      "step": 103
    },
    {
      "epoch": 0.1475177304964539,
      "grad_norm": 1.1152970790863037,
      "learning_rate": 0.00019486682808716706,
      "loss": 2.0811,
      "step": 104
    },
    {
      "epoch": 0.14893617021276595,
      "grad_norm": 0.9925697445869446,
      "learning_rate": 0.00019476997578692494,
      "loss": 1.7898,
      "step": 105
    },
    {
      "epoch": 0.150354609929078,
      "grad_norm": 0.9788102507591248,
      "learning_rate": 0.00019467312348668282,
      "loss": 1.8498,
      "step": 106
    },
    {
      "epoch": 0.15177304964539007,
      "grad_norm": 1.0104581117630005,
      "learning_rate": 0.0001945762711864407,
      "loss": 1.7161,
      "step": 107
    },
    {
      "epoch": 0.15319148936170213,
      "grad_norm": 1.0371237993240356,
      "learning_rate": 0.00019447941888619857,
      "loss": 2.1101,
      "step": 108
    },
    {
      "epoch": 0.15460992907801419,
      "grad_norm": 1.0925321578979492,
      "learning_rate": 0.00019438256658595643,
      "loss": 2.0603,
      "step": 109
    },
    {
      "epoch": 0.15602836879432624,
      "grad_norm": 0.9612079858779907,
      "learning_rate": 0.0001942857142857143,
      "loss": 1.8853,
      "step": 110
    },
    {
      "epoch": 0.1574468085106383,
      "grad_norm": 1.045314073562622,
      "learning_rate": 0.00019418886198547218,
      "loss": 1.715,
      "step": 111
    },
    {
      "epoch": 0.15886524822695036,
      "grad_norm": 0.9515319466590881,
      "learning_rate": 0.00019409200968523003,
      "loss": 1.9728,
      "step": 112
    },
    {
      "epoch": 0.16028368794326242,
      "grad_norm": 0.9624865055084229,
      "learning_rate": 0.0001939951573849879,
      "loss": 1.9609,
      "step": 113
    },
    {
      "epoch": 0.16170212765957448,
      "grad_norm": 1.0090194940567017,
      "learning_rate": 0.0001938983050847458,
      "loss": 1.9138,
      "step": 114
    },
    {
      "epoch": 0.16312056737588654,
      "grad_norm": 0.997510552406311,
      "learning_rate": 0.00019380145278450364,
      "loss": 1.8258,
      "step": 115
    },
    {
      "epoch": 0.16453900709219857,
      "grad_norm": 1.1058714389801025,
      "learning_rate": 0.00019370460048426152,
      "loss": 1.9574,
      "step": 116
    },
    {
      "epoch": 0.16595744680851063,
      "grad_norm": 0.9852263927459717,
      "learning_rate": 0.0001936077481840194,
      "loss": 1.9388,
      "step": 117
    },
    {
      "epoch": 0.1673758865248227,
      "grad_norm": 1.0362639427185059,
      "learning_rate": 0.00019351089588377725,
      "loss": 1.9979,
      "step": 118
    },
    {
      "epoch": 0.16879432624113475,
      "grad_norm": 0.9582896828651428,
      "learning_rate": 0.00019341404358353513,
      "loss": 1.9451,
      "step": 119
    },
    {
      "epoch": 0.1702127659574468,
      "grad_norm": 1.2235652208328247,
      "learning_rate": 0.000193317191283293,
      "loss": 1.9965,
      "step": 120
    },
    {
      "epoch": 0.17163120567375886,
      "grad_norm": 0.9667586088180542,
      "learning_rate": 0.00019322033898305085,
      "loss": 2.0728,
      "step": 121
    },
    {
      "epoch": 0.17304964539007092,
      "grad_norm": 0.9890563488006592,
      "learning_rate": 0.00019312348668280873,
      "loss": 1.9984,
      "step": 122
    },
    {
      "epoch": 0.17446808510638298,
      "grad_norm": 0.9136565923690796,
      "learning_rate": 0.00019302663438256658,
      "loss": 1.9968,
      "step": 123
    },
    {
      "epoch": 0.17588652482269504,
      "grad_norm": 1.1284838914871216,
      "learning_rate": 0.00019292978208232446,
      "loss": 2.0538,
      "step": 124
    },
    {
      "epoch": 0.1773049645390071,
      "grad_norm": 0.9669015407562256,
      "learning_rate": 0.00019283292978208234,
      "loss": 1.9323,
      "step": 125
    },
    {
      "epoch": 0.17872340425531916,
      "grad_norm": 0.9495101571083069,
      "learning_rate": 0.0001927360774818402,
      "loss": 1.9049,
      "step": 126
    },
    {
      "epoch": 0.18014184397163122,
      "grad_norm": 0.9509551525115967,
      "learning_rate": 0.00019263922518159807,
      "loss": 1.9258,
      "step": 127
    },
    {
      "epoch": 0.18156028368794327,
      "grad_norm": 0.9739115834236145,
      "learning_rate": 0.00019254237288135595,
      "loss": 1.7723,
      "step": 128
    },
    {
      "epoch": 0.1829787234042553,
      "grad_norm": 0.9413913488388062,
      "learning_rate": 0.0001924455205811138,
      "loss": 1.8601,
      "step": 129
    },
    {
      "epoch": 0.18439716312056736,
      "grad_norm": 1.0929319858551025,
      "learning_rate": 0.00019234866828087168,
      "loss": 2.0404,
      "step": 130
    },
    {
      "epoch": 0.18581560283687942,
      "grad_norm": 0.9712840914726257,
      "learning_rate": 0.00019225181598062955,
      "loss": 1.6331,
      "step": 131
    },
    {
      "epoch": 0.18723404255319148,
      "grad_norm": 1.0864912271499634,
      "learning_rate": 0.0001921549636803874,
      "loss": 1.8071,
      "step": 132
    },
    {
      "epoch": 0.18865248226950354,
      "grad_norm": 1.011907696723938,
      "learning_rate": 0.00019205811138014528,
      "loss": 1.7877,
      "step": 133
    },
    {
      "epoch": 0.1900709219858156,
      "grad_norm": 1.0717408657073975,
      "learning_rate": 0.00019196125907990316,
      "loss": 2.0539,
      "step": 134
    },
    {
      "epoch": 0.19148936170212766,
      "grad_norm": 0.9419696927070618,
      "learning_rate": 0.000191864406779661,
      "loss": 1.6857,
      "step": 135
    },
    {
      "epoch": 0.19290780141843972,
      "grad_norm": 1.008816123008728,
      "learning_rate": 0.0001917675544794189,
      "loss": 1.7538,
      "step": 136
    },
    {
      "epoch": 0.19432624113475178,
      "grad_norm": 0.954567551612854,
      "learning_rate": 0.00019167070217917677,
      "loss": 1.7457,
      "step": 137
    },
    {
      "epoch": 0.19574468085106383,
      "grad_norm": 1.0287903547286987,
      "learning_rate": 0.00019157384987893462,
      "loss": 1.8841,
      "step": 138
    },
    {
      "epoch": 0.1971631205673759,
      "grad_norm": 1.0585139989852905,
      "learning_rate": 0.0001914769975786925,
      "loss": 1.831,
      "step": 139
    },
    {
      "epoch": 0.19858156028368795,
      "grad_norm": 0.8999666571617126,
      "learning_rate": 0.00019138014527845035,
      "loss": 1.7679,
      "step": 140
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.9993060231208801,
      "learning_rate": 0.00019128329297820823,
      "loss": 1.7601,
      "step": 141
    },
    {
      "epoch": 0.20141843971631207,
      "grad_norm": 1.0174119472503662,
      "learning_rate": 0.0001911864406779661,
      "loss": 2.0544,
      "step": 142
    },
    {
      "epoch": 0.2028368794326241,
      "grad_norm": 0.9215055704116821,
      "learning_rate": 0.00019108958837772398,
      "loss": 1.7231,
      "step": 143
    },
    {
      "epoch": 0.20425531914893616,
      "grad_norm": 1.06234610080719,
      "learning_rate": 0.00019099273607748186,
      "loss": 1.9472,
      "step": 144
    },
    {
      "epoch": 0.20567375886524822,
      "grad_norm": 0.9426120519638062,
      "learning_rate": 0.0001908958837772397,
      "loss": 1.8451,
      "step": 145
    },
    {
      "epoch": 0.20709219858156028,
      "grad_norm": 0.9534973502159119,
      "learning_rate": 0.0001907990314769976,
      "loss": 1.9874,
      "step": 146
    },
    {
      "epoch": 0.20851063829787234,
      "grad_norm": 0.9999266266822815,
      "learning_rate": 0.00019070217917675547,
      "loss": 1.9135,
      "step": 147
    },
    {
      "epoch": 0.2099290780141844,
      "grad_norm": 0.9851370453834534,
      "learning_rate": 0.00019060532687651335,
      "loss": 1.9419,
      "step": 148
    },
    {
      "epoch": 0.21134751773049645,
      "grad_norm": 1.1135103702545166,
      "learning_rate": 0.0001905084745762712,
      "loss": 2.2241,
      "step": 149
    },
    {
      "epoch": 0.2127659574468085,
      "grad_norm": 1.024118423461914,
      "learning_rate": 0.00019041162227602908,
      "loss": 1.8521,
      "step": 150
    },
    {
      "epoch": 0.21418439716312057,
      "grad_norm": 0.9475961923599243,
      "learning_rate": 0.00019031476997578695,
      "loss": 1.8281,
      "step": 151
    },
    {
      "epoch": 0.21560283687943263,
      "grad_norm": 1.0830246210098267,
      "learning_rate": 0.0001902179176755448,
      "loss": 2.0724,
      "step": 152
    },
    {
      "epoch": 0.2170212765957447,
      "grad_norm": 1.0670086145401,
      "learning_rate": 0.00019012106537530268,
      "loss": 2.0665,
      "step": 153
    },
    {
      "epoch": 0.21843971631205675,
      "grad_norm": 0.9611037969589233,
      "learning_rate": 0.00019002421307506053,
      "loss": 1.8794,
      "step": 154
    },
    {
      "epoch": 0.2198581560283688,
      "grad_norm": 0.9807606935501099,
      "learning_rate": 0.0001899273607748184,
      "loss": 1.7585,
      "step": 155
    },
    {
      "epoch": 0.22127659574468084,
      "grad_norm": 0.89825040102005,
      "learning_rate": 0.0001898305084745763,
      "loss": 1.715,
      "step": 156
    },
    {
      "epoch": 0.2226950354609929,
      "grad_norm": 1.0388842821121216,
      "learning_rate": 0.00018973365617433414,
      "loss": 2.0931,
      "step": 157
    },
    {
      "epoch": 0.22411347517730495,
      "grad_norm": 0.9821796417236328,
      "learning_rate": 0.00018963680387409202,
      "loss": 1.8478,
      "step": 158
    },
    {
      "epoch": 0.225531914893617,
      "grad_norm": 0.9460282921791077,
      "learning_rate": 0.0001895399515738499,
      "loss": 1.913,
      "step": 159
    },
    {
      "epoch": 0.22695035460992907,
      "grad_norm": 0.9254499673843384,
      "learning_rate": 0.00018944309927360775,
      "loss": 1.9416,
      "step": 160
    },
    {
      "epoch": 0.22836879432624113,
      "grad_norm": 0.980393648147583,
      "learning_rate": 0.00018934624697336563,
      "loss": 1.8052,
      "step": 161
    },
    {
      "epoch": 0.2297872340425532,
      "grad_norm": 0.9918868541717529,
      "learning_rate": 0.0001892493946731235,
      "loss": 1.9824,
      "step": 162
    },
    {
      "epoch": 0.23120567375886525,
      "grad_norm": 0.9574214816093445,
      "learning_rate": 0.00018915254237288136,
      "loss": 1.7779,
      "step": 163
    },
    {
      "epoch": 0.2326241134751773,
      "grad_norm": 0.955186128616333,
      "learning_rate": 0.00018905569007263923,
      "loss": 1.7754,
      "step": 164
    },
    {
      "epoch": 0.23404255319148937,
      "grad_norm": 1.004132866859436,
      "learning_rate": 0.0001889588377723971,
      "loss": 1.883,
      "step": 165
    },
    {
      "epoch": 0.23546099290780143,
      "grad_norm": 0.8997734785079956,
      "learning_rate": 0.00018886198547215496,
      "loss": 2.0024,
      "step": 166
    },
    {
      "epoch": 0.23687943262411348,
      "grad_norm": 0.9180673956871033,
      "learning_rate": 0.00018876513317191284,
      "loss": 1.8712,
      "step": 167
    },
    {
      "epoch": 0.23829787234042554,
      "grad_norm": 1.0329842567443848,
      "learning_rate": 0.00018866828087167072,
      "loss": 2.1347,
      "step": 168
    },
    {
      "epoch": 0.2397163120567376,
      "grad_norm": 1.0167003870010376,
      "learning_rate": 0.00018857142857142857,
      "loss": 1.8339,
      "step": 169
    },
    {
      "epoch": 0.24113475177304963,
      "grad_norm": 1.0670790672302246,
      "learning_rate": 0.00018847457627118645,
      "loss": 1.9222,
      "step": 170
    },
    {
      "epoch": 0.2425531914893617,
      "grad_norm": 0.9621996879577637,
      "learning_rate": 0.0001883777239709443,
      "loss": 1.9693,
      "step": 171
    },
    {
      "epoch": 0.24397163120567375,
      "grad_norm": 0.928979754447937,
      "learning_rate": 0.00018828087167070218,
      "loss": 1.737,
      "step": 172
    },
    {
      "epoch": 0.2453900709219858,
      "grad_norm": 1.0045229196548462,
      "learning_rate": 0.00018818401937046006,
      "loss": 1.9461,
      "step": 173
    },
    {
      "epoch": 0.24680851063829787,
      "grad_norm": 1.0096064805984497,
      "learning_rate": 0.0001880871670702179,
      "loss": 1.8946,
      "step": 174
    },
    {
      "epoch": 0.24822695035460993,
      "grad_norm": 1.0389715433120728,
      "learning_rate": 0.00018799031476997578,
      "loss": 1.9128,
      "step": 175
    },
    {
      "epoch": 0.24964539007092199,
      "grad_norm": 0.999501645565033,
      "learning_rate": 0.00018789346246973366,
      "loss": 1.8569,
      "step": 176
    },
    {
      "epoch": 0.251063829787234,
      "grad_norm": 0.9583911895751953,
      "learning_rate": 0.00018779661016949151,
      "loss": 1.8501,
      "step": 177
    },
    {
      "epoch": 0.2524822695035461,
      "grad_norm": 0.9775153994560242,
      "learning_rate": 0.0001876997578692494,
      "loss": 1.6771,
      "step": 178
    },
    {
      "epoch": 0.25390070921985813,
      "grad_norm": 0.9665774703025818,
      "learning_rate": 0.00018760290556900727,
      "loss": 1.8818,
      "step": 179
    },
    {
      "epoch": 0.2553191489361702,
      "grad_norm": 0.9724550247192383,
      "learning_rate": 0.00018750605326876515,
      "loss": 2.1773,
      "step": 180
    },
    {
      "epoch": 0.25673758865248225,
      "grad_norm": 1.1561572551727295,
      "learning_rate": 0.00018740920096852303,
      "loss": 1.8608,
      "step": 181
    },
    {
      "epoch": 0.2581560283687943,
      "grad_norm": 1.0202478170394897,
      "learning_rate": 0.00018731234866828088,
      "loss": 1.7356,
      "step": 182
    },
    {
      "epoch": 0.25957446808510637,
      "grad_norm": 1.0430041551589966,
      "learning_rate": 0.00018721549636803876,
      "loss": 1.9362,
      "step": 183
    },
    {
      "epoch": 0.26099290780141843,
      "grad_norm": 1.038509488105774,
      "learning_rate": 0.00018711864406779663,
      "loss": 2.1047,
      "step": 184
    },
    {
      "epoch": 0.2624113475177305,
      "grad_norm": 1.0352694988250732,
      "learning_rate": 0.00018702179176755448,
      "loss": 1.9668,
      "step": 185
    },
    {
      "epoch": 0.26382978723404255,
      "grad_norm": 1.0145736932754517,
      "learning_rate": 0.00018692493946731236,
      "loss": 1.9073,
      "step": 186
    },
    {
      "epoch": 0.2652482269503546,
      "grad_norm": 0.985048770904541,
      "learning_rate": 0.00018682808716707024,
      "loss": 1.7562,
      "step": 187
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 1.057384967803955,
      "learning_rate": 0.0001867312348668281,
      "loss": 1.8624,
      "step": 188
    },
    {
      "epoch": 0.2680851063829787,
      "grad_norm": 0.9623408317565918,
      "learning_rate": 0.00018663438256658597,
      "loss": 1.7815,
      "step": 189
    },
    {
      "epoch": 0.2695035460992908,
      "grad_norm": 1.0034300088882446,
      "learning_rate": 0.00018653753026634385,
      "loss": 1.9532,
      "step": 190
    },
    {
      "epoch": 0.27092198581560284,
      "grad_norm": 0.9395196437835693,
      "learning_rate": 0.0001864406779661017,
      "loss": 1.6993,
      "step": 191
    },
    {
      "epoch": 0.2723404255319149,
      "grad_norm": 0.9910885095596313,
      "learning_rate": 0.00018634382566585958,
      "loss": 1.7409,
      "step": 192
    },
    {
      "epoch": 0.27375886524822696,
      "grad_norm": 0.9712793827056885,
      "learning_rate": 0.00018624697336561746,
      "loss": 1.7994,
      "step": 193
    },
    {
      "epoch": 0.275177304964539,
      "grad_norm": 0.9951639771461487,
      "learning_rate": 0.0001861501210653753,
      "loss": 1.9165,
      "step": 194
    },
    {
      "epoch": 0.2765957446808511,
      "grad_norm": 0.9464163184165955,
      "learning_rate": 0.00018605326876513318,
      "loss": 1.8001,
      "step": 195
    },
    {
      "epoch": 0.27801418439716313,
      "grad_norm": 1.0139267444610596,
      "learning_rate": 0.00018595641646489106,
      "loss": 1.8521,
      "step": 196
    },
    {
      "epoch": 0.2794326241134752,
      "grad_norm": 0.9855507016181946,
      "learning_rate": 0.00018585956416464891,
      "loss": 1.9984,
      "step": 197
    },
    {
      "epoch": 0.28085106382978725,
      "grad_norm": 1.098187804222107,
      "learning_rate": 0.0001857627118644068,
      "loss": 1.9804,
      "step": 198
    },
    {
      "epoch": 0.2822695035460993,
      "grad_norm": 0.8774276971817017,
      "learning_rate": 0.00018566585956416467,
      "loss": 1.8172,
      "step": 199
    },
    {
      "epoch": 0.28368794326241137,
      "grad_norm": 0.9513270258903503,
      "learning_rate": 0.00018556900726392252,
      "loss": 2.0358,
      "step": 200
    },
    {
      "epoch": 0.2851063829787234,
      "grad_norm": 1.068496823310852,
      "learning_rate": 0.0001854721549636804,
      "loss": 1.589,
      "step": 201
    },
    {
      "epoch": 0.2865248226950355,
      "grad_norm": 0.9528117179870605,
      "learning_rate": 0.00018537530266343825,
      "loss": 1.7981,
      "step": 202
    },
    {
      "epoch": 0.28794326241134754,
      "grad_norm": 0.9149752259254456,
      "learning_rate": 0.00018527845036319613,
      "loss": 1.7106,
      "step": 203
    },
    {
      "epoch": 0.28936170212765955,
      "grad_norm": 1.0679677724838257,
      "learning_rate": 0.000185181598062954,
      "loss": 1.7816,
      "step": 204
    },
    {
      "epoch": 0.2907801418439716,
      "grad_norm": 1.0009958744049072,
      "learning_rate": 0.00018508474576271186,
      "loss": 1.9823,
      "step": 205
    },
    {
      "epoch": 0.29219858156028367,
      "grad_norm": 0.9693969488143921,
      "learning_rate": 0.00018498789346246974,
      "loss": 1.7922,
      "step": 206
    },
    {
      "epoch": 0.2936170212765957,
      "grad_norm": 0.8737604022026062,
      "learning_rate": 0.00018489104116222761,
      "loss": 1.7473,
      "step": 207
    },
    {
      "epoch": 0.2950354609929078,
      "grad_norm": 0.9264819622039795,
      "learning_rate": 0.00018479418886198546,
      "loss": 1.8496,
      "step": 208
    },
    {
      "epoch": 0.29645390070921984,
      "grad_norm": 0.9392140507698059,
      "learning_rate": 0.00018469733656174334,
      "loss": 1.9292,
      "step": 209
    },
    {
      "epoch": 0.2978723404255319,
      "grad_norm": 0.9021729230880737,
      "learning_rate": 0.00018460048426150122,
      "loss": 1.771,
      "step": 210
    },
    {
      "epoch": 0.29929078014184396,
      "grad_norm": 0.9156889319419861,
      "learning_rate": 0.00018450363196125907,
      "loss": 1.7914,
      "step": 211
    },
    {
      "epoch": 0.300709219858156,
      "grad_norm": 0.9815962314605713,
      "learning_rate": 0.00018440677966101695,
      "loss": 2.0523,
      "step": 212
    },
    {
      "epoch": 0.3021276595744681,
      "grad_norm": 0.9518104195594788,
      "learning_rate": 0.00018430992736077483,
      "loss": 1.8942,
      "step": 213
    },
    {
      "epoch": 0.30354609929078014,
      "grad_norm": 1.049188256263733,
      "learning_rate": 0.00018421307506053268,
      "loss": 2.2277,
      "step": 214
    },
    {
      "epoch": 0.3049645390070922,
      "grad_norm": 0.9291843175888062,
      "learning_rate": 0.00018411622276029056,
      "loss": 1.9009,
      "step": 215
    },
    {
      "epoch": 0.30638297872340425,
      "grad_norm": 1.0086026191711426,
      "learning_rate": 0.00018401937046004844,
      "loss": 1.8147,
      "step": 216
    },
    {
      "epoch": 0.3078014184397163,
      "grad_norm": 1.0855687856674194,
      "learning_rate": 0.0001839225181598063,
      "loss": 1.6274,
      "step": 217
    },
    {
      "epoch": 0.30921985815602837,
      "grad_norm": 1.01848304271698,
      "learning_rate": 0.00018382566585956416,
      "loss": 1.9065,
      "step": 218
    },
    {
      "epoch": 0.31063829787234043,
      "grad_norm": 1.115381121635437,
      "learning_rate": 0.00018372881355932204,
      "loss": 1.8363,
      "step": 219
    },
    {
      "epoch": 0.3120567375886525,
      "grad_norm": 1.0715506076812744,
      "learning_rate": 0.00018363196125907992,
      "loss": 1.8608,
      "step": 220
    },
    {
      "epoch": 0.31347517730496455,
      "grad_norm": 0.9929397702217102,
      "learning_rate": 0.0001835351089588378,
      "loss": 1.7621,
      "step": 221
    },
    {
      "epoch": 0.3148936170212766,
      "grad_norm": 1.007621169090271,
      "learning_rate": 0.00018343825665859565,
      "loss": 1.7274,
      "step": 222
    },
    {
      "epoch": 0.31631205673758866,
      "grad_norm": 1.038918137550354,
      "learning_rate": 0.00018334140435835353,
      "loss": 2.0179,
      "step": 223
    },
    {
      "epoch": 0.3177304964539007,
      "grad_norm": 0.9554470777511597,
      "learning_rate": 0.0001832445520581114,
      "loss": 1.8415,
      "step": 224
    },
    {
      "epoch": 0.3191489361702128,
      "grad_norm": 0.9952170848846436,
      "learning_rate": 0.00018314769975786926,
      "loss": 1.8044,
      "step": 225
    },
    {
      "epoch": 0.32056737588652484,
      "grad_norm": 0.9729833006858826,
      "learning_rate": 0.00018305084745762714,
      "loss": 1.8347,
      "step": 226
    },
    {
      "epoch": 0.3219858156028369,
      "grad_norm": 1.0922369956970215,
      "learning_rate": 0.000182953995157385,
      "loss": 2.1871,
      "step": 227
    },
    {
      "epoch": 0.32340425531914896,
      "grad_norm": 1.0044426918029785,
      "learning_rate": 0.00018285714285714286,
      "loss": 1.8984,
      "step": 228
    },
    {
      "epoch": 0.324822695035461,
      "grad_norm": 0.9098760485649109,
      "learning_rate": 0.00018276029055690074,
      "loss": 1.8968,
      "step": 229
    },
    {
      "epoch": 0.3262411347517731,
      "grad_norm": 0.9837214350700378,
      "learning_rate": 0.00018266343825665862,
      "loss": 2.0426,
      "step": 230
    },
    {
      "epoch": 0.3276595744680851,
      "grad_norm": 0.9324159026145935,
      "learning_rate": 0.00018256658595641647,
      "loss": 1.9843,
      "step": 231
    },
    {
      "epoch": 0.32907801418439714,
      "grad_norm": 1.0140657424926758,
      "learning_rate": 0.00018246973365617435,
      "loss": 1.9063,
      "step": 232
    },
    {
      "epoch": 0.3304964539007092,
      "grad_norm": 0.9005314111709595,
      "learning_rate": 0.0001823728813559322,
      "loss": 1.8591,
      "step": 233
    },
    {
      "epoch": 0.33191489361702126,
      "grad_norm": 0.8580217957496643,
      "learning_rate": 0.00018227602905569008,
      "loss": 1.5464,
      "step": 234
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.8500463366508484,
      "learning_rate": 0.00018217917675544796,
      "loss": 1.7719,
      "step": 235
    },
    {
      "epoch": 0.3347517730496454,
      "grad_norm": 0.9767171144485474,
      "learning_rate": 0.0001820823244552058,
      "loss": 2.0819,
      "step": 236
    },
    {
      "epoch": 0.33617021276595743,
      "grad_norm": 0.8938849568367004,
      "learning_rate": 0.00018198547215496369,
      "loss": 1.8295,
      "step": 237
    },
    {
      "epoch": 0.3375886524822695,
      "grad_norm": 0.9511204361915588,
      "learning_rate": 0.00018188861985472156,
      "loss": 1.7266,
      "step": 238
    },
    {
      "epoch": 0.33900709219858155,
      "grad_norm": 0.9522839188575745,
      "learning_rate": 0.00018179176755447942,
      "loss": 1.772,
      "step": 239
    },
    {
      "epoch": 0.3404255319148936,
      "grad_norm": 0.9642924666404724,
      "learning_rate": 0.0001816949152542373,
      "loss": 2.1366,
      "step": 240
    },
    {
      "epoch": 0.34184397163120567,
      "grad_norm": 0.9406882524490356,
      "learning_rate": 0.00018159806295399517,
      "loss": 2.0296,
      "step": 241
    },
    {
      "epoch": 0.3432624113475177,
      "grad_norm": 0.967184841632843,
      "learning_rate": 0.00018150121065375302,
      "loss": 1.8968,
      "step": 242
    },
    {
      "epoch": 0.3446808510638298,
      "grad_norm": 0.893243134021759,
      "learning_rate": 0.0001814043583535109,
      "loss": 1.7,
      "step": 243
    },
    {
      "epoch": 0.34609929078014184,
      "grad_norm": 0.9362541437149048,
      "learning_rate": 0.00018130750605326878,
      "loss": 1.9237,
      "step": 244
    },
    {
      "epoch": 0.3475177304964539,
      "grad_norm": 0.9011443257331848,
      "learning_rate": 0.00018121065375302663,
      "loss": 1.7482,
      "step": 245
    },
    {
      "epoch": 0.34893617021276596,
      "grad_norm": 0.9381369948387146,
      "learning_rate": 0.0001811138014527845,
      "loss": 2.0529,
      "step": 246
    },
    {
      "epoch": 0.350354609929078,
      "grad_norm": 0.9607241153717041,
      "learning_rate": 0.00018101694915254239,
      "loss": 1.7078,
      "step": 247
    },
    {
      "epoch": 0.3517730496453901,
      "grad_norm": 1.008245825767517,
      "learning_rate": 0.00018092009685230024,
      "loss": 1.6759,
      "step": 248
    },
    {
      "epoch": 0.35319148936170214,
      "grad_norm": 0.9571892023086548,
      "learning_rate": 0.00018082324455205812,
      "loss": 2.0354,
      "step": 249
    },
    {
      "epoch": 0.3546099290780142,
      "grad_norm": 0.9735412001609802,
      "learning_rate": 0.00018072639225181597,
      "loss": 1.6914,
      "step": 250
    },
    {
      "epoch": 0.35602836879432626,
      "grad_norm": 1.0446652173995972,
      "learning_rate": 0.00018062953995157384,
      "loss": 1.8721,
      "step": 251
    },
    {
      "epoch": 0.3574468085106383,
      "grad_norm": 1.3239846229553223,
      "learning_rate": 0.00018053268765133172,
      "loss": 1.9019,
      "step": 252
    },
    {
      "epoch": 0.3588652482269504,
      "grad_norm": 0.9775428175926208,
      "learning_rate": 0.0001804358353510896,
      "loss": 1.6349,
      "step": 253
    },
    {
      "epoch": 0.36028368794326243,
      "grad_norm": 1.0675644874572754,
      "learning_rate": 0.00018033898305084748,
      "loss": 2.192,
      "step": 254
    },
    {
      "epoch": 0.3617021276595745,
      "grad_norm": 0.9878771305084229,
      "learning_rate": 0.00018024213075060533,
      "loss": 1.7361,
      "step": 255
    },
    {
      "epoch": 0.36312056737588655,
      "grad_norm": 0.8959107995033264,
      "learning_rate": 0.0001801452784503632,
      "loss": 1.6891,
      "step": 256
    },
    {
      "epoch": 0.3645390070921986,
      "grad_norm": 0.9647307991981506,
      "learning_rate": 0.00018004842615012109,
      "loss": 1.7116,
      "step": 257
    },
    {
      "epoch": 0.3659574468085106,
      "grad_norm": 1.0258516073226929,
      "learning_rate": 0.00017995157384987896,
      "loss": 2.0333,
      "step": 258
    },
    {
      "epoch": 0.36737588652482267,
      "grad_norm": 0.904725193977356,
      "learning_rate": 0.00017985472154963681,
      "loss": 1.765,
      "step": 259
    },
    {
      "epoch": 0.36879432624113473,
      "grad_norm": 0.9768916964530945,
      "learning_rate": 0.0001797578692493947,
      "loss": 1.8871,
      "step": 260
    },
    {
      "epoch": 0.3702127659574468,
      "grad_norm": 0.9332278966903687,
      "learning_rate": 0.00017966101694915257,
      "loss": 1.8291,
      "step": 261
    },
    {
      "epoch": 0.37163120567375885,
      "grad_norm": 0.978183925151825,
      "learning_rate": 0.00017956416464891042,
      "loss": 1.8143,
      "step": 262
    },
    {
      "epoch": 0.3730496453900709,
      "grad_norm": 0.9541735649108887,
      "learning_rate": 0.0001794673123486683,
      "loss": 1.8159,
      "step": 263
    },
    {
      "epoch": 0.37446808510638296,
      "grad_norm": 0.9887608885765076,
      "learning_rate": 0.00017937046004842615,
      "loss": 1.9049,
      "step": 264
    },
    {
      "epoch": 0.375886524822695,
      "grad_norm": 0.9812231659889221,
      "learning_rate": 0.00017927360774818403,
      "loss": 1.8132,
      "step": 265
    },
    {
      "epoch": 0.3773049645390071,
      "grad_norm": 0.9784431457519531,
      "learning_rate": 0.0001791767554479419,
      "loss": 1.8101,
      "step": 266
    },
    {
      "epoch": 0.37872340425531914,
      "grad_norm": 0.9982566833496094,
      "learning_rate": 0.00017907990314769976,
      "loss": 1.7921,
      "step": 267
    },
    {
      "epoch": 0.3801418439716312,
      "grad_norm": 0.9074917435646057,
      "learning_rate": 0.00017898305084745764,
      "loss": 1.9321,
      "step": 268
    },
    {
      "epoch": 0.38156028368794326,
      "grad_norm": 1.0277540683746338,
      "learning_rate": 0.00017888619854721551,
      "loss": 2.126,
      "step": 269
    },
    {
      "epoch": 0.3829787234042553,
      "grad_norm": 0.8828142285346985,
      "learning_rate": 0.00017878934624697337,
      "loss": 1.5719,
      "step": 270
    },
    {
      "epoch": 0.3843971631205674,
      "grad_norm": 0.9607075452804565,
      "learning_rate": 0.00017869249394673124,
      "loss": 1.7884,
      "step": 271
    },
    {
      "epoch": 0.38581560283687943,
      "grad_norm": 0.9126862287521362,
      "learning_rate": 0.00017859564164648912,
      "loss": 1.9482,
      "step": 272
    },
    {
      "epoch": 0.3872340425531915,
      "grad_norm": 0.9614412188529968,
      "learning_rate": 0.00017849878934624697,
      "loss": 1.7178,
      "step": 273
    },
    {
      "epoch": 0.38865248226950355,
      "grad_norm": 0.9119043350219727,
      "learning_rate": 0.00017840193704600485,
      "loss": 1.8499,
      "step": 274
    },
    {
      "epoch": 0.3900709219858156,
      "grad_norm": 1.0984479188919067,
      "learning_rate": 0.00017830508474576273,
      "loss": 1.759,
      "step": 275
    },
    {
      "epoch": 0.39148936170212767,
      "grad_norm": 0.9465433955192566,
      "learning_rate": 0.00017820823244552058,
      "loss": 1.807,
      "step": 276
    },
    {
      "epoch": 0.39290780141843973,
      "grad_norm": 0.9199845194816589,
      "learning_rate": 0.00017811138014527846,
      "loss": 1.8849,
      "step": 277
    },
    {
      "epoch": 0.3943262411347518,
      "grad_norm": 0.9319270849227905,
      "learning_rate": 0.00017801452784503634,
      "loss": 1.859,
      "step": 278
    },
    {
      "epoch": 0.39574468085106385,
      "grad_norm": 0.949927031993866,
      "learning_rate": 0.0001779176755447942,
      "loss": 1.9239,
      "step": 279
    },
    {
      "epoch": 0.3971631205673759,
      "grad_norm": 0.9235413074493408,
      "learning_rate": 0.00017782082324455207,
      "loss": 1.8989,
      "step": 280
    },
    {
      "epoch": 0.39858156028368796,
      "grad_norm": 0.943297266960144,
      "learning_rate": 0.00017772397094430992,
      "loss": 1.8911,
      "step": 281
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.9669685959815979,
      "learning_rate": 0.0001776271186440678,
      "loss": 1.8185,
      "step": 282
    },
    {
      "epoch": 0.4014184397163121,
      "grad_norm": 0.9418421387672424,
      "learning_rate": 0.00017753026634382567,
      "loss": 1.8185,
      "step": 283
    },
    {
      "epoch": 0.40283687943262414,
      "grad_norm": 1.0247584581375122,
      "learning_rate": 0.00017743341404358352,
      "loss": 1.8827,
      "step": 284
    },
    {
      "epoch": 0.40425531914893614,
      "grad_norm": 1.0367324352264404,
      "learning_rate": 0.0001773365617433414,
      "loss": 1.9937,
      "step": 285
    },
    {
      "epoch": 0.4056737588652482,
      "grad_norm": 1.0447474718093872,
      "learning_rate": 0.00017723970944309928,
      "loss": 1.924,
      "step": 286
    },
    {
      "epoch": 0.40709219858156026,
      "grad_norm": 0.9520232677459717,
      "learning_rate": 0.00017714285714285713,
      "loss": 1.936,
      "step": 287
    },
    {
      "epoch": 0.4085106382978723,
      "grad_norm": 0.9429634213447571,
      "learning_rate": 0.000177046004842615,
      "loss": 1.9339,
      "step": 288
    },
    {
      "epoch": 0.4099290780141844,
      "grad_norm": 0.9821912050247192,
      "learning_rate": 0.0001769491525423729,
      "loss": 2.0034,
      "step": 289
    },
    {
      "epoch": 0.41134751773049644,
      "grad_norm": 0.9924930930137634,
      "learning_rate": 0.00017685230024213077,
      "loss": 1.8511,
      "step": 290
    },
    {
      "epoch": 0.4127659574468085,
      "grad_norm": 1.0038831233978271,
      "learning_rate": 0.00017675544794188862,
      "loss": 2.1056,
      "step": 291
    },
    {
      "epoch": 0.41418439716312055,
      "grad_norm": 0.9564592838287354,
      "learning_rate": 0.0001766585956416465,
      "loss": 1.8863,
      "step": 292
    },
    {
      "epoch": 0.4156028368794326,
      "grad_norm": 0.9334070086479187,
      "learning_rate": 0.00017656174334140437,
      "loss": 1.8718,
      "step": 293
    },
    {
      "epoch": 0.41702127659574467,
      "grad_norm": 0.8720489144325256,
      "learning_rate": 0.00017646489104116225,
      "loss": 1.8468,
      "step": 294
    },
    {
      "epoch": 0.41843971631205673,
      "grad_norm": 0.8848581314086914,
      "learning_rate": 0.00017636803874092013,
      "loss": 1.8084,
      "step": 295
    },
    {
      "epoch": 0.4198581560283688,
      "grad_norm": 0.9114629626274109,
      "learning_rate": 0.00017627118644067798,
      "loss": 1.9487,
      "step": 296
    },
    {
      "epoch": 0.42127659574468085,
      "grad_norm": 0.9775903820991516,
      "learning_rate": 0.00017617433414043586,
      "loss": 1.7691,
      "step": 297
    },
    {
      "epoch": 0.4226950354609929,
      "grad_norm": 1.0030083656311035,
      "learning_rate": 0.0001760774818401937,
      "loss": 1.7996,
      "step": 298
    },
    {
      "epoch": 0.42411347517730497,
      "grad_norm": 0.9292450547218323,
      "learning_rate": 0.0001759806295399516,
      "loss": 1.7146,
      "step": 299
    },
    {
      "epoch": 0.425531914893617,
      "grad_norm": 0.9759061336517334,
      "learning_rate": 0.00017588377723970947,
      "loss": 1.7757,
      "step": 300
    },
    {
      "epoch": 0.4269503546099291,
      "grad_norm": 0.9845147132873535,
      "learning_rate": 0.00017578692493946732,
      "loss": 1.768,
      "step": 301
    },
    {
      "epoch": 0.42836879432624114,
      "grad_norm": 0.9506266713142395,
      "learning_rate": 0.0001756900726392252,
      "loss": 2.0084,
      "step": 302
    },
    {
      "epoch": 0.4297872340425532,
      "grad_norm": 0.9350481033325195,
      "learning_rate": 0.00017559322033898307,
      "loss": 1.9765,
      "step": 303
    },
    {
      "epoch": 0.43120567375886526,
      "grad_norm": 0.9896640181541443,
      "learning_rate": 0.00017549636803874092,
      "loss": 1.893,
      "step": 304
    },
    {
      "epoch": 0.4326241134751773,
      "grad_norm": 0.9888864159584045,
      "learning_rate": 0.0001753995157384988,
      "loss": 1.88,
      "step": 305
    },
    {
      "epoch": 0.4340425531914894,
      "grad_norm": 0.937113881111145,
      "learning_rate": 0.00017530266343825668,
      "loss": 1.8432,
      "step": 306
    },
    {
      "epoch": 0.43546099290780144,
      "grad_norm": 0.9765783548355103,
      "learning_rate": 0.00017520581113801453,
      "loss": 1.7804,
      "step": 307
    },
    {
      "epoch": 0.4368794326241135,
      "grad_norm": 0.9332955479621887,
      "learning_rate": 0.0001751089588377724,
      "loss": 1.8956,
      "step": 308
    },
    {
      "epoch": 0.43829787234042555,
      "grad_norm": 0.9328266382217407,
      "learning_rate": 0.0001750121065375303,
      "loss": 1.782,
      "step": 309
    },
    {
      "epoch": 0.4397163120567376,
      "grad_norm": 0.9297088980674744,
      "learning_rate": 0.00017491525423728814,
      "loss": 1.805,
      "step": 310
    },
    {
      "epoch": 0.44113475177304967,
      "grad_norm": 0.9776032567024231,
      "learning_rate": 0.00017481840193704602,
      "loss": 1.8934,
      "step": 311
    },
    {
      "epoch": 0.4425531914893617,
      "grad_norm": 0.9452140927314758,
      "learning_rate": 0.00017472154963680387,
      "loss": 1.8798,
      "step": 312
    },
    {
      "epoch": 0.44397163120567373,
      "grad_norm": 1.030831217765808,
      "learning_rate": 0.00017462469733656175,
      "loss": 1.7181,
      "step": 313
    },
    {
      "epoch": 0.4453900709219858,
      "grad_norm": 0.8747674226760864,
      "learning_rate": 0.00017452784503631962,
      "loss": 1.6531,
      "step": 314
    },
    {
      "epoch": 0.44680851063829785,
      "grad_norm": 0.9434899687767029,
      "learning_rate": 0.00017443099273607747,
      "loss": 1.7733,
      "step": 315
    },
    {
      "epoch": 0.4482269503546099,
      "grad_norm": 0.9592564105987549,
      "learning_rate": 0.00017433414043583535,
      "loss": 2.0461,
      "step": 316
    },
    {
      "epoch": 0.44964539007092197,
      "grad_norm": 0.9413037300109863,
      "learning_rate": 0.00017423728813559323,
      "loss": 1.9054,
      "step": 317
    },
    {
      "epoch": 0.451063829787234,
      "grad_norm": 0.9602166414260864,
      "learning_rate": 0.00017414043583535108,
      "loss": 1.8483,
      "step": 318
    },
    {
      "epoch": 0.4524822695035461,
      "grad_norm": 1.0252346992492676,
      "learning_rate": 0.00017404358353510896,
      "loss": 1.8019,
      "step": 319
    },
    {
      "epoch": 0.45390070921985815,
      "grad_norm": 0.9301417469978333,
      "learning_rate": 0.00017394673123486684,
      "loss": 1.7285,
      "step": 320
    },
    {
      "epoch": 0.4553191489361702,
      "grad_norm": 0.9384927153587341,
      "learning_rate": 0.0001738498789346247,
      "loss": 1.6223,
      "step": 321
    },
    {
      "epoch": 0.45673758865248226,
      "grad_norm": 0.9198197722434998,
      "learning_rate": 0.00017375302663438257,
      "loss": 1.7974,
      "step": 322
    },
    {
      "epoch": 0.4581560283687943,
      "grad_norm": 0.9289765954017639,
      "learning_rate": 0.00017365617433414045,
      "loss": 1.6964,
      "step": 323
    },
    {
      "epoch": 0.4595744680851064,
      "grad_norm": 0.9340206384658813,
      "learning_rate": 0.0001735593220338983,
      "loss": 1.9346,
      "step": 324
    },
    {
      "epoch": 0.46099290780141844,
      "grad_norm": 1.032168984413147,
      "learning_rate": 0.00017346246973365617,
      "loss": 1.8882,
      "step": 325
    },
    {
      "epoch": 0.4624113475177305,
      "grad_norm": 1.043074607849121,
      "learning_rate": 0.00017336561743341405,
      "loss": 1.5286,
      "step": 326
    },
    {
      "epoch": 0.46382978723404256,
      "grad_norm": 0.9749189615249634,
      "learning_rate": 0.00017326876513317193,
      "loss": 1.9693,
      "step": 327
    },
    {
      "epoch": 0.4652482269503546,
      "grad_norm": 0.9572228789329529,
      "learning_rate": 0.00017317191283292978,
      "loss": 1.8084,
      "step": 328
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 1.0045562982559204,
      "learning_rate": 0.00017307506053268766,
      "loss": 1.7183,
      "step": 329
    },
    {
      "epoch": 0.46808510638297873,
      "grad_norm": 1.1227355003356934,
      "learning_rate": 0.00017297820823244554,
      "loss": 1.9009,
      "step": 330
    },
    {
      "epoch": 0.4695035460992908,
      "grad_norm": 1.0158571004867554,
      "learning_rate": 0.00017288135593220342,
      "loss": 1.9807,
      "step": 331
    },
    {
      "epoch": 0.47092198581560285,
      "grad_norm": 0.9610460996627808,
      "learning_rate": 0.00017278450363196127,
      "loss": 1.7399,
      "step": 332
    },
    {
      "epoch": 0.4723404255319149,
      "grad_norm": 0.8958942294120789,
      "learning_rate": 0.00017268765133171915,
      "loss": 1.7994,
      "step": 333
    },
    {
      "epoch": 0.47375886524822697,
      "grad_norm": 0.9412689805030823,
      "learning_rate": 0.00017259079903147702,
      "loss": 2.0594,
      "step": 334
    },
    {
      "epoch": 0.475177304964539,
      "grad_norm": 0.9631602764129639,
      "learning_rate": 0.00017249394673123487,
      "loss": 1.7864,
      "step": 335
    },
    {
      "epoch": 0.4765957446808511,
      "grad_norm": 1.1445562839508057,
      "learning_rate": 0.00017239709443099275,
      "loss": 1.5906,
      "step": 336
    },
    {
      "epoch": 0.47801418439716314,
      "grad_norm": 0.9993076920509338,
      "learning_rate": 0.00017230024213075063,
      "loss": 1.6411,
      "step": 337
    },
    {
      "epoch": 0.4794326241134752,
      "grad_norm": 1.0374830961227417,
      "learning_rate": 0.00017220338983050848,
      "loss": 1.842,
      "step": 338
    },
    {
      "epoch": 0.4808510638297872,
      "grad_norm": 0.973639726638794,
      "learning_rate": 0.00017210653753026636,
      "loss": 1.7878,
      "step": 339
    },
    {
      "epoch": 0.48226950354609927,
      "grad_norm": 0.9438994526863098,
      "learning_rate": 0.00017200968523002424,
      "loss": 2.0517,
      "step": 340
    },
    {
      "epoch": 0.4836879432624113,
      "grad_norm": 0.9525969624519348,
      "learning_rate": 0.0001719128329297821,
      "loss": 1.9102,
      "step": 341
    },
    {
      "epoch": 0.4851063829787234,
      "grad_norm": 0.9234567284584045,
      "learning_rate": 0.00017181598062953997,
      "loss": 1.932,
      "step": 342
    },
    {
      "epoch": 0.48652482269503544,
      "grad_norm": 0.9590464234352112,
      "learning_rate": 0.00017171912832929782,
      "loss": 1.7139,
      "step": 343
    },
    {
      "epoch": 0.4879432624113475,
      "grad_norm": 0.9586837291717529,
      "learning_rate": 0.0001716222760290557,
      "loss": 1.9188,
      "step": 344
    },
    {
      "epoch": 0.48936170212765956,
      "grad_norm": 0.9036353826522827,
      "learning_rate": 0.00017152542372881357,
      "loss": 1.6881,
      "step": 345
    },
    {
      "epoch": 0.4907801418439716,
      "grad_norm": 0.9529016017913818,
      "learning_rate": 0.00017142857142857143,
      "loss": 1.8478,
      "step": 346
    },
    {
      "epoch": 0.4921985815602837,
      "grad_norm": 1.007252812385559,
      "learning_rate": 0.0001713317191283293,
      "loss": 1.8538,
      "step": 347
    },
    {
      "epoch": 0.49361702127659574,
      "grad_norm": 0.945681631565094,
      "learning_rate": 0.00017123486682808718,
      "loss": 1.9226,
      "step": 348
    },
    {
      "epoch": 0.4950354609929078,
      "grad_norm": 0.9263474941253662,
      "learning_rate": 0.00017113801452784503,
      "loss": 1.7189,
      "step": 349
    },
    {
      "epoch": 0.49645390070921985,
      "grad_norm": 0.9747632741928101,
      "learning_rate": 0.0001710411622276029,
      "loss": 1.8628,
      "step": 350
    },
    {
      "epoch": 0.4978723404255319,
      "grad_norm": 0.9846658706665039,
      "learning_rate": 0.0001709443099273608,
      "loss": 1.9134,
      "step": 351
    },
    {
      "epoch": 0.49929078014184397,
      "grad_norm": 0.9113063812255859,
      "learning_rate": 0.00017084745762711864,
      "loss": 1.899,
      "step": 352
    },
    {
      "epoch": 0.500709219858156,
      "grad_norm": 0.9714037179946899,
      "learning_rate": 0.00017075060532687652,
      "loss": 1.9107,
      "step": 353
    },
    {
      "epoch": 0.502127659574468,
      "grad_norm": 0.9537795186042786,
      "learning_rate": 0.0001706537530266344,
      "loss": 1.8053,
      "step": 354
    },
    {
      "epoch": 0.5035460992907801,
      "grad_norm": 0.9485891461372375,
      "learning_rate": 0.00017055690072639225,
      "loss": 1.7612,
      "step": 355
    },
    {
      "epoch": 0.5049645390070922,
      "grad_norm": 0.9222022891044617,
      "learning_rate": 0.00017046004842615013,
      "loss": 1.8346,
      "step": 356
    },
    {
      "epoch": 0.5063829787234042,
      "grad_norm": 0.9114431142807007,
      "learning_rate": 0.000170363196125908,
      "loss": 1.6909,
      "step": 357
    },
    {
      "epoch": 0.5078014184397163,
      "grad_norm": 0.9674013257026672,
      "learning_rate": 0.00017026634382566585,
      "loss": 1.8031,
      "step": 358
    },
    {
      "epoch": 0.5092198581560283,
      "grad_norm": 0.9337841272354126,
      "learning_rate": 0.00017016949152542373,
      "loss": 1.7164,
      "step": 359
    },
    {
      "epoch": 0.5106382978723404,
      "grad_norm": 0.9948791861534119,
      "learning_rate": 0.00017007263922518158,
      "loss": 1.7063,
      "step": 360
    },
    {
      "epoch": 0.5120567375886524,
      "grad_norm": 1.0778685808181763,
      "learning_rate": 0.00016997578692493946,
      "loss": 2.0027,
      "step": 361
    },
    {
      "epoch": 0.5134751773049645,
      "grad_norm": 0.9692330360412598,
      "learning_rate": 0.00016987893462469734,
      "loss": 1.7529,
      "step": 362
    },
    {
      "epoch": 0.5148936170212766,
      "grad_norm": 1.045678734779358,
      "learning_rate": 0.00016978208232445522,
      "loss": 1.908,
      "step": 363
    },
    {
      "epoch": 0.5163120567375886,
      "grad_norm": 1.0215644836425781,
      "learning_rate": 0.00016968523002421307,
      "loss": 2.0764,
      "step": 364
    },
    {
      "epoch": 0.5177304964539007,
      "grad_norm": 0.9777445793151855,
      "learning_rate": 0.00016958837772397095,
      "loss": 1.871,
      "step": 365
    },
    {
      "epoch": 0.5191489361702127,
      "grad_norm": 0.9745745062828064,
      "learning_rate": 0.00016949152542372882,
      "loss": 1.6008,
      "step": 366
    },
    {
      "epoch": 0.5205673758865248,
      "grad_norm": 1.0185343027114868,
      "learning_rate": 0.0001693946731234867,
      "loss": 1.9607,
      "step": 367
    },
    {
      "epoch": 0.5219858156028369,
      "grad_norm": 0.9472505450248718,
      "learning_rate": 0.00016929782082324458,
      "loss": 1.861,
      "step": 368
    },
    {
      "epoch": 0.5234042553191489,
      "grad_norm": 1.0330811738967896,
      "learning_rate": 0.00016920096852300243,
      "loss": 2.0062,
      "step": 369
    },
    {
      "epoch": 0.524822695035461,
      "grad_norm": 0.931767463684082,
      "learning_rate": 0.0001691041162227603,
      "loss": 1.7943,
      "step": 370
    },
    {
      "epoch": 0.526241134751773,
      "grad_norm": 0.8553069829940796,
      "learning_rate": 0.0001690072639225182,
      "loss": 1.7324,
      "step": 371
    },
    {
      "epoch": 0.5276595744680851,
      "grad_norm": 0.8727834224700928,
      "learning_rate": 0.00016891041162227604,
      "loss": 1.7278,
      "step": 372
    },
    {
      "epoch": 0.5290780141843971,
      "grad_norm": 0.8997536301612854,
      "learning_rate": 0.00016881355932203392,
      "loss": 1.8374,
      "step": 373
    },
    {
      "epoch": 0.5304964539007092,
      "grad_norm": 0.9161466360092163,
      "learning_rate": 0.0001687167070217918,
      "loss": 1.8076,
      "step": 374
    },
    {
      "epoch": 0.5319148936170213,
      "grad_norm": 1.0687780380249023,
      "learning_rate": 0.00016861985472154965,
      "loss": 1.5887,
      "step": 375
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 0.8630894422531128,
      "learning_rate": 0.00016852300242130752,
      "loss": 1.7742,
      "step": 376
    },
    {
      "epoch": 0.5347517730496454,
      "grad_norm": 0.998538076877594,
      "learning_rate": 0.00016842615012106538,
      "loss": 1.7454,
      "step": 377
    },
    {
      "epoch": 0.5361702127659574,
      "grad_norm": 0.9754890203475952,
      "learning_rate": 0.00016832929782082325,
      "loss": 1.8326,
      "step": 378
    },
    {
      "epoch": 0.5375886524822695,
      "grad_norm": 0.9351779222488403,
      "learning_rate": 0.00016823244552058113,
      "loss": 1.8356,
      "step": 379
    },
    {
      "epoch": 0.5390070921985816,
      "grad_norm": 0.8755729794502258,
      "learning_rate": 0.00016813559322033898,
      "loss": 1.6331,
      "step": 380
    },
    {
      "epoch": 0.5404255319148936,
      "grad_norm": 1.0290228128433228,
      "learning_rate": 0.00016803874092009686,
      "loss": 2.1401,
      "step": 381
    },
    {
      "epoch": 0.5418439716312057,
      "grad_norm": 0.9561594724655151,
      "learning_rate": 0.00016794188861985474,
      "loss": 1.7978,
      "step": 382
    },
    {
      "epoch": 0.5432624113475177,
      "grad_norm": 0.9548543691635132,
      "learning_rate": 0.0001678450363196126,
      "loss": 1.8855,
      "step": 383
    },
    {
      "epoch": 0.5446808510638298,
      "grad_norm": 0.9470059871673584,
      "learning_rate": 0.00016774818401937047,
      "loss": 1.7313,
      "step": 384
    },
    {
      "epoch": 0.5460992907801419,
      "grad_norm": 0.9736037254333496,
      "learning_rate": 0.00016765133171912835,
      "loss": 1.8428,
      "step": 385
    },
    {
      "epoch": 0.5475177304964539,
      "grad_norm": 0.957306981086731,
      "learning_rate": 0.0001675544794188862,
      "loss": 2.0756,
      "step": 386
    },
    {
      "epoch": 0.548936170212766,
      "grad_norm": 0.9309136867523193,
      "learning_rate": 0.00016745762711864408,
      "loss": 1.8929,
      "step": 387
    },
    {
      "epoch": 0.550354609929078,
      "grad_norm": 0.8406240940093994,
      "learning_rate": 0.00016736077481840195,
      "loss": 1.8103,
      "step": 388
    },
    {
      "epoch": 0.5517730496453901,
      "grad_norm": 0.8818572163581848,
      "learning_rate": 0.0001672639225181598,
      "loss": 1.9621,
      "step": 389
    },
    {
      "epoch": 0.5531914893617021,
      "grad_norm": 1.0767971277236938,
      "learning_rate": 0.00016716707021791768,
      "loss": 1.8136,
      "step": 390
    },
    {
      "epoch": 0.5546099290780142,
      "grad_norm": 0.9289748668670654,
      "learning_rate": 0.00016707021791767553,
      "loss": 1.7687,
      "step": 391
    },
    {
      "epoch": 0.5560283687943263,
      "grad_norm": 0.962541937828064,
      "learning_rate": 0.0001669733656174334,
      "loss": 1.8763,
      "step": 392
    },
    {
      "epoch": 0.5574468085106383,
      "grad_norm": 0.9504688382148743,
      "learning_rate": 0.0001668765133171913,
      "loss": 2.0939,
      "step": 393
    },
    {
      "epoch": 0.5588652482269504,
      "grad_norm": 0.9227321147918701,
      "learning_rate": 0.00016677966101694914,
      "loss": 1.8242,
      "step": 394
    },
    {
      "epoch": 0.5602836879432624,
      "grad_norm": 0.9383020401000977,
      "learning_rate": 0.00016668280871670702,
      "loss": 2.0296,
      "step": 395
    },
    {
      "epoch": 0.5617021276595745,
      "grad_norm": 0.9648865461349487,
      "learning_rate": 0.0001665859564164649,
      "loss": 1.8459,
      "step": 396
    },
    {
      "epoch": 0.5631205673758866,
      "grad_norm": 0.8519643545150757,
      "learning_rate": 0.00016648910411622275,
      "loss": 1.6578,
      "step": 397
    },
    {
      "epoch": 0.5645390070921986,
      "grad_norm": 0.900329053401947,
      "learning_rate": 0.00016639225181598063,
      "loss": 1.8174,
      "step": 398
    },
    {
      "epoch": 0.5659574468085107,
      "grad_norm": 0.8840930461883545,
      "learning_rate": 0.0001662953995157385,
      "loss": 1.6711,
      "step": 399
    },
    {
      "epoch": 0.5673758865248227,
      "grad_norm": 0.9421043992042542,
      "learning_rate": 0.00016619854721549638,
      "loss": 1.9039,
      "step": 400
    },
    {
      "epoch": 0.5687943262411348,
      "grad_norm": 0.9165019989013672,
      "learning_rate": 0.00016610169491525423,
      "loss": 1.8825,
      "step": 401
    },
    {
      "epoch": 0.5702127659574469,
      "grad_norm": 0.9477689862251282,
      "learning_rate": 0.0001660048426150121,
      "loss": 1.7967,
      "step": 402
    },
    {
      "epoch": 0.5716312056737589,
      "grad_norm": 0.9842367172241211,
      "learning_rate": 0.00016590799031477,
      "loss": 1.812,
      "step": 403
    },
    {
      "epoch": 0.573049645390071,
      "grad_norm": 0.9256036281585693,
      "learning_rate": 0.00016581113801452787,
      "loss": 1.7424,
      "step": 404
    },
    {
      "epoch": 0.574468085106383,
      "grad_norm": 0.9276215434074402,
      "learning_rate": 0.00016571428571428575,
      "loss": 1.9102,
      "step": 405
    },
    {
      "epoch": 0.5758865248226951,
      "grad_norm": 0.9226938486099243,
      "learning_rate": 0.0001656174334140436,
      "loss": 1.7025,
      "step": 406
    },
    {
      "epoch": 0.577304964539007,
      "grad_norm": 0.8902654051780701,
      "learning_rate": 0.00016552058111380148,
      "loss": 1.7183,
      "step": 407
    },
    {
      "epoch": 0.5787234042553191,
      "grad_norm": 0.9309189319610596,
      "learning_rate": 0.00016542372881355933,
      "loss": 1.7781,
      "step": 408
    },
    {
      "epoch": 0.5801418439716312,
      "grad_norm": 0.947432279586792,
      "learning_rate": 0.0001653268765133172,
      "loss": 1.7323,
      "step": 409
    },
    {
      "epoch": 0.5815602836879432,
      "grad_norm": 0.9832228422164917,
      "learning_rate": 0.00016523002421307508,
      "loss": 1.9145,
      "step": 410
    },
    {
      "epoch": 0.5829787234042553,
      "grad_norm": 0.9246309995651245,
      "learning_rate": 0.00016513317191283293,
      "loss": 1.6524,
      "step": 411
    },
    {
      "epoch": 0.5843971631205673,
      "grad_norm": 0.9460448622703552,
      "learning_rate": 0.0001650363196125908,
      "loss": 1.7523,
      "step": 412
    },
    {
      "epoch": 0.5858156028368794,
      "grad_norm": 1.0051263570785522,
      "learning_rate": 0.0001649394673123487,
      "loss": 1.8572,
      "step": 413
    },
    {
      "epoch": 0.5872340425531914,
      "grad_norm": 0.9661255478858948,
      "learning_rate": 0.00016484261501210654,
      "loss": 1.8508,
      "step": 414
    },
    {
      "epoch": 0.5886524822695035,
      "grad_norm": 0.9462977647781372,
      "learning_rate": 0.00016474576271186442,
      "loss": 1.8244,
      "step": 415
    },
    {
      "epoch": 0.5900709219858156,
      "grad_norm": 1.021762490272522,
      "learning_rate": 0.0001646489104116223,
      "loss": 1.797,
      "step": 416
    },
    {
      "epoch": 0.5914893617021276,
      "grad_norm": 0.9398526549339294,
      "learning_rate": 0.00016455205811138015,
      "loss": 1.9082,
      "step": 417
    },
    {
      "epoch": 0.5929078014184397,
      "grad_norm": 0.9971896409988403,
      "learning_rate": 0.00016445520581113803,
      "loss": 2.1014,
      "step": 418
    },
    {
      "epoch": 0.5943262411347517,
      "grad_norm": 0.9796718955039978,
      "learning_rate": 0.0001643583535108959,
      "loss": 1.7606,
      "step": 419
    },
    {
      "epoch": 0.5957446808510638,
      "grad_norm": 1.0000969171524048,
      "learning_rate": 0.00016426150121065376,
      "loss": 1.9288,
      "step": 420
    },
    {
      "epoch": 0.5971631205673759,
      "grad_norm": 0.9603239297866821,
      "learning_rate": 0.00016416464891041163,
      "loss": 1.9338,
      "step": 421
    },
    {
      "epoch": 0.5985815602836879,
      "grad_norm": 1.0116310119628906,
      "learning_rate": 0.00016406779661016948,
      "loss": 1.6369,
      "step": 422
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.9344016909599304,
      "learning_rate": 0.00016397094430992736,
      "loss": 1.8784,
      "step": 423
    },
    {
      "epoch": 0.601418439716312,
      "grad_norm": 0.9857602715492249,
      "learning_rate": 0.00016387409200968524,
      "loss": 1.8524,
      "step": 424
    },
    {
      "epoch": 0.6028368794326241,
      "grad_norm": 0.9478918313980103,
      "learning_rate": 0.0001637772397094431,
      "loss": 1.779,
      "step": 425
    },
    {
      "epoch": 0.6042553191489362,
      "grad_norm": 0.9827153086662292,
      "learning_rate": 0.00016368038740920097,
      "loss": 1.8628,
      "step": 426
    },
    {
      "epoch": 0.6056737588652482,
      "grad_norm": 0.9552390575408936,
      "learning_rate": 0.00016358353510895885,
      "loss": 1.9878,
      "step": 427
    },
    {
      "epoch": 0.6070921985815603,
      "grad_norm": 0.8602179884910583,
      "learning_rate": 0.0001634866828087167,
      "loss": 1.5751,
      "step": 428
    },
    {
      "epoch": 0.6085106382978723,
      "grad_norm": 1.0037013292312622,
      "learning_rate": 0.00016338983050847458,
      "loss": 1.9663,
      "step": 429
    },
    {
      "epoch": 0.6099290780141844,
      "grad_norm": 0.9661442637443542,
      "learning_rate": 0.00016329297820823246,
      "loss": 1.6912,
      "step": 430
    },
    {
      "epoch": 0.6113475177304964,
      "grad_norm": 0.9556114673614502,
      "learning_rate": 0.0001631961259079903,
      "loss": 1.7937,
      "step": 431
    },
    {
      "epoch": 0.6127659574468085,
      "grad_norm": 1.1967833042144775,
      "learning_rate": 0.00016309927360774818,
      "loss": 1.938,
      "step": 432
    },
    {
      "epoch": 0.6141843971631206,
      "grad_norm": 0.9502696990966797,
      "learning_rate": 0.00016300242130750606,
      "loss": 1.7276,
      "step": 433
    },
    {
      "epoch": 0.6156028368794326,
      "grad_norm": 1.0521098375320435,
      "learning_rate": 0.0001629055690072639,
      "loss": 1.8143,
      "step": 434
    },
    {
      "epoch": 0.6170212765957447,
      "grad_norm": 0.9686905145645142,
      "learning_rate": 0.0001628087167070218,
      "loss": 1.9932,
      "step": 435
    },
    {
      "epoch": 0.6184397163120567,
      "grad_norm": 0.9943152070045471,
      "learning_rate": 0.00016271186440677967,
      "loss": 1.9851,
      "step": 436
    },
    {
      "epoch": 0.6198581560283688,
      "grad_norm": 0.9060384631156921,
      "learning_rate": 0.00016261501210653752,
      "loss": 1.8452,
      "step": 437
    },
    {
      "epoch": 0.6212765957446809,
      "grad_norm": 0.9493729472160339,
      "learning_rate": 0.0001625181598062954,
      "loss": 1.6709,
      "step": 438
    },
    {
      "epoch": 0.6226950354609929,
      "grad_norm": 0.9582311511039734,
      "learning_rate": 0.00016242130750605328,
      "loss": 1.8994,
      "step": 439
    },
    {
      "epoch": 0.624113475177305,
      "grad_norm": 1.0585215091705322,
      "learning_rate": 0.00016232445520581116,
      "loss": 1.8092,
      "step": 440
    },
    {
      "epoch": 0.625531914893617,
      "grad_norm": 1.0869824886322021,
      "learning_rate": 0.00016222760290556903,
      "loss": 1.8234,
      "step": 441
    },
    {
      "epoch": 0.6269503546099291,
      "grad_norm": 0.9643338918685913,
      "learning_rate": 0.00016213075060532688,
      "loss": 1.7018,
      "step": 442
    },
    {
      "epoch": 0.6283687943262412,
      "grad_norm": 0.9336432218551636,
      "learning_rate": 0.00016203389830508476,
      "loss": 1.686,
      "step": 443
    },
    {
      "epoch": 0.6297872340425532,
      "grad_norm": 1.0179376602172852,
      "learning_rate": 0.00016193704600484264,
      "loss": 1.9509,
      "step": 444
    },
    {
      "epoch": 0.6312056737588653,
      "grad_norm": 0.9448987245559692,
      "learning_rate": 0.0001618401937046005,
      "loss": 1.8221,
      "step": 445
    },
    {
      "epoch": 0.6326241134751773,
      "grad_norm": 1.063069462776184,
      "learning_rate": 0.00016174334140435837,
      "loss": 2.2294,
      "step": 446
    },
    {
      "epoch": 0.6340425531914894,
      "grad_norm": 0.9483571648597717,
      "learning_rate": 0.00016164648910411625,
      "loss": 1.9025,
      "step": 447
    },
    {
      "epoch": 0.6354609929078014,
      "grad_norm": 0.9159568548202515,
      "learning_rate": 0.0001615496368038741,
      "loss": 1.8817,
      "step": 448
    },
    {
      "epoch": 0.6368794326241135,
      "grad_norm": 0.9895548820495605,
      "learning_rate": 0.00016145278450363198,
      "loss": 1.7253,
      "step": 449
    },
    {
      "epoch": 0.6382978723404256,
      "grad_norm": 0.957243025302887,
      "learning_rate": 0.00016135593220338985,
      "loss": 1.8266,
      "step": 450
    },
    {
      "epoch": 0.6397163120567376,
      "grad_norm": 0.9528331756591797,
      "learning_rate": 0.0001612590799031477,
      "loss": 1.8683,
      "step": 451
    },
    {
      "epoch": 0.6411347517730497,
      "grad_norm": 0.9599409103393555,
      "learning_rate": 0.00016116222760290558,
      "loss": 1.6851,
      "step": 452
    },
    {
      "epoch": 0.6425531914893617,
      "grad_norm": 0.9517663717269897,
      "learning_rate": 0.00016106537530266344,
      "loss": 1.8819,
      "step": 453
    },
    {
      "epoch": 0.6439716312056738,
      "grad_norm": 0.9752060174942017,
      "learning_rate": 0.0001609685230024213,
      "loss": 1.6375,
      "step": 454
    },
    {
      "epoch": 0.6453900709219859,
      "grad_norm": 0.9666942358016968,
      "learning_rate": 0.0001608716707021792,
      "loss": 2.0449,
      "step": 455
    },
    {
      "epoch": 0.6468085106382979,
      "grad_norm": 0.9405897259712219,
      "learning_rate": 0.00016077481840193704,
      "loss": 1.6178,
      "step": 456
    },
    {
      "epoch": 0.64822695035461,
      "grad_norm": 0.9592404961585999,
      "learning_rate": 0.00016067796610169492,
      "loss": 1.8184,
      "step": 457
    },
    {
      "epoch": 0.649645390070922,
      "grad_norm": 0.9113569855690002,
      "learning_rate": 0.0001605811138014528,
      "loss": 1.67,
      "step": 458
    },
    {
      "epoch": 0.6510638297872341,
      "grad_norm": 1.0250040292739868,
      "learning_rate": 0.00016048426150121065,
      "loss": 1.7273,
      "step": 459
    },
    {
      "epoch": 0.6524822695035462,
      "grad_norm": 1.0332226753234863,
      "learning_rate": 0.00016038740920096853,
      "loss": 1.8214,
      "step": 460
    },
    {
      "epoch": 0.6539007092198581,
      "grad_norm": 0.8301089406013489,
      "learning_rate": 0.0001602905569007264,
      "loss": 1.725,
      "step": 461
    },
    {
      "epoch": 0.6553191489361702,
      "grad_norm": 0.9999291300773621,
      "learning_rate": 0.00016019370460048426,
      "loss": 1.8431,
      "step": 462
    },
    {
      "epoch": 0.6567375886524822,
      "grad_norm": 0.9815519452095032,
      "learning_rate": 0.00016009685230024213,
      "loss": 1.7325,
      "step": 463
    },
    {
      "epoch": 0.6581560283687943,
      "grad_norm": 1.0464285612106323,
      "learning_rate": 0.00016,
      "loss": 1.742,
      "step": 464
    },
    {
      "epoch": 0.6595744680851063,
      "grad_norm": 0.9548895955085754,
      "learning_rate": 0.00015990314769975786,
      "loss": 1.6047,
      "step": 465
    },
    {
      "epoch": 0.6609929078014184,
      "grad_norm": 1.084294319152832,
      "learning_rate": 0.00015980629539951574,
      "loss": 1.9552,
      "step": 466
    },
    {
      "epoch": 0.6624113475177305,
      "grad_norm": 1.0143967866897583,
      "learning_rate": 0.00015970944309927362,
      "loss": 1.8402,
      "step": 467
    },
    {
      "epoch": 0.6638297872340425,
      "grad_norm": 0.9333505630493164,
      "learning_rate": 0.00015961259079903147,
      "loss": 1.7068,
      "step": 468
    },
    {
      "epoch": 0.6652482269503546,
      "grad_norm": 0.9322513937950134,
      "learning_rate": 0.00015951573849878935,
      "loss": 1.7812,
      "step": 469
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.994925320148468,
      "learning_rate": 0.0001594188861985472,
      "loss": 1.7499,
      "step": 470
    },
    {
      "epoch": 0.6680851063829787,
      "grad_norm": 0.9555068612098694,
      "learning_rate": 0.00015932203389830508,
      "loss": 1.8724,
      "step": 471
    },
    {
      "epoch": 0.6695035460992907,
      "grad_norm": 0.9105766415596008,
      "learning_rate": 0.00015922518159806296,
      "loss": 1.6967,
      "step": 472
    },
    {
      "epoch": 0.6709219858156028,
      "grad_norm": 0.974273145198822,
      "learning_rate": 0.00015912832929782083,
      "loss": 1.7501,
      "step": 473
    },
    {
      "epoch": 0.6723404255319149,
      "grad_norm": 0.9696586728096008,
      "learning_rate": 0.00015903147699757869,
      "loss": 1.6895,
      "step": 474
    },
    {
      "epoch": 0.6737588652482269,
      "grad_norm": 0.9671431183815002,
      "learning_rate": 0.00015893462469733656,
      "loss": 1.8288,
      "step": 475
    },
    {
      "epoch": 0.675177304964539,
      "grad_norm": 0.9355440735816956,
      "learning_rate": 0.00015883777239709444,
      "loss": 1.9129,
      "step": 476
    },
    {
      "epoch": 0.676595744680851,
      "grad_norm": 0.9853194355964661,
      "learning_rate": 0.00015874092009685232,
      "loss": 1.9956,
      "step": 477
    },
    {
      "epoch": 0.6780141843971631,
      "grad_norm": 0.9325653910636902,
      "learning_rate": 0.0001586440677966102,
      "loss": 1.5878,
      "step": 478
    },
    {
      "epoch": 0.6794326241134752,
      "grad_norm": 0.9293085932731628,
      "learning_rate": 0.00015854721549636805,
      "loss": 1.6628,
      "step": 479
    },
    {
      "epoch": 0.6808510638297872,
      "grad_norm": 0.9750685095787048,
      "learning_rate": 0.00015845036319612593,
      "loss": 1.7832,
      "step": 480
    },
    {
      "epoch": 0.6822695035460993,
      "grad_norm": 0.9032379984855652,
      "learning_rate": 0.0001583535108958838,
      "loss": 1.8512,
      "step": 481
    },
    {
      "epoch": 0.6836879432624113,
      "grad_norm": 0.8798975944519043,
      "learning_rate": 0.00015825665859564166,
      "loss": 1.7611,
      "step": 482
    },
    {
      "epoch": 0.6851063829787234,
      "grad_norm": 0.9648714661598206,
      "learning_rate": 0.00015815980629539953,
      "loss": 1.8787,
      "step": 483
    },
    {
      "epoch": 0.6865248226950355,
      "grad_norm": 1.0370577573776245,
      "learning_rate": 0.0001580629539951574,
      "loss": 1.8954,
      "step": 484
    },
    {
      "epoch": 0.6879432624113475,
      "grad_norm": 0.8941919803619385,
      "learning_rate": 0.00015796610169491526,
      "loss": 1.6513,
      "step": 485
    },
    {
      "epoch": 0.6893617021276596,
      "grad_norm": 1.005692958831787,
      "learning_rate": 0.00015786924939467314,
      "loss": 1.815,
      "step": 486
    },
    {
      "epoch": 0.6907801418439716,
      "grad_norm": 0.8727826476097107,
      "learning_rate": 0.000157772397094431,
      "loss": 1.8058,
      "step": 487
    },
    {
      "epoch": 0.6921985815602837,
      "grad_norm": 0.8884201645851135,
      "learning_rate": 0.00015767554479418887,
      "loss": 1.9065,
      "step": 488
    },
    {
      "epoch": 0.6936170212765957,
      "grad_norm": 0.8967852592468262,
      "learning_rate": 0.00015757869249394675,
      "loss": 1.7835,
      "step": 489
    },
    {
      "epoch": 0.6950354609929078,
      "grad_norm": 0.9886108636856079,
      "learning_rate": 0.0001574818401937046,
      "loss": 1.868,
      "step": 490
    },
    {
      "epoch": 0.6964539007092199,
      "grad_norm": 1.0012317895889282,
      "learning_rate": 0.00015738498789346248,
      "loss": 2.0529,
      "step": 491
    },
    {
      "epoch": 0.6978723404255319,
      "grad_norm": 0.9667900204658508,
      "learning_rate": 0.00015728813559322036,
      "loss": 1.9801,
      "step": 492
    },
    {
      "epoch": 0.699290780141844,
      "grad_norm": 0.9269720315933228,
      "learning_rate": 0.0001571912832929782,
      "loss": 2.0074,
      "step": 493
    },
    {
      "epoch": 0.700709219858156,
      "grad_norm": 0.9457390904426575,
      "learning_rate": 0.00015709443099273609,
      "loss": 1.7077,
      "step": 494
    },
    {
      "epoch": 0.7021276595744681,
      "grad_norm": 0.929481029510498,
      "learning_rate": 0.00015699757869249396,
      "loss": 1.7801,
      "step": 495
    },
    {
      "epoch": 0.7035460992907802,
      "grad_norm": 1.0903468132019043,
      "learning_rate": 0.00015690072639225181,
      "loss": 1.8324,
      "step": 496
    },
    {
      "epoch": 0.7049645390070922,
      "grad_norm": 0.886135995388031,
      "learning_rate": 0.0001568038740920097,
      "loss": 1.6575,
      "step": 497
    },
    {
      "epoch": 0.7063829787234043,
      "grad_norm": 0.9447641372680664,
      "learning_rate": 0.00015670702179176757,
      "loss": 1.8172,
      "step": 498
    },
    {
      "epoch": 0.7078014184397163,
      "grad_norm": 0.9351337552070618,
      "learning_rate": 0.00015661016949152542,
      "loss": 1.866,
      "step": 499
    },
    {
      "epoch": 0.7092198581560284,
      "grad_norm": 0.9051510095596313,
      "learning_rate": 0.0001565133171912833,
      "loss": 1.6929,
      "step": 500
    },
    {
      "epoch": 0.7092198581560284,
      "eval_loss": 1.806251049041748,
      "eval_runtime": 91.737,
      "eval_samples_per_second": 15.37,
      "eval_steps_per_second": 7.685,
      "step": 500
    },
    {
      "epoch": 0.7106382978723405,
      "grad_norm": 0.9373800158500671,
      "learning_rate": 0.00015641646489104115,
      "loss": 1.7382,
      "step": 501
    },
    {
      "epoch": 0.7120567375886525,
      "grad_norm": 1.0061854124069214,
      "learning_rate": 0.00015631961259079903,
      "loss": 1.9958,
      "step": 502
    },
    {
      "epoch": 0.7134751773049646,
      "grad_norm": 0.9167385697364807,
      "learning_rate": 0.0001562227602905569,
      "loss": 1.7058,
      "step": 503
    },
    {
      "epoch": 0.7148936170212766,
      "grad_norm": 0.9798259139060974,
      "learning_rate": 0.00015612590799031476,
      "loss": 2.0648,
      "step": 504
    },
    {
      "epoch": 0.7163120567375887,
      "grad_norm": 0.8581452965736389,
      "learning_rate": 0.00015602905569007264,
      "loss": 1.6057,
      "step": 505
    },
    {
      "epoch": 0.7177304964539007,
      "grad_norm": 0.8883776664733887,
      "learning_rate": 0.00015593220338983051,
      "loss": 1.7381,
      "step": 506
    },
    {
      "epoch": 0.7191489361702128,
      "grad_norm": 0.9018328785896301,
      "learning_rate": 0.00015583535108958837,
      "loss": 2.0363,
      "step": 507
    },
    {
      "epoch": 0.7205673758865249,
      "grad_norm": 0.9636991024017334,
      "learning_rate": 0.00015573849878934624,
      "loss": 1.8643,
      "step": 508
    },
    {
      "epoch": 0.7219858156028369,
      "grad_norm": 0.9437087178230286,
      "learning_rate": 0.00015564164648910412,
      "loss": 1.8585,
      "step": 509
    },
    {
      "epoch": 0.723404255319149,
      "grad_norm": 0.9153167009353638,
      "learning_rate": 0.000155544794188862,
      "loss": 1.6232,
      "step": 510
    },
    {
      "epoch": 0.724822695035461,
      "grad_norm": 0.9500200748443604,
      "learning_rate": 0.00015544794188861985,
      "loss": 1.6023,
      "step": 511
    },
    {
      "epoch": 0.7262411347517731,
      "grad_norm": 1.0050026178359985,
      "learning_rate": 0.00015535108958837773,
      "loss": 1.911,
      "step": 512
    },
    {
      "epoch": 0.7276595744680852,
      "grad_norm": 1.0488208532333374,
      "learning_rate": 0.0001552542372881356,
      "loss": 2.0252,
      "step": 513
    },
    {
      "epoch": 0.7290780141843972,
      "grad_norm": 1.0257785320281982,
      "learning_rate": 0.00015515738498789349,
      "loss": 1.7381,
      "step": 514
    },
    {
      "epoch": 0.7304964539007093,
      "grad_norm": 1.1056292057037354,
      "learning_rate": 0.00015506053268765134,
      "loss": 1.7445,
      "step": 515
    },
    {
      "epoch": 0.7319148936170212,
      "grad_norm": 0.9302806854248047,
      "learning_rate": 0.00015496368038740921,
      "loss": 1.7969,
      "step": 516
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 1.0347586870193481,
      "learning_rate": 0.0001548668280871671,
      "loss": 1.8761,
      "step": 517
    },
    {
      "epoch": 0.7347517730496453,
      "grad_norm": 1.0026211738586426,
      "learning_rate": 0.00015476997578692494,
      "loss": 1.7041,
      "step": 518
    },
    {
      "epoch": 0.7361702127659574,
      "grad_norm": 1.0126824378967285,
      "learning_rate": 0.00015467312348668282,
      "loss": 1.9945,
      "step": 519
    },
    {
      "epoch": 0.7375886524822695,
      "grad_norm": 0.9673506021499634,
      "learning_rate": 0.0001545762711864407,
      "loss": 1.8969,
      "step": 520
    },
    {
      "epoch": 0.7390070921985815,
      "grad_norm": 0.9812307357788086,
      "learning_rate": 0.00015447941888619855,
      "loss": 1.9236,
      "step": 521
    },
    {
      "epoch": 0.7404255319148936,
      "grad_norm": 0.9484560489654541,
      "learning_rate": 0.00015438256658595643,
      "loss": 1.8147,
      "step": 522
    },
    {
      "epoch": 0.7418439716312056,
      "grad_norm": 0.9468274712562561,
      "learning_rate": 0.0001542857142857143,
      "loss": 1.8016,
      "step": 523
    },
    {
      "epoch": 0.7432624113475177,
      "grad_norm": 0.9180728793144226,
      "learning_rate": 0.00015418886198547216,
      "loss": 1.8935,
      "step": 524
    },
    {
      "epoch": 0.7446808510638298,
      "grad_norm": 0.9321813583374023,
      "learning_rate": 0.00015409200968523004,
      "loss": 1.9893,
      "step": 525
    },
    {
      "epoch": 0.7460992907801418,
      "grad_norm": 1.008211374282837,
      "learning_rate": 0.00015399515738498791,
      "loss": 1.9819,
      "step": 526
    },
    {
      "epoch": 0.7475177304964539,
      "grad_norm": 0.9611465334892273,
      "learning_rate": 0.00015389830508474577,
      "loss": 1.8028,
      "step": 527
    },
    {
      "epoch": 0.7489361702127659,
      "grad_norm": 1.06980299949646,
      "learning_rate": 0.00015380145278450364,
      "loss": 1.8331,
      "step": 528
    },
    {
      "epoch": 0.750354609929078,
      "grad_norm": 0.9951017498970032,
      "learning_rate": 0.00015370460048426152,
      "loss": 1.733,
      "step": 529
    },
    {
      "epoch": 0.75177304964539,
      "grad_norm": 0.9143504500389099,
      "learning_rate": 0.00015360774818401937,
      "loss": 1.9987,
      "step": 530
    },
    {
      "epoch": 0.7531914893617021,
      "grad_norm": 1.035732626914978,
      "learning_rate": 0.00015351089588377725,
      "loss": 1.8822,
      "step": 531
    },
    {
      "epoch": 0.7546099290780142,
      "grad_norm": 0.9665756225585938,
      "learning_rate": 0.0001534140435835351,
      "loss": 1.5895,
      "step": 532
    },
    {
      "epoch": 0.7560283687943262,
      "grad_norm": 0.9881098866462708,
      "learning_rate": 0.00015331719128329298,
      "loss": 1.6994,
      "step": 533
    },
    {
      "epoch": 0.7574468085106383,
      "grad_norm": 0.9169169068336487,
      "learning_rate": 0.00015322033898305086,
      "loss": 1.8721,
      "step": 534
    },
    {
      "epoch": 0.7588652482269503,
      "grad_norm": 1.005684494972229,
      "learning_rate": 0.0001531234866828087,
      "loss": 1.7963,
      "step": 535
    },
    {
      "epoch": 0.7602836879432624,
      "grad_norm": 0.9482589364051819,
      "learning_rate": 0.0001530266343825666,
      "loss": 1.8089,
      "step": 536
    },
    {
      "epoch": 0.7617021276595745,
      "grad_norm": 1.0248510837554932,
      "learning_rate": 0.00015292978208232447,
      "loss": 1.6966,
      "step": 537
    },
    {
      "epoch": 0.7631205673758865,
      "grad_norm": 0.9485064148902893,
      "learning_rate": 0.00015283292978208232,
      "loss": 1.5582,
      "step": 538
    },
    {
      "epoch": 0.7645390070921986,
      "grad_norm": 0.9537084102630615,
      "learning_rate": 0.0001527360774818402,
      "loss": 1.6908,
      "step": 539
    },
    {
      "epoch": 0.7659574468085106,
      "grad_norm": 0.9340246915817261,
      "learning_rate": 0.00015263922518159807,
      "loss": 1.8275,
      "step": 540
    },
    {
      "epoch": 0.7673758865248227,
      "grad_norm": 1.0218333005905151,
      "learning_rate": 0.00015254237288135592,
      "loss": 2.0469,
      "step": 541
    },
    {
      "epoch": 0.7687943262411348,
      "grad_norm": 0.93470299243927,
      "learning_rate": 0.0001524455205811138,
      "loss": 1.7582,
      "step": 542
    },
    {
      "epoch": 0.7702127659574468,
      "grad_norm": 0.9361689686775208,
      "learning_rate": 0.00015234866828087168,
      "loss": 1.7492,
      "step": 543
    },
    {
      "epoch": 0.7716312056737589,
      "grad_norm": 1.0975946187973022,
      "learning_rate": 0.00015225181598062953,
      "loss": 1.9672,
      "step": 544
    },
    {
      "epoch": 0.7730496453900709,
      "grad_norm": 1.0108466148376465,
      "learning_rate": 0.0001521549636803874,
      "loss": 1.9047,
      "step": 545
    },
    {
      "epoch": 0.774468085106383,
      "grad_norm": 1.0511740446090698,
      "learning_rate": 0.0001520581113801453,
      "loss": 1.9796,
      "step": 546
    },
    {
      "epoch": 0.775886524822695,
      "grad_norm": 0.9763832688331604,
      "learning_rate": 0.00015196125907990314,
      "loss": 1.7714,
      "step": 547
    },
    {
      "epoch": 0.7773049645390071,
      "grad_norm": 0.938847005367279,
      "learning_rate": 0.00015186440677966102,
      "loss": 1.8183,
      "step": 548
    },
    {
      "epoch": 0.7787234042553192,
      "grad_norm": 0.8810025453567505,
      "learning_rate": 0.0001517675544794189,
      "loss": 1.7477,
      "step": 549
    },
    {
      "epoch": 0.7801418439716312,
      "grad_norm": 0.9073086380958557,
      "learning_rate": 0.00015167070217917677,
      "loss": 1.7315,
      "step": 550
    },
    {
      "epoch": 0.7815602836879433,
      "grad_norm": 0.9286612868309021,
      "learning_rate": 0.00015157384987893465,
      "loss": 1.7977,
      "step": 551
    },
    {
      "epoch": 0.7829787234042553,
      "grad_norm": 0.9796596765518188,
      "learning_rate": 0.0001514769975786925,
      "loss": 2.112,
      "step": 552
    },
    {
      "epoch": 0.7843971631205674,
      "grad_norm": 0.9236878156661987,
      "learning_rate": 0.00015138014527845038,
      "loss": 1.9134,
      "step": 553
    },
    {
      "epoch": 0.7858156028368795,
      "grad_norm": 0.9988068342208862,
      "learning_rate": 0.00015128329297820826,
      "loss": 1.8176,
      "step": 554
    },
    {
      "epoch": 0.7872340425531915,
      "grad_norm": 0.9496425986289978,
      "learning_rate": 0.0001511864406779661,
      "loss": 1.7943,
      "step": 555
    },
    {
      "epoch": 0.7886524822695036,
      "grad_norm": 0.9448116421699524,
      "learning_rate": 0.000151089588377724,
      "loss": 1.8102,
      "step": 556
    },
    {
      "epoch": 0.7900709219858156,
      "grad_norm": 0.9122583866119385,
      "learning_rate": 0.00015099273607748186,
      "loss": 1.7606,
      "step": 557
    },
    {
      "epoch": 0.7914893617021277,
      "grad_norm": 0.8941739201545715,
      "learning_rate": 0.00015089588377723972,
      "loss": 1.873,
      "step": 558
    },
    {
      "epoch": 0.7929078014184398,
      "grad_norm": 0.9107263684272766,
      "learning_rate": 0.0001507990314769976,
      "loss": 1.6111,
      "step": 559
    },
    {
      "epoch": 0.7943262411347518,
      "grad_norm": 0.9208337068557739,
      "learning_rate": 0.00015070217917675547,
      "loss": 1.7329,
      "step": 560
    },
    {
      "epoch": 0.7957446808510639,
      "grad_norm": 0.9259641170501709,
      "learning_rate": 0.00015060532687651332,
      "loss": 1.8765,
      "step": 561
    },
    {
      "epoch": 0.7971631205673759,
      "grad_norm": 1.0317492485046387,
      "learning_rate": 0.0001505084745762712,
      "loss": 1.8607,
      "step": 562
    },
    {
      "epoch": 0.798581560283688,
      "grad_norm": 0.9023297429084778,
      "learning_rate": 0.00015041162227602908,
      "loss": 1.489,
      "step": 563
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.8855882883071899,
      "learning_rate": 0.00015031476997578693,
      "loss": 1.773,
      "step": 564
    },
    {
      "epoch": 0.8014184397163121,
      "grad_norm": 0.9644176959991455,
      "learning_rate": 0.0001502179176755448,
      "loss": 1.8337,
      "step": 565
    },
    {
      "epoch": 0.8028368794326242,
      "grad_norm": 0.9933615922927856,
      "learning_rate": 0.00015012106537530266,
      "loss": 1.8721,
      "step": 566
    },
    {
      "epoch": 0.8042553191489362,
      "grad_norm": 0.9730909466743469,
      "learning_rate": 0.00015002421307506054,
      "loss": 1.5937,
      "step": 567
    },
    {
      "epoch": 0.8056737588652483,
      "grad_norm": 0.9592195749282837,
      "learning_rate": 0.00014992736077481842,
      "loss": 1.9941,
      "step": 568
    },
    {
      "epoch": 0.8070921985815603,
      "grad_norm": 0.9991093277931213,
      "learning_rate": 0.00014983050847457627,
      "loss": 1.8707,
      "step": 569
    },
    {
      "epoch": 0.8085106382978723,
      "grad_norm": 0.9750028252601624,
      "learning_rate": 0.00014973365617433414,
      "loss": 1.8025,
      "step": 570
    },
    {
      "epoch": 0.8099290780141843,
      "grad_norm": 1.0412169694900513,
      "learning_rate": 0.00014963680387409202,
      "loss": 1.7719,
      "step": 571
    },
    {
      "epoch": 0.8113475177304964,
      "grad_norm": 1.0398106575012207,
      "learning_rate": 0.00014953995157384987,
      "loss": 1.661,
      "step": 572
    },
    {
      "epoch": 0.8127659574468085,
      "grad_norm": 0.8933267593383789,
      "learning_rate": 0.00014944309927360775,
      "loss": 1.8424,
      "step": 573
    },
    {
      "epoch": 0.8141843971631205,
      "grad_norm": 0.9653945565223694,
      "learning_rate": 0.00014934624697336563,
      "loss": 1.713,
      "step": 574
    },
    {
      "epoch": 0.8156028368794326,
      "grad_norm": 1.0151563882827759,
      "learning_rate": 0.00014924939467312348,
      "loss": 1.6218,
      "step": 575
    },
    {
      "epoch": 0.8170212765957446,
      "grad_norm": 0.9580346941947937,
      "learning_rate": 0.00014915254237288136,
      "loss": 1.8652,
      "step": 576
    },
    {
      "epoch": 0.8184397163120567,
      "grad_norm": 1.0817276239395142,
      "learning_rate": 0.00014905569007263924,
      "loss": 1.9472,
      "step": 577
    },
    {
      "epoch": 0.8198581560283688,
      "grad_norm": 0.9456907510757446,
      "learning_rate": 0.0001489588377723971,
      "loss": 1.7959,
      "step": 578
    },
    {
      "epoch": 0.8212765957446808,
      "grad_norm": 0.9884796738624573,
      "learning_rate": 0.00014886198547215497,
      "loss": 1.8121,
      "step": 579
    },
    {
      "epoch": 0.8226950354609929,
      "grad_norm": 1.0423191785812378,
      "learning_rate": 0.00014876513317191282,
      "loss": 1.7294,
      "step": 580
    },
    {
      "epoch": 0.8241134751773049,
      "grad_norm": 0.8762698769569397,
      "learning_rate": 0.0001486682808716707,
      "loss": 1.8848,
      "step": 581
    },
    {
      "epoch": 0.825531914893617,
      "grad_norm": 0.882957398891449,
      "learning_rate": 0.00014857142857142857,
      "loss": 1.6479,
      "step": 582
    },
    {
      "epoch": 0.826950354609929,
      "grad_norm": 0.9469701647758484,
      "learning_rate": 0.00014847457627118645,
      "loss": 1.5607,
      "step": 583
    },
    {
      "epoch": 0.8283687943262411,
      "grad_norm": 0.9412171840667725,
      "learning_rate": 0.0001483777239709443,
      "loss": 1.8733,
      "step": 584
    },
    {
      "epoch": 0.8297872340425532,
      "grad_norm": 1.0088787078857422,
      "learning_rate": 0.00014828087167070218,
      "loss": 1.8975,
      "step": 585
    },
    {
      "epoch": 0.8312056737588652,
      "grad_norm": 1.1250557899475098,
      "learning_rate": 0.00014818401937046006,
      "loss": 1.8192,
      "step": 586
    },
    {
      "epoch": 0.8326241134751773,
      "grad_norm": 0.9558066129684448,
      "learning_rate": 0.00014808716707021794,
      "loss": 1.7909,
      "step": 587
    },
    {
      "epoch": 0.8340425531914893,
      "grad_norm": 0.979161262512207,
      "learning_rate": 0.0001479903147699758,
      "loss": 1.8969,
      "step": 588
    },
    {
      "epoch": 0.8354609929078014,
      "grad_norm": 1.0074257850646973,
      "learning_rate": 0.00014789346246973367,
      "loss": 2.0733,
      "step": 589
    },
    {
      "epoch": 0.8368794326241135,
      "grad_norm": 0.9141056537628174,
      "learning_rate": 0.00014779661016949154,
      "loss": 1.7769,
      "step": 590
    },
    {
      "epoch": 0.8382978723404255,
      "grad_norm": 0.8834106922149658,
      "learning_rate": 0.00014769975786924942,
      "loss": 1.6314,
      "step": 591
    },
    {
      "epoch": 0.8397163120567376,
      "grad_norm": 0.9422737956047058,
      "learning_rate": 0.00014760290556900727,
      "loss": 1.8027,
      "step": 592
    },
    {
      "epoch": 0.8411347517730496,
      "grad_norm": 0.8941066265106201,
      "learning_rate": 0.00014750605326876515,
      "loss": 1.8671,
      "step": 593
    },
    {
      "epoch": 0.8425531914893617,
      "grad_norm": 0.9444699883460999,
      "learning_rate": 0.00014740920096852303,
      "loss": 2.0242,
      "step": 594
    },
    {
      "epoch": 0.8439716312056738,
      "grad_norm": 0.8961655497550964,
      "learning_rate": 0.00014731234866828088,
      "loss": 1.9358,
      "step": 595
    },
    {
      "epoch": 0.8453900709219858,
      "grad_norm": 0.8932142853736877,
      "learning_rate": 0.00014721549636803876,
      "loss": 2.0009,
      "step": 596
    },
    {
      "epoch": 0.8468085106382979,
      "grad_norm": 0.9080725908279419,
      "learning_rate": 0.0001471186440677966,
      "loss": 1.702,
      "step": 597
    },
    {
      "epoch": 0.8482269503546099,
      "grad_norm": 0.9869876503944397,
      "learning_rate": 0.0001470217917675545,
      "loss": 1.8054,
      "step": 598
    },
    {
      "epoch": 0.849645390070922,
      "grad_norm": 0.9595502614974976,
      "learning_rate": 0.00014692493946731237,
      "loss": 1.854,
      "step": 599
    },
    {
      "epoch": 0.851063829787234,
      "grad_norm": 0.975337028503418,
      "learning_rate": 0.00014682808716707022,
      "loss": 2.0526,
      "step": 600
    },
    {
      "epoch": 0.8524822695035461,
      "grad_norm": 0.9461711645126343,
      "learning_rate": 0.0001467312348668281,
      "loss": 1.7712,
      "step": 601
    },
    {
      "epoch": 0.8539007092198582,
      "grad_norm": 1.002305507659912,
      "learning_rate": 0.00014663438256658597,
      "loss": 1.9774,
      "step": 602
    },
    {
      "epoch": 0.8553191489361702,
      "grad_norm": 0.9175607562065125,
      "learning_rate": 0.00014653753026634382,
      "loss": 1.7826,
      "step": 603
    },
    {
      "epoch": 0.8567375886524823,
      "grad_norm": 0.9470594525337219,
      "learning_rate": 0.0001464406779661017,
      "loss": 1.805,
      "step": 604
    },
    {
      "epoch": 0.8581560283687943,
      "grad_norm": 0.9285657405853271,
      "learning_rate": 0.00014634382566585958,
      "loss": 1.8489,
      "step": 605
    },
    {
      "epoch": 0.8595744680851064,
      "grad_norm": 1.013704776763916,
      "learning_rate": 0.00014624697336561743,
      "loss": 1.7888,
      "step": 606
    },
    {
      "epoch": 0.8609929078014185,
      "grad_norm": 1.061592936515808,
      "learning_rate": 0.0001461501210653753,
      "loss": 1.8539,
      "step": 607
    },
    {
      "epoch": 0.8624113475177305,
      "grad_norm": 0.9778721928596497,
      "learning_rate": 0.0001460532687651332,
      "loss": 2.0674,
      "step": 608
    },
    {
      "epoch": 0.8638297872340426,
      "grad_norm": 1.0700124502182007,
      "learning_rate": 0.00014595641646489104,
      "loss": 2.0469,
      "step": 609
    },
    {
      "epoch": 0.8652482269503546,
      "grad_norm": 0.932140052318573,
      "learning_rate": 0.00014585956416464892,
      "loss": 1.832,
      "step": 610
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 0.8720954060554504,
      "learning_rate": 0.00014576271186440677,
      "loss": 1.5422,
      "step": 611
    },
    {
      "epoch": 0.8680851063829788,
      "grad_norm": 0.9799209833145142,
      "learning_rate": 0.00014566585956416465,
      "loss": 1.8203,
      "step": 612
    },
    {
      "epoch": 0.8695035460992908,
      "grad_norm": 0.9990918636322021,
      "learning_rate": 0.00014556900726392252,
      "loss": 1.7772,
      "step": 613
    },
    {
      "epoch": 0.8709219858156029,
      "grad_norm": 0.9366308450698853,
      "learning_rate": 0.00014547215496368038,
      "loss": 1.9041,
      "step": 614
    },
    {
      "epoch": 0.8723404255319149,
      "grad_norm": 0.9561300277709961,
      "learning_rate": 0.00014537530266343825,
      "loss": 2.0094,
      "step": 615
    },
    {
      "epoch": 0.873758865248227,
      "grad_norm": 0.9853776097297668,
      "learning_rate": 0.00014527845036319613,
      "loss": 1.853,
      "step": 616
    },
    {
      "epoch": 0.875177304964539,
      "grad_norm": 0.9627854228019714,
      "learning_rate": 0.00014518159806295398,
      "loss": 1.7166,
      "step": 617
    },
    {
      "epoch": 0.8765957446808511,
      "grad_norm": 0.9175072908401489,
      "learning_rate": 0.00014508474576271186,
      "loss": 1.9452,
      "step": 618
    },
    {
      "epoch": 0.8780141843971632,
      "grad_norm": 0.9186393022537231,
      "learning_rate": 0.00014498789346246974,
      "loss": 1.9466,
      "step": 619
    },
    {
      "epoch": 0.8794326241134752,
      "grad_norm": 0.975797712802887,
      "learning_rate": 0.0001448910411622276,
      "loss": 1.8509,
      "step": 620
    },
    {
      "epoch": 0.8808510638297873,
      "grad_norm": 0.9033548831939697,
      "learning_rate": 0.00014479418886198547,
      "loss": 1.6784,
      "step": 621
    },
    {
      "epoch": 0.8822695035460993,
      "grad_norm": 2.0145516395568848,
      "learning_rate": 0.00014469733656174335,
      "loss": 1.9831,
      "step": 622
    },
    {
      "epoch": 0.8836879432624114,
      "grad_norm": 0.9748150110244751,
      "learning_rate": 0.00014460048426150122,
      "loss": 1.8112,
      "step": 623
    },
    {
      "epoch": 0.8851063829787233,
      "grad_norm": 0.928269624710083,
      "learning_rate": 0.0001445036319612591,
      "loss": 1.8436,
      "step": 624
    },
    {
      "epoch": 0.8865248226950354,
      "grad_norm": 1.0261791944503784,
      "learning_rate": 0.00014440677966101695,
      "loss": 1.6131,
      "step": 625
    },
    {
      "epoch": 0.8879432624113475,
      "grad_norm": 0.9150677919387817,
      "learning_rate": 0.00014430992736077483,
      "loss": 1.711,
      "step": 626
    },
    {
      "epoch": 0.8893617021276595,
      "grad_norm": 0.9695219993591309,
      "learning_rate": 0.0001442130750605327,
      "loss": 1.6629,
      "step": 627
    },
    {
      "epoch": 0.8907801418439716,
      "grad_norm": 1.0402165651321411,
      "learning_rate": 0.00014411622276029056,
      "loss": 1.8659,
      "step": 628
    },
    {
      "epoch": 0.8921985815602836,
      "grad_norm": 0.9994091391563416,
      "learning_rate": 0.00014401937046004844,
      "loss": 1.8511,
      "step": 629
    },
    {
      "epoch": 0.8936170212765957,
      "grad_norm": 0.9416429996490479,
      "learning_rate": 0.00014392251815980632,
      "loss": 1.541,
      "step": 630
    },
    {
      "epoch": 0.8950354609929078,
      "grad_norm": 0.9779669642448425,
      "learning_rate": 0.00014382566585956417,
      "loss": 1.4849,
      "step": 631
    },
    {
      "epoch": 0.8964539007092198,
      "grad_norm": 0.9192597270011902,
      "learning_rate": 0.00014372881355932205,
      "loss": 1.7296,
      "step": 632
    },
    {
      "epoch": 0.8978723404255319,
      "grad_norm": 0.9987861514091492,
      "learning_rate": 0.00014363196125907992,
      "loss": 1.7969,
      "step": 633
    },
    {
      "epoch": 0.8992907801418439,
      "grad_norm": 0.9664397239685059,
      "learning_rate": 0.00014353510895883778,
      "loss": 1.5772,
      "step": 634
    },
    {
      "epoch": 0.900709219858156,
      "grad_norm": 0.9543797969818115,
      "learning_rate": 0.00014343825665859565,
      "loss": 1.9208,
      "step": 635
    },
    {
      "epoch": 0.902127659574468,
      "grad_norm": 1.030147910118103,
      "learning_rate": 0.00014334140435835353,
      "loss": 1.8075,
      "step": 636
    },
    {
      "epoch": 0.9035460992907801,
      "grad_norm": 1.0873552560806274,
      "learning_rate": 0.00014324455205811138,
      "loss": 1.8447,
      "step": 637
    },
    {
      "epoch": 0.9049645390070922,
      "grad_norm": 0.9692791104316711,
      "learning_rate": 0.00014314769975786926,
      "loss": 1.7809,
      "step": 638
    },
    {
      "epoch": 0.9063829787234042,
      "grad_norm": 0.918589174747467,
      "learning_rate": 0.00014305084745762714,
      "loss": 1.8654,
      "step": 639
    },
    {
      "epoch": 0.9078014184397163,
      "grad_norm": 0.9659599661827087,
      "learning_rate": 0.000142953995157385,
      "loss": 1.8804,
      "step": 640
    },
    {
      "epoch": 0.9092198581560283,
      "grad_norm": 0.9876123666763306,
      "learning_rate": 0.00014285714285714287,
      "loss": 1.9563,
      "step": 641
    },
    {
      "epoch": 0.9106382978723404,
      "grad_norm": 1.0985149145126343,
      "learning_rate": 0.00014276029055690075,
      "loss": 1.9155,
      "step": 642
    },
    {
      "epoch": 0.9120567375886525,
      "grad_norm": 0.9651275277137756,
      "learning_rate": 0.0001426634382566586,
      "loss": 1.7785,
      "step": 643
    },
    {
      "epoch": 0.9134751773049645,
      "grad_norm": 0.9180082082748413,
      "learning_rate": 0.00014256658595641648,
      "loss": 1.8405,
      "step": 644
    },
    {
      "epoch": 0.9148936170212766,
      "grad_norm": 1.0275384187698364,
      "learning_rate": 0.00014246973365617433,
      "loss": 2.048,
      "step": 645
    },
    {
      "epoch": 0.9163120567375886,
      "grad_norm": 1.0340267419815063,
      "learning_rate": 0.0001423728813559322,
      "loss": 1.6341,
      "step": 646
    },
    {
      "epoch": 0.9177304964539007,
      "grad_norm": 0.858508288860321,
      "learning_rate": 0.00014227602905569008,
      "loss": 1.5195,
      "step": 647
    },
    {
      "epoch": 0.9191489361702128,
      "grad_norm": 0.938224732875824,
      "learning_rate": 0.00014217917675544793,
      "loss": 1.6986,
      "step": 648
    },
    {
      "epoch": 0.9205673758865248,
      "grad_norm": 0.9428529739379883,
      "learning_rate": 0.0001420823244552058,
      "loss": 1.7616,
      "step": 649
    },
    {
      "epoch": 0.9219858156028369,
      "grad_norm": 0.984078049659729,
      "learning_rate": 0.0001419854721549637,
      "loss": 1.9531,
      "step": 650
    },
    {
      "epoch": 0.9234042553191489,
      "grad_norm": 0.9620499610900879,
      "learning_rate": 0.00014188861985472154,
      "loss": 1.8108,
      "step": 651
    },
    {
      "epoch": 0.924822695035461,
      "grad_norm": 1.060816764831543,
      "learning_rate": 0.00014179176755447942,
      "loss": 1.8468,
      "step": 652
    },
    {
      "epoch": 0.926241134751773,
      "grad_norm": 0.9061930179595947,
      "learning_rate": 0.0001416949152542373,
      "loss": 1.708,
      "step": 653
    },
    {
      "epoch": 0.9276595744680851,
      "grad_norm": 0.9653571248054504,
      "learning_rate": 0.00014159806295399515,
      "loss": 1.8584,
      "step": 654
    },
    {
      "epoch": 0.9290780141843972,
      "grad_norm": 0.974460244178772,
      "learning_rate": 0.00014150121065375303,
      "loss": 2.0488,
      "step": 655
    },
    {
      "epoch": 0.9304964539007092,
      "grad_norm": 0.9710502028465271,
      "learning_rate": 0.0001414043583535109,
      "loss": 1.9374,
      "step": 656
    },
    {
      "epoch": 0.9319148936170213,
      "grad_norm": 0.9244868159294128,
      "learning_rate": 0.00014130750605326876,
      "loss": 1.7593,
      "step": 657
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 1.024077296257019,
      "learning_rate": 0.00014121065375302663,
      "loss": 1.8946,
      "step": 658
    },
    {
      "epoch": 0.9347517730496454,
      "grad_norm": 0.9225230813026428,
      "learning_rate": 0.0001411138014527845,
      "loss": 1.8369,
      "step": 659
    },
    {
      "epoch": 0.9361702127659575,
      "grad_norm": 0.9084619879722595,
      "learning_rate": 0.0001410169491525424,
      "loss": 1.7481,
      "step": 660
    },
    {
      "epoch": 0.9375886524822695,
      "grad_norm": 0.8650357127189636,
      "learning_rate": 0.00014092009685230027,
      "loss": 1.7555,
      "step": 661
    },
    {
      "epoch": 0.9390070921985816,
      "grad_norm": 0.9914340376853943,
      "learning_rate": 0.00014082324455205812,
      "loss": 1.8704,
      "step": 662
    },
    {
      "epoch": 0.9404255319148936,
      "grad_norm": 0.9047065377235413,
      "learning_rate": 0.000140726392251816,
      "loss": 1.7808,
      "step": 663
    },
    {
      "epoch": 0.9418439716312057,
      "grad_norm": 0.9416099786758423,
      "learning_rate": 0.00014062953995157387,
      "loss": 1.9871,
      "step": 664
    },
    {
      "epoch": 0.9432624113475178,
      "grad_norm": 1.0570451021194458,
      "learning_rate": 0.00014053268765133173,
      "loss": 1.9121,
      "step": 665
    },
    {
      "epoch": 0.9446808510638298,
      "grad_norm": 0.9373465776443481,
      "learning_rate": 0.0001404358353510896,
      "loss": 1.9358,
      "step": 666
    },
    {
      "epoch": 0.9460992907801419,
      "grad_norm": 0.897957444190979,
      "learning_rate": 0.00014033898305084748,
      "loss": 1.6269,
      "step": 667
    },
    {
      "epoch": 0.9475177304964539,
      "grad_norm": 0.9292532205581665,
      "learning_rate": 0.00014024213075060533,
      "loss": 1.8592,
      "step": 668
    },
    {
      "epoch": 0.948936170212766,
      "grad_norm": 1.0342576503753662,
      "learning_rate": 0.0001401452784503632,
      "loss": 1.5566,
      "step": 669
    },
    {
      "epoch": 0.950354609929078,
      "grad_norm": 0.9260356426239014,
      "learning_rate": 0.0001400484261501211,
      "loss": 1.8768,
      "step": 670
    },
    {
      "epoch": 0.9517730496453901,
      "grad_norm": 0.9853015542030334,
      "learning_rate": 0.00013995157384987894,
      "loss": 1.7743,
      "step": 671
    },
    {
      "epoch": 0.9531914893617022,
      "grad_norm": 1.0748721361160278,
      "learning_rate": 0.00013985472154963682,
      "loss": 2.079,
      "step": 672
    },
    {
      "epoch": 0.9546099290780142,
      "grad_norm": 0.9573856592178345,
      "learning_rate": 0.0001397578692493947,
      "loss": 1.675,
      "step": 673
    },
    {
      "epoch": 0.9560283687943263,
      "grad_norm": 0.8905030488967896,
      "learning_rate": 0.00013966101694915255,
      "loss": 1.7418,
      "step": 674
    },
    {
      "epoch": 0.9574468085106383,
      "grad_norm": 0.9220945835113525,
      "learning_rate": 0.00013956416464891043,
      "loss": 1.9879,
      "step": 675
    },
    {
      "epoch": 0.9588652482269504,
      "grad_norm": 1.0226384401321411,
      "learning_rate": 0.00013946731234866828,
      "loss": 2.0695,
      "step": 676
    },
    {
      "epoch": 0.9602836879432625,
      "grad_norm": 0.9441185593605042,
      "learning_rate": 0.00013937046004842615,
      "loss": 1.9575,
      "step": 677
    },
    {
      "epoch": 0.9617021276595744,
      "grad_norm": 0.8684213757514954,
      "learning_rate": 0.00013927360774818403,
      "loss": 1.8215,
      "step": 678
    },
    {
      "epoch": 0.9631205673758865,
      "grad_norm": 0.9086737036705017,
      "learning_rate": 0.00013917675544794188,
      "loss": 1.7301,
      "step": 679
    },
    {
      "epoch": 0.9645390070921985,
      "grad_norm": 0.895923912525177,
      "learning_rate": 0.00013907990314769976,
      "loss": 1.6081,
      "step": 680
    },
    {
      "epoch": 0.9659574468085106,
      "grad_norm": 0.9483720064163208,
      "learning_rate": 0.00013898305084745764,
      "loss": 1.5901,
      "step": 681
    },
    {
      "epoch": 0.9673758865248226,
      "grad_norm": 1.0106722116470337,
      "learning_rate": 0.0001388861985472155,
      "loss": 1.7852,
      "step": 682
    },
    {
      "epoch": 0.9687943262411347,
      "grad_norm": 1.0110578536987305,
      "learning_rate": 0.00013878934624697337,
      "loss": 1.8604,
      "step": 683
    },
    {
      "epoch": 0.9702127659574468,
      "grad_norm": 0.9774700403213501,
      "learning_rate": 0.00013869249394673125,
      "loss": 1.8243,
      "step": 684
    },
    {
      "epoch": 0.9716312056737588,
      "grad_norm": 0.9234907031059265,
      "learning_rate": 0.0001385956416464891,
      "loss": 1.5916,
      "step": 685
    },
    {
      "epoch": 0.9730496453900709,
      "grad_norm": 1.059147596359253,
      "learning_rate": 0.00013849878934624698,
      "loss": 1.843,
      "step": 686
    },
    {
      "epoch": 0.9744680851063829,
      "grad_norm": 0.9501283764839172,
      "learning_rate": 0.00013840193704600485,
      "loss": 1.5896,
      "step": 687
    },
    {
      "epoch": 0.975886524822695,
      "grad_norm": 0.925462007522583,
      "learning_rate": 0.0001383050847457627,
      "loss": 1.8908,
      "step": 688
    },
    {
      "epoch": 0.9773049645390071,
      "grad_norm": 1.0491056442260742,
      "learning_rate": 0.00013820823244552058,
      "loss": 1.7496,
      "step": 689
    },
    {
      "epoch": 0.9787234042553191,
      "grad_norm": 0.9669529795646667,
      "learning_rate": 0.00013811138014527843,
      "loss": 1.8373,
      "step": 690
    },
    {
      "epoch": 0.9801418439716312,
      "grad_norm": 1.0195949077606201,
      "learning_rate": 0.0001380145278450363,
      "loss": 1.9521,
      "step": 691
    },
    {
      "epoch": 0.9815602836879432,
      "grad_norm": 1.0332221984863281,
      "learning_rate": 0.0001379176755447942,
      "loss": 1.8843,
      "step": 692
    },
    {
      "epoch": 0.9829787234042553,
      "grad_norm": 0.9786433577537537,
      "learning_rate": 0.00013782082324455204,
      "loss": 1.6895,
      "step": 693
    },
    {
      "epoch": 0.9843971631205674,
      "grad_norm": 0.9934961199760437,
      "learning_rate": 0.00013772397094430992,
      "loss": 1.9579,
      "step": 694
    },
    {
      "epoch": 0.9858156028368794,
      "grad_norm": 0.856977105140686,
      "learning_rate": 0.0001376271186440678,
      "loss": 1.6905,
      "step": 695
    },
    {
      "epoch": 0.9872340425531915,
      "grad_norm": 0.9279069304466248,
      "learning_rate": 0.00013753026634382568,
      "loss": 1.6489,
      "step": 696
    },
    {
      "epoch": 0.9886524822695035,
      "grad_norm": 0.9529761075973511,
      "learning_rate": 0.00013743341404358355,
      "loss": 1.6892,
      "step": 697
    },
    {
      "epoch": 0.9900709219858156,
      "grad_norm": 1.0541527271270752,
      "learning_rate": 0.0001373365617433414,
      "loss": 1.9965,
      "step": 698
    },
    {
      "epoch": 0.9914893617021276,
      "grad_norm": 0.9821218848228455,
      "learning_rate": 0.00013723970944309928,
      "loss": 1.9619,
      "step": 699
    },
    {
      "epoch": 0.9929078014184397,
      "grad_norm": 0.957706868648529,
      "learning_rate": 0.00013714285714285716,
      "loss": 1.8005,
      "step": 700
    },
    {
      "epoch": 0.9943262411347518,
      "grad_norm": 1.0196388959884644,
      "learning_rate": 0.00013704600484261504,
      "loss": 1.7739,
      "step": 701
    },
    {
      "epoch": 0.9957446808510638,
      "grad_norm": 0.9991929531097412,
      "learning_rate": 0.0001369491525423729,
      "loss": 1.7795,
      "step": 702
    },
    {
      "epoch": 0.9971631205673759,
      "grad_norm": 0.9795322418212891,
      "learning_rate": 0.00013685230024213077,
      "loss": 1.9412,
      "step": 703
    },
    {
      "epoch": 0.9985815602836879,
      "grad_norm": 0.9208971261978149,
      "learning_rate": 0.00013675544794188865,
      "loss": 1.851,
      "step": 704
    },
    {
      "epoch": 1.0,
      "grad_norm": 1.0984488725662231,
      "learning_rate": 0.0001366585956416465,
      "loss": 1.5486,
      "step": 705
    },
    {
      "epoch": 1.001418439716312,
      "grad_norm": 0.8669431805610657,
      "learning_rate": 0.00013656174334140438,
      "loss": 1.5551,
      "step": 706
    },
    {
      "epoch": 1.0028368794326241,
      "grad_norm": 0.9462072849273682,
      "learning_rate": 0.00013646489104116223,
      "loss": 1.4165,
      "step": 707
    },
    {
      "epoch": 1.004255319148936,
      "grad_norm": 0.9423401355743408,
      "learning_rate": 0.0001363680387409201,
      "loss": 1.7539,
      "step": 708
    },
    {
      "epoch": 1.0056737588652482,
      "grad_norm": 0.8739091753959656,
      "learning_rate": 0.00013627118644067798,
      "loss": 1.4029,
      "step": 709
    },
    {
      "epoch": 1.0070921985815602,
      "grad_norm": 0.9132165312767029,
      "learning_rate": 0.00013617433414043583,
      "loss": 1.4626,
      "step": 710
    },
    {
      "epoch": 1.0085106382978724,
      "grad_norm": 0.9093713164329529,
      "learning_rate": 0.0001360774818401937,
      "loss": 1.8219,
      "step": 711
    },
    {
      "epoch": 1.0099290780141843,
      "grad_norm": 0.9057936668395996,
      "learning_rate": 0.0001359806295399516,
      "loss": 1.6615,
      "step": 712
    },
    {
      "epoch": 1.0113475177304965,
      "grad_norm": 0.9246616959571838,
      "learning_rate": 0.00013588377723970944,
      "loss": 1.5746,
      "step": 713
    },
    {
      "epoch": 1.0127659574468084,
      "grad_norm": 1.0594182014465332,
      "learning_rate": 0.00013578692493946732,
      "loss": 1.6838,
      "step": 714
    },
    {
      "epoch": 1.0141843971631206,
      "grad_norm": 1.0186001062393188,
      "learning_rate": 0.0001356900726392252,
      "loss": 1.8892,
      "step": 715
    },
    {
      "epoch": 1.0156028368794325,
      "grad_norm": 0.9838067889213562,
      "learning_rate": 0.00013559322033898305,
      "loss": 1.5421,
      "step": 716
    },
    {
      "epoch": 1.0170212765957447,
      "grad_norm": 0.9654032588005066,
      "learning_rate": 0.00013549636803874093,
      "loss": 1.6379,
      "step": 717
    },
    {
      "epoch": 1.0184397163120567,
      "grad_norm": 0.9566038250923157,
      "learning_rate": 0.0001353995157384988,
      "loss": 1.5206,
      "step": 718
    },
    {
      "epoch": 1.0198581560283688,
      "grad_norm": 1.0163044929504395,
      "learning_rate": 0.00013530266343825666,
      "loss": 1.7308,
      "step": 719
    },
    {
      "epoch": 1.0212765957446808,
      "grad_norm": 1.0072609186172485,
      "learning_rate": 0.00013520581113801453,
      "loss": 1.5444,
      "step": 720
    },
    {
      "epoch": 1.022695035460993,
      "grad_norm": 0.954924464225769,
      "learning_rate": 0.00013510895883777239,
      "loss": 1.557,
      "step": 721
    },
    {
      "epoch": 1.0241134751773049,
      "grad_norm": 1.01553213596344,
      "learning_rate": 0.00013501210653753026,
      "loss": 1.5773,
      "step": 722
    },
    {
      "epoch": 1.025531914893617,
      "grad_norm": 1.0861798524856567,
      "learning_rate": 0.00013491525423728814,
      "loss": 1.8833,
      "step": 723
    },
    {
      "epoch": 1.026950354609929,
      "grad_norm": 1.0201469659805298,
      "learning_rate": 0.000134818401937046,
      "loss": 1.6846,
      "step": 724
    },
    {
      "epoch": 1.0283687943262412,
      "grad_norm": 1.0009260177612305,
      "learning_rate": 0.00013472154963680387,
      "loss": 1.6597,
      "step": 725
    },
    {
      "epoch": 1.0297872340425531,
      "grad_norm": 0.9549226760864258,
      "learning_rate": 0.00013462469733656175,
      "loss": 1.5257,
      "step": 726
    },
    {
      "epoch": 1.0312056737588653,
      "grad_norm": 1.0315558910369873,
      "learning_rate": 0.0001345278450363196,
      "loss": 1.7265,
      "step": 727
    },
    {
      "epoch": 1.0326241134751772,
      "grad_norm": 1.0820245742797852,
      "learning_rate": 0.00013443099273607748,
      "loss": 1.3508,
      "step": 728
    },
    {
      "epoch": 1.0340425531914894,
      "grad_norm": 0.9885953664779663,
      "learning_rate": 0.00013433414043583536,
      "loss": 1.8148,
      "step": 729
    },
    {
      "epoch": 1.0354609929078014,
      "grad_norm": 0.99950110912323,
      "learning_rate": 0.0001342372881355932,
      "loss": 1.5989,
      "step": 730
    },
    {
      "epoch": 1.0368794326241135,
      "grad_norm": 1.0264092683792114,
      "learning_rate": 0.00013414043583535109,
      "loss": 1.5121,
      "step": 731
    },
    {
      "epoch": 1.0382978723404255,
      "grad_norm": 0.9720777273178101,
      "learning_rate": 0.00013404358353510896,
      "loss": 1.5167,
      "step": 732
    },
    {
      "epoch": 1.0397163120567376,
      "grad_norm": 0.9726420044898987,
      "learning_rate": 0.00013394673123486684,
      "loss": 1.5827,
      "step": 733
    },
    {
      "epoch": 1.0411347517730496,
      "grad_norm": 1.089019775390625,
      "learning_rate": 0.00013384987893462472,
      "loss": 1.6869,
      "step": 734
    },
    {
      "epoch": 1.0425531914893618,
      "grad_norm": 1.0289783477783203,
      "learning_rate": 0.00013375302663438257,
      "loss": 1.5885,
      "step": 735
    },
    {
      "epoch": 1.0439716312056737,
      "grad_norm": 1.0663580894470215,
      "learning_rate": 0.00013365617433414045,
      "loss": 1.641,
      "step": 736
    },
    {
      "epoch": 1.0453900709219859,
      "grad_norm": 1.0268943309783936,
      "learning_rate": 0.00013355932203389833,
      "loss": 1.5581,
      "step": 737
    },
    {
      "epoch": 1.0468085106382978,
      "grad_norm": 1.0885965824127197,
      "learning_rate": 0.00013346246973365618,
      "loss": 1.6535,
      "step": 738
    },
    {
      "epoch": 1.04822695035461,
      "grad_norm": 1.0613908767700195,
      "learning_rate": 0.00013336561743341406,
      "loss": 1.8136,
      "step": 739
    },
    {
      "epoch": 1.049645390070922,
      "grad_norm": 1.0064640045166016,
      "learning_rate": 0.00013326876513317193,
      "loss": 1.6304,
      "step": 740
    },
    {
      "epoch": 1.0510638297872341,
      "grad_norm": 1.0421061515808105,
      "learning_rate": 0.00013317191283292979,
      "loss": 1.7192,
      "step": 741
    },
    {
      "epoch": 1.052482269503546,
      "grad_norm": 1.0375536680221558,
      "learning_rate": 0.00013307506053268766,
      "loss": 1.6004,
      "step": 742
    },
    {
      "epoch": 1.0539007092198582,
      "grad_norm": 1.040773630142212,
      "learning_rate": 0.00013297820823244554,
      "loss": 1.5675,
      "step": 743
    },
    {
      "epoch": 1.0553191489361702,
      "grad_norm": 1.1050875186920166,
      "learning_rate": 0.0001328813559322034,
      "loss": 1.5882,
      "step": 744
    },
    {
      "epoch": 1.0567375886524824,
      "grad_norm": 1.0099092721939087,
      "learning_rate": 0.00013278450363196127,
      "loss": 1.6221,
      "step": 745
    },
    {
      "epoch": 1.0581560283687943,
      "grad_norm": 0.9932960867881775,
      "learning_rate": 0.00013268765133171915,
      "loss": 1.6655,
      "step": 746
    },
    {
      "epoch": 1.0595744680851065,
      "grad_norm": 1.049930214881897,
      "learning_rate": 0.000132590799031477,
      "loss": 1.4372,
      "step": 747
    },
    {
      "epoch": 1.0609929078014184,
      "grad_norm": 1.054844856262207,
      "learning_rate": 0.00013249394673123488,
      "loss": 1.594,
      "step": 748
    },
    {
      "epoch": 1.0624113475177306,
      "grad_norm": 1.0007810592651367,
      "learning_rate": 0.00013239709443099276,
      "loss": 1.5366,
      "step": 749
    },
    {
      "epoch": 1.0638297872340425,
      "grad_norm": 1.2219514846801758,
      "learning_rate": 0.0001323002421307506,
      "loss": 1.6559,
      "step": 750
    },
    {
      "epoch": 1.0652482269503547,
      "grad_norm": 1.020301103591919,
      "learning_rate": 0.00013220338983050849,
      "loss": 1.7955,
      "step": 751
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 1.0003628730773926,
      "learning_rate": 0.00013210653753026636,
      "loss": 1.697,
      "step": 752
    },
    {
      "epoch": 1.0680851063829788,
      "grad_norm": 1.0451273918151855,
      "learning_rate": 0.00013200968523002421,
      "loss": 1.6042,
      "step": 753
    },
    {
      "epoch": 1.0695035460992908,
      "grad_norm": 1.191812515258789,
      "learning_rate": 0.0001319128329297821,
      "loss": 1.7299,
      "step": 754
    },
    {
      "epoch": 1.070921985815603,
      "grad_norm": 1.0480337142944336,
      "learning_rate": 0.00013181598062953994,
      "loss": 1.7916,
      "step": 755
    },
    {
      "epoch": 1.0723404255319149,
      "grad_norm": 1.090874195098877,
      "learning_rate": 0.00013171912832929782,
      "loss": 1.7273,
      "step": 756
    },
    {
      "epoch": 1.073758865248227,
      "grad_norm": 1.0175896883010864,
      "learning_rate": 0.0001316222760290557,
      "loss": 1.648,
      "step": 757
    },
    {
      "epoch": 1.075177304964539,
      "grad_norm": 1.338780164718628,
      "learning_rate": 0.00013152542372881355,
      "loss": 1.6185,
      "step": 758
    },
    {
      "epoch": 1.076595744680851,
      "grad_norm": 1.1820087432861328,
      "learning_rate": 0.00013142857142857143,
      "loss": 1.5906,
      "step": 759
    },
    {
      "epoch": 1.0780141843971631,
      "grad_norm": 1.0093029737472534,
      "learning_rate": 0.0001313317191283293,
      "loss": 1.6203,
      "step": 760
    },
    {
      "epoch": 1.0794326241134753,
      "grad_norm": 0.9925059676170349,
      "learning_rate": 0.00013123486682808716,
      "loss": 1.6228,
      "step": 761
    },
    {
      "epoch": 1.0808510638297872,
      "grad_norm": 1.1004815101623535,
      "learning_rate": 0.00013113801452784504,
      "loss": 1.7204,
      "step": 762
    },
    {
      "epoch": 1.0822695035460992,
      "grad_norm": 1.026804804801941,
      "learning_rate": 0.00013104116222760291,
      "loss": 1.5004,
      "step": 763
    },
    {
      "epoch": 1.0836879432624114,
      "grad_norm": 1.07239830493927,
      "learning_rate": 0.00013094430992736077,
      "loss": 1.8015,
      "step": 764
    },
    {
      "epoch": 1.0851063829787233,
      "grad_norm": 1.0456151962280273,
      "learning_rate": 0.00013084745762711864,
      "loss": 1.6246,
      "step": 765
    },
    {
      "epoch": 1.0865248226950355,
      "grad_norm": 1.0390042066574097,
      "learning_rate": 0.00013075060532687652,
      "loss": 1.5896,
      "step": 766
    },
    {
      "epoch": 1.0879432624113474,
      "grad_norm": 1.2059139013290405,
      "learning_rate": 0.00013065375302663437,
      "loss": 1.6985,
      "step": 767
    },
    {
      "epoch": 1.0893617021276596,
      "grad_norm": 1.0211321115493774,
      "learning_rate": 0.00013055690072639225,
      "loss": 1.6248,
      "step": 768
    },
    {
      "epoch": 1.0907801418439715,
      "grad_norm": 1.0914887189865112,
      "learning_rate": 0.00013046004842615013,
      "loss": 1.5503,
      "step": 769
    },
    {
      "epoch": 1.0921985815602837,
      "grad_norm": 0.9979832172393799,
      "learning_rate": 0.000130363196125908,
      "loss": 1.6063,
      "step": 770
    },
    {
      "epoch": 1.0936170212765957,
      "grad_norm": 1.163329839706421,
      "learning_rate": 0.00013026634382566586,
      "loss": 1.7596,
      "step": 771
    },
    {
      "epoch": 1.0950354609929078,
      "grad_norm": 0.9895117282867432,
      "learning_rate": 0.00013016949152542374,
      "loss": 1.6197,
      "step": 772
    },
    {
      "epoch": 1.0964539007092198,
      "grad_norm": 1.10126793384552,
      "learning_rate": 0.00013007263922518161,
      "loss": 1.7804,
      "step": 773
    },
    {
      "epoch": 1.097872340425532,
      "grad_norm": 1.0471243858337402,
      "learning_rate": 0.0001299757869249395,
      "loss": 1.3824,
      "step": 774
    },
    {
      "epoch": 1.099290780141844,
      "grad_norm": 1.0833115577697754,
      "learning_rate": 0.00012987893462469734,
      "loss": 1.6021,
      "step": 775
    },
    {
      "epoch": 1.100709219858156,
      "grad_norm": 1.0613868236541748,
      "learning_rate": 0.00012978208232445522,
      "loss": 1.6289,
      "step": 776
    },
    {
      "epoch": 1.102127659574468,
      "grad_norm": 1.1191869974136353,
      "learning_rate": 0.0001296852300242131,
      "loss": 1.5971,
      "step": 777
    },
    {
      "epoch": 1.1035460992907802,
      "grad_norm": 1.0563015937805176,
      "learning_rate": 0.00012958837772397095,
      "loss": 1.6224,
      "step": 778
    },
    {
      "epoch": 1.1049645390070921,
      "grad_norm": 1.1402943134307861,
      "learning_rate": 0.00012949152542372883,
      "loss": 1.6066,
      "step": 779
    },
    {
      "epoch": 1.1063829787234043,
      "grad_norm": 1.0745183229446411,
      "learning_rate": 0.0001293946731234867,
      "loss": 1.4848,
      "step": 780
    },
    {
      "epoch": 1.1078014184397162,
      "grad_norm": 1.1375377178192139,
      "learning_rate": 0.00012929782082324456,
      "loss": 1.6518,
      "step": 781
    },
    {
      "epoch": 1.1092198581560284,
      "grad_norm": 1.0856523513793945,
      "learning_rate": 0.00012920096852300244,
      "loss": 1.6505,
      "step": 782
    },
    {
      "epoch": 1.1106382978723404,
      "grad_norm": 1.0998051166534424,
      "learning_rate": 0.00012910411622276031,
      "loss": 1.6443,
      "step": 783
    },
    {
      "epoch": 1.1120567375886525,
      "grad_norm": 1.0637699365615845,
      "learning_rate": 0.00012900726392251816,
      "loss": 1.6017,
      "step": 784
    },
    {
      "epoch": 1.1134751773049645,
      "grad_norm": 1.1011300086975098,
      "learning_rate": 0.00012891041162227604,
      "loss": 1.6817,
      "step": 785
    },
    {
      "epoch": 1.1148936170212767,
      "grad_norm": 1.0492033958435059,
      "learning_rate": 0.0001288135593220339,
      "loss": 1.6339,
      "step": 786
    },
    {
      "epoch": 1.1163120567375886,
      "grad_norm": 1.0780454874038696,
      "learning_rate": 0.00012871670702179177,
      "loss": 1.6973,
      "step": 787
    },
    {
      "epoch": 1.1177304964539008,
      "grad_norm": 1.0570387840270996,
      "learning_rate": 0.00012861985472154965,
      "loss": 1.6281,
      "step": 788
    },
    {
      "epoch": 1.1191489361702127,
      "grad_norm": 1.108502745628357,
      "learning_rate": 0.0001285230024213075,
      "loss": 1.7035,
      "step": 789
    },
    {
      "epoch": 1.1205673758865249,
      "grad_norm": 1.0517934560775757,
      "learning_rate": 0.00012842615012106538,
      "loss": 1.5869,
      "step": 790
    },
    {
      "epoch": 1.1219858156028368,
      "grad_norm": 1.0766640901565552,
      "learning_rate": 0.00012832929782082326,
      "loss": 1.4739,
      "step": 791
    },
    {
      "epoch": 1.123404255319149,
      "grad_norm": 1.0901997089385986,
      "learning_rate": 0.0001282324455205811,
      "loss": 1.6162,
      "step": 792
    },
    {
      "epoch": 1.124822695035461,
      "grad_norm": 1.116286277770996,
      "learning_rate": 0.000128135593220339,
      "loss": 1.6359,
      "step": 793
    },
    {
      "epoch": 1.1262411347517731,
      "grad_norm": 1.0820567607879639,
      "learning_rate": 0.00012803874092009686,
      "loss": 1.5726,
      "step": 794
    },
    {
      "epoch": 1.127659574468085,
      "grad_norm": 1.0170390605926514,
      "learning_rate": 0.00012794188861985472,
      "loss": 1.537,
      "step": 795
    },
    {
      "epoch": 1.1290780141843972,
      "grad_norm": 1.0629899501800537,
      "learning_rate": 0.0001278450363196126,
      "loss": 1.5736,
      "step": 796
    },
    {
      "epoch": 1.1304964539007092,
      "grad_norm": 1.117285966873169,
      "learning_rate": 0.00012774818401937047,
      "loss": 1.7205,
      "step": 797
    },
    {
      "epoch": 1.1319148936170214,
      "grad_norm": 1.0091588497161865,
      "learning_rate": 0.00012765133171912832,
      "loss": 1.567,
      "step": 798
    },
    {
      "epoch": 1.1333333333333333,
      "grad_norm": 1.0815651416778564,
      "learning_rate": 0.0001275544794188862,
      "loss": 1.6088,
      "step": 799
    },
    {
      "epoch": 1.1347517730496455,
      "grad_norm": 1.0486299991607666,
      "learning_rate": 0.00012745762711864405,
      "loss": 1.5704,
      "step": 800
    },
    {
      "epoch": 1.1361702127659574,
      "grad_norm": 1.2071648836135864,
      "learning_rate": 0.00012736077481840193,
      "loss": 1.6601,
      "step": 801
    },
    {
      "epoch": 1.1375886524822696,
      "grad_norm": 1.1117751598358154,
      "learning_rate": 0.0001272639225181598,
      "loss": 1.7526,
      "step": 802
    },
    {
      "epoch": 1.1390070921985815,
      "grad_norm": 1.1300843954086304,
      "learning_rate": 0.00012716707021791766,
      "loss": 1.9232,
      "step": 803
    },
    {
      "epoch": 1.1404255319148937,
      "grad_norm": 1.1168575286865234,
      "learning_rate": 0.00012707021791767554,
      "loss": 1.4264,
      "step": 804
    },
    {
      "epoch": 1.1418439716312057,
      "grad_norm": 1.0603300333023071,
      "learning_rate": 0.00012697336561743342,
      "loss": 1.5906,
      "step": 805
    },
    {
      "epoch": 1.1432624113475178,
      "grad_norm": 1.142404556274414,
      "learning_rate": 0.0001268765133171913,
      "loss": 1.4126,
      "step": 806
    },
    {
      "epoch": 1.1446808510638298,
      "grad_norm": 1.0878480672836304,
      "learning_rate": 0.00012677966101694917,
      "loss": 1.6963,
      "step": 807
    },
    {
      "epoch": 1.1460992907801417,
      "grad_norm": 1.1728767156600952,
      "learning_rate": 0.00012668280871670702,
      "loss": 1.8231,
      "step": 808
    },
    {
      "epoch": 1.147517730496454,
      "grad_norm": 1.1441761255264282,
      "learning_rate": 0.0001265859564164649,
      "loss": 1.692,
      "step": 809
    },
    {
      "epoch": 1.148936170212766,
      "grad_norm": 1.0993083715438843,
      "learning_rate": 0.00012648910411622278,
      "loss": 1.6154,
      "step": 810
    },
    {
      "epoch": 1.150354609929078,
      "grad_norm": 1.0974794626235962,
      "learning_rate": 0.00012639225181598066,
      "loss": 1.6216,
      "step": 811
    },
    {
      "epoch": 1.15177304964539,
      "grad_norm": 1.1409790515899658,
      "learning_rate": 0.0001262953995157385,
      "loss": 1.6398,
      "step": 812
    },
    {
      "epoch": 1.1531914893617021,
      "grad_norm": 1.0230299234390259,
      "learning_rate": 0.00012619854721549639,
      "loss": 1.5056,
      "step": 813
    },
    {
      "epoch": 1.1546099290780143,
      "grad_norm": 1.2628209590911865,
      "learning_rate": 0.00012610169491525426,
      "loss": 1.8601,
      "step": 814
    },
    {
      "epoch": 1.1560283687943262,
      "grad_norm": 1.1834397315979004,
      "learning_rate": 0.00012600484261501212,
      "loss": 1.8378,
      "step": 815
    },
    {
      "epoch": 1.1574468085106382,
      "grad_norm": 1.1164121627807617,
      "learning_rate": 0.00012590799031477,
      "loss": 1.7314,
      "step": 816
    },
    {
      "epoch": 1.1588652482269504,
      "grad_norm": 1.1394809484481812,
      "learning_rate": 0.00012581113801452784,
      "loss": 1.4417,
      "step": 817
    },
    {
      "epoch": 1.1602836879432625,
      "grad_norm": 1.227352499961853,
      "learning_rate": 0.00012571428571428572,
      "loss": 1.9046,
      "step": 818
    },
    {
      "epoch": 1.1617021276595745,
      "grad_norm": 1.044887900352478,
      "learning_rate": 0.0001256174334140436,
      "loss": 1.5116,
      "step": 819
    },
    {
      "epoch": 1.1631205673758864,
      "grad_norm": 1.1246867179870605,
      "learning_rate": 0.00012552058111380145,
      "loss": 1.6009,
      "step": 820
    },
    {
      "epoch": 1.1645390070921986,
      "grad_norm": 1.1201499700546265,
      "learning_rate": 0.00012542372881355933,
      "loss": 1.5495,
      "step": 821
    },
    {
      "epoch": 1.1659574468085105,
      "grad_norm": 1.162782073020935,
      "learning_rate": 0.0001253268765133172,
      "loss": 1.816,
      "step": 822
    },
    {
      "epoch": 1.1673758865248227,
      "grad_norm": 1.167772889137268,
      "learning_rate": 0.00012523002421307506,
      "loss": 1.7147,
      "step": 823
    },
    {
      "epoch": 1.1687943262411347,
      "grad_norm": 1.1469957828521729,
      "learning_rate": 0.00012513317191283294,
      "loss": 1.5245,
      "step": 824
    },
    {
      "epoch": 1.1702127659574468,
      "grad_norm": 1.1207575798034668,
      "learning_rate": 0.00012503631961259082,
      "loss": 1.662,
      "step": 825
    },
    {
      "epoch": 1.1716312056737588,
      "grad_norm": 1.089674949645996,
      "learning_rate": 0.00012493946731234867,
      "loss": 1.6581,
      "step": 826
    },
    {
      "epoch": 1.173049645390071,
      "grad_norm": 1.1373921632766724,
      "learning_rate": 0.00012484261501210654,
      "loss": 1.6793,
      "step": 827
    },
    {
      "epoch": 1.174468085106383,
      "grad_norm": 1.0678393840789795,
      "learning_rate": 0.00012474576271186442,
      "loss": 1.4781,
      "step": 828
    },
    {
      "epoch": 1.175886524822695,
      "grad_norm": 1.1053776741027832,
      "learning_rate": 0.00012464891041162227,
      "loss": 1.6218,
      "step": 829
    },
    {
      "epoch": 1.177304964539007,
      "grad_norm": 1.1970406770706177,
      "learning_rate": 0.00012455205811138015,
      "loss": 1.8182,
      "step": 830
    },
    {
      "epoch": 1.1787234042553192,
      "grad_norm": 1.108733892440796,
      "learning_rate": 0.00012445520581113803,
      "loss": 1.6454,
      "step": 831
    },
    {
      "epoch": 1.1801418439716311,
      "grad_norm": 1.1200484037399292,
      "learning_rate": 0.00012435835351089588,
      "loss": 1.7415,
      "step": 832
    },
    {
      "epoch": 1.1815602836879433,
      "grad_norm": 1.1099495887756348,
      "learning_rate": 0.00012426150121065376,
      "loss": 1.5547,
      "step": 833
    },
    {
      "epoch": 1.1829787234042553,
      "grad_norm": 1.2226362228393555,
      "learning_rate": 0.0001241646489104116,
      "loss": 1.6778,
      "step": 834
    },
    {
      "epoch": 1.1843971631205674,
      "grad_norm": 1.1625699996948242,
      "learning_rate": 0.0001240677966101695,
      "loss": 1.7189,
      "step": 835
    },
    {
      "epoch": 1.1858156028368794,
      "grad_norm": 1.1172806024551392,
      "learning_rate": 0.00012397094430992737,
      "loss": 1.5602,
      "step": 836
    },
    {
      "epoch": 1.1872340425531915,
      "grad_norm": 1.2000049352645874,
      "learning_rate": 0.00012387409200968522,
      "loss": 1.6432,
      "step": 837
    },
    {
      "epoch": 1.1886524822695035,
      "grad_norm": 1.047294020652771,
      "learning_rate": 0.0001237772397094431,
      "loss": 1.5153,
      "step": 838
    },
    {
      "epoch": 1.1900709219858157,
      "grad_norm": 1.1712678670883179,
      "learning_rate": 0.00012368038740920097,
      "loss": 1.7065,
      "step": 839
    },
    {
      "epoch": 1.1914893617021276,
      "grad_norm": 1.1383200883865356,
      "learning_rate": 0.00012358353510895882,
      "loss": 1.8006,
      "step": 840
    },
    {
      "epoch": 1.1929078014184398,
      "grad_norm": 1.162231683731079,
      "learning_rate": 0.0001234866828087167,
      "loss": 1.7702,
      "step": 841
    },
    {
      "epoch": 1.1943262411347517,
      "grad_norm": 1.1455134153366089,
      "learning_rate": 0.00012338983050847458,
      "loss": 1.5294,
      "step": 842
    },
    {
      "epoch": 1.195744680851064,
      "grad_norm": 1.2015560865402222,
      "learning_rate": 0.00012329297820823246,
      "loss": 1.6117,
      "step": 843
    },
    {
      "epoch": 1.1971631205673758,
      "grad_norm": 1.1155803203582764,
      "learning_rate": 0.0001231961259079903,
      "loss": 1.6747,
      "step": 844
    },
    {
      "epoch": 1.198581560283688,
      "grad_norm": 1.2041141986846924,
      "learning_rate": 0.0001230992736077482,
      "loss": 1.8117,
      "step": 845
    },
    {
      "epoch": 1.2,
      "grad_norm": 1.1454250812530518,
      "learning_rate": 0.00012300242130750607,
      "loss": 1.5233,
      "step": 846
    },
    {
      "epoch": 1.2014184397163121,
      "grad_norm": 1.2244088649749756,
      "learning_rate": 0.00012290556900726394,
      "loss": 1.6077,
      "step": 847
    },
    {
      "epoch": 1.202836879432624,
      "grad_norm": 1.071986436843872,
      "learning_rate": 0.0001228087167070218,
      "loss": 1.4989,
      "step": 848
    },
    {
      "epoch": 1.2042553191489362,
      "grad_norm": 1.2518149614334106,
      "learning_rate": 0.00012271186440677967,
      "loss": 1.567,
      "step": 849
    },
    {
      "epoch": 1.2056737588652482,
      "grad_norm": 1.145087480545044,
      "learning_rate": 0.00012261501210653755,
      "loss": 1.6361,
      "step": 850
    },
    {
      "epoch": 1.2070921985815604,
      "grad_norm": 1.1603142023086548,
      "learning_rate": 0.0001225181598062954,
      "loss": 1.566,
      "step": 851
    },
    {
      "epoch": 1.2085106382978723,
      "grad_norm": 1.1530247926712036,
      "learning_rate": 0.00012242130750605328,
      "loss": 1.6674,
      "step": 852
    },
    {
      "epoch": 1.2099290780141845,
      "grad_norm": 1.0845212936401367,
      "learning_rate": 0.00012232445520581116,
      "loss": 1.7064,
      "step": 853
    },
    {
      "epoch": 1.2113475177304964,
      "grad_norm": 1.1687835454940796,
      "learning_rate": 0.000122227602905569,
      "loss": 1.6285,
      "step": 854
    },
    {
      "epoch": 1.2127659574468086,
      "grad_norm": 1.1240934133529663,
      "learning_rate": 0.0001221307506053269,
      "loss": 1.6136,
      "step": 855
    },
    {
      "epoch": 1.2141843971631205,
      "grad_norm": 1.0865541696548462,
      "learning_rate": 0.00012203389830508477,
      "loss": 1.4974,
      "step": 856
    },
    {
      "epoch": 1.2156028368794327,
      "grad_norm": 1.0967127084732056,
      "learning_rate": 0.00012193704600484262,
      "loss": 1.5726,
      "step": 857
    },
    {
      "epoch": 1.2170212765957447,
      "grad_norm": 1.1025390625,
      "learning_rate": 0.0001218401937046005,
      "loss": 1.5633,
      "step": 858
    },
    {
      "epoch": 1.2184397163120568,
      "grad_norm": 1.2273186445236206,
      "learning_rate": 0.00012174334140435837,
      "loss": 1.8067,
      "step": 859
    },
    {
      "epoch": 1.2198581560283688,
      "grad_norm": 1.1347017288208008,
      "learning_rate": 0.00012164648910411622,
      "loss": 1.7052,
      "step": 860
    },
    {
      "epoch": 1.2212765957446807,
      "grad_norm": 1.1563421487808228,
      "learning_rate": 0.0001215496368038741,
      "loss": 1.4675,
      "step": 861
    },
    {
      "epoch": 1.222695035460993,
      "grad_norm": 1.111360788345337,
      "learning_rate": 0.00012145278450363198,
      "loss": 1.5548,
      "step": 862
    },
    {
      "epoch": 1.224113475177305,
      "grad_norm": 1.181559443473816,
      "learning_rate": 0.00012135593220338983,
      "loss": 1.5412,
      "step": 863
    },
    {
      "epoch": 1.225531914893617,
      "grad_norm": 1.2207701206207275,
      "learning_rate": 0.00012125907990314771,
      "loss": 1.5782,
      "step": 864
    },
    {
      "epoch": 1.226950354609929,
      "grad_norm": 1.1988660097122192,
      "learning_rate": 0.00012116222760290556,
      "loss": 1.8256,
      "step": 865
    },
    {
      "epoch": 1.2283687943262411,
      "grad_norm": 1.193527340888977,
      "learning_rate": 0.00012106537530266344,
      "loss": 1.7381,
      "step": 866
    },
    {
      "epoch": 1.2297872340425533,
      "grad_norm": 1.1142988204956055,
      "learning_rate": 0.00012096852300242132,
      "loss": 1.6554,
      "step": 867
    },
    {
      "epoch": 1.2312056737588652,
      "grad_norm": 1.1546838283538818,
      "learning_rate": 0.00012087167070217917,
      "loss": 1.7093,
      "step": 868
    },
    {
      "epoch": 1.2326241134751772,
      "grad_norm": 1.0819395780563354,
      "learning_rate": 0.00012077481840193705,
      "loss": 1.508,
      "step": 869
    },
    {
      "epoch": 1.2340425531914894,
      "grad_norm": 1.1872516870498657,
      "learning_rate": 0.00012067796610169492,
      "loss": 1.5969,
      "step": 870
    },
    {
      "epoch": 1.2354609929078015,
      "grad_norm": 1.1412676572799683,
      "learning_rate": 0.00012058111380145279,
      "loss": 1.5301,
      "step": 871
    },
    {
      "epoch": 1.2368794326241135,
      "grad_norm": 1.07659912109375,
      "learning_rate": 0.00012048426150121067,
      "loss": 1.6849,
      "step": 872
    },
    {
      "epoch": 1.2382978723404254,
      "grad_norm": 1.0153110027313232,
      "learning_rate": 0.00012038740920096853,
      "loss": 1.4639,
      "step": 873
    },
    {
      "epoch": 1.2397163120567376,
      "grad_norm": 1.050123929977417,
      "learning_rate": 0.0001202905569007264,
      "loss": 1.494,
      "step": 874
    },
    {
      "epoch": 1.2411347517730495,
      "grad_norm": 1.132237434387207,
      "learning_rate": 0.00012019370460048427,
      "loss": 1.3303,
      "step": 875
    },
    {
      "epoch": 1.2425531914893617,
      "grad_norm": 1.2590436935424805,
      "learning_rate": 0.00012009685230024215,
      "loss": 1.7616,
      "step": 876
    },
    {
      "epoch": 1.2439716312056737,
      "grad_norm": 1.1915855407714844,
      "learning_rate": 0.00012,
      "loss": 1.5911,
      "step": 877
    },
    {
      "epoch": 1.2453900709219858,
      "grad_norm": 1.1991710662841797,
      "learning_rate": 0.00011990314769975788,
      "loss": 1.6931,
      "step": 878
    },
    {
      "epoch": 1.2468085106382978,
      "grad_norm": 1.1409502029418945,
      "learning_rate": 0.00011980629539951573,
      "loss": 1.3562,
      "step": 879
    },
    {
      "epoch": 1.24822695035461,
      "grad_norm": 1.1501595973968506,
      "learning_rate": 0.00011970944309927361,
      "loss": 1.4261,
      "step": 880
    },
    {
      "epoch": 1.249645390070922,
      "grad_norm": 1.1364527940750122,
      "learning_rate": 0.00011961259079903149,
      "loss": 1.5007,
      "step": 881
    },
    {
      "epoch": 1.251063829787234,
      "grad_norm": 1.2418267726898193,
      "learning_rate": 0.00011951573849878934,
      "loss": 1.5265,
      "step": 882
    },
    {
      "epoch": 1.252482269503546,
      "grad_norm": 1.1675043106079102,
      "learning_rate": 0.00011941888619854722,
      "loss": 1.6537,
      "step": 883
    },
    {
      "epoch": 1.2539007092198582,
      "grad_norm": 1.2078756093978882,
      "learning_rate": 0.0001193220338983051,
      "loss": 1.4663,
      "step": 884
    },
    {
      "epoch": 1.2553191489361701,
      "grad_norm": 1.1733118295669556,
      "learning_rate": 0.00011922518159806295,
      "loss": 1.7232,
      "step": 885
    },
    {
      "epoch": 1.2567375886524823,
      "grad_norm": 1.1305021047592163,
      "learning_rate": 0.00011912832929782082,
      "loss": 1.7467,
      "step": 886
    },
    {
      "epoch": 1.2581560283687943,
      "grad_norm": 1.1178840398788452,
      "learning_rate": 0.0001190314769975787,
      "loss": 1.4699,
      "step": 887
    },
    {
      "epoch": 1.2595744680851064,
      "grad_norm": 1.1186991930007935,
      "learning_rate": 0.00011893462469733657,
      "loss": 1.6801,
      "step": 888
    },
    {
      "epoch": 1.2609929078014184,
      "grad_norm": 1.2062313556671143,
      "learning_rate": 0.00011883777239709443,
      "loss": 1.8058,
      "step": 889
    },
    {
      "epoch": 1.2624113475177305,
      "grad_norm": 1.242905855178833,
      "learning_rate": 0.00011874092009685231,
      "loss": 1.5943,
      "step": 890
    },
    {
      "epoch": 1.2638297872340425,
      "grad_norm": 1.21308434009552,
      "learning_rate": 0.00011864406779661017,
      "loss": 1.4569,
      "step": 891
    },
    {
      "epoch": 1.2652482269503547,
      "grad_norm": 1.0986233949661255,
      "learning_rate": 0.00011854721549636805,
      "loss": 1.6523,
      "step": 892
    },
    {
      "epoch": 1.2666666666666666,
      "grad_norm": 1.2085695266723633,
      "learning_rate": 0.00011845036319612592,
      "loss": 1.642,
      "step": 893
    },
    {
      "epoch": 1.2680851063829788,
      "grad_norm": 1.212265968322754,
      "learning_rate": 0.00011835351089588378,
      "loss": 1.6045,
      "step": 894
    },
    {
      "epoch": 1.2695035460992907,
      "grad_norm": 1.0752696990966797,
      "learning_rate": 0.00011825665859564166,
      "loss": 1.5124,
      "step": 895
    },
    {
      "epoch": 1.270921985815603,
      "grad_norm": 1.0601632595062256,
      "learning_rate": 0.00011815980629539951,
      "loss": 1.6149,
      "step": 896
    },
    {
      "epoch": 1.2723404255319148,
      "grad_norm": 1.165026068687439,
      "learning_rate": 0.00011806295399515739,
      "loss": 1.6937,
      "step": 897
    },
    {
      "epoch": 1.273758865248227,
      "grad_norm": 1.1537418365478516,
      "learning_rate": 0.00011796610169491527,
      "loss": 1.8067,
      "step": 898
    },
    {
      "epoch": 1.275177304964539,
      "grad_norm": 1.143288016319275,
      "learning_rate": 0.00011786924939467312,
      "loss": 1.6607,
      "step": 899
    },
    {
      "epoch": 1.2765957446808511,
      "grad_norm": 1.1708904504776,
      "learning_rate": 0.000117772397094431,
      "loss": 1.4811,
      "step": 900
    },
    {
      "epoch": 1.278014184397163,
      "grad_norm": 1.1173733472824097,
      "learning_rate": 0.00011767554479418887,
      "loss": 1.4682,
      "step": 901
    },
    {
      "epoch": 1.2794326241134752,
      "grad_norm": 1.0386710166931152,
      "learning_rate": 0.00011757869249394673,
      "loss": 1.5555,
      "step": 902
    },
    {
      "epoch": 1.2808510638297872,
      "grad_norm": 1.1206083297729492,
      "learning_rate": 0.0001174818401937046,
      "loss": 1.6718,
      "step": 903
    },
    {
      "epoch": 1.2822695035460994,
      "grad_norm": 1.0450093746185303,
      "learning_rate": 0.00011738498789346248,
      "loss": 1.6509,
      "step": 904
    },
    {
      "epoch": 1.2836879432624113,
      "grad_norm": 1.117169976234436,
      "learning_rate": 0.00011728813559322033,
      "loss": 1.5285,
      "step": 905
    },
    {
      "epoch": 1.2851063829787235,
      "grad_norm": 1.251837968826294,
      "learning_rate": 0.00011719128329297821,
      "loss": 1.7585,
      "step": 906
    },
    {
      "epoch": 1.2865248226950354,
      "grad_norm": 1.0525087118148804,
      "learning_rate": 0.00011709443099273609,
      "loss": 1.6006,
      "step": 907
    },
    {
      "epoch": 1.2879432624113476,
      "grad_norm": 1.113349199295044,
      "learning_rate": 0.00011699757869249395,
      "loss": 1.4166,
      "step": 908
    },
    {
      "epoch": 1.2893617021276595,
      "grad_norm": 1.0720531940460205,
      "learning_rate": 0.00011690072639225182,
      "loss": 1.5842,
      "step": 909
    },
    {
      "epoch": 1.2907801418439715,
      "grad_norm": 1.0865756273269653,
      "learning_rate": 0.0001168038740920097,
      "loss": 1.87,
      "step": 910
    },
    {
      "epoch": 1.2921985815602837,
      "grad_norm": 1.221030354499817,
      "learning_rate": 0.00011670702179176756,
      "loss": 1.6823,
      "step": 911
    },
    {
      "epoch": 1.2936170212765958,
      "grad_norm": 1.0974559783935547,
      "learning_rate": 0.00011661016949152544,
      "loss": 1.4962,
      "step": 912
    },
    {
      "epoch": 1.2950354609929078,
      "grad_norm": 1.1802328824996948,
      "learning_rate": 0.00011651331719128329,
      "loss": 1.6488,
      "step": 913
    },
    {
      "epoch": 1.2964539007092197,
      "grad_norm": 1.2646870613098145,
      "learning_rate": 0.00011641646489104117,
      "loss": 1.7997,
      "step": 914
    },
    {
      "epoch": 1.297872340425532,
      "grad_norm": 1.0725520849227905,
      "learning_rate": 0.00011631961259079905,
      "loss": 1.426,
      "step": 915
    },
    {
      "epoch": 1.299290780141844,
      "grad_norm": 1.141984462738037,
      "learning_rate": 0.0001162227602905569,
      "loss": 1.6595,
      "step": 916
    },
    {
      "epoch": 1.300709219858156,
      "grad_norm": 1.2002671957015991,
      "learning_rate": 0.00011612590799031478,
      "loss": 1.7648,
      "step": 917
    },
    {
      "epoch": 1.302127659574468,
      "grad_norm": 1.1848368644714355,
      "learning_rate": 0.00011602905569007265,
      "loss": 1.4617,
      "step": 918
    },
    {
      "epoch": 1.3035460992907801,
      "grad_norm": 1.2445858716964722,
      "learning_rate": 0.0001159322033898305,
      "loss": 1.6178,
      "step": 919
    },
    {
      "epoch": 1.3049645390070923,
      "grad_norm": 1.129919409751892,
      "learning_rate": 0.00011583535108958838,
      "loss": 1.609,
      "step": 920
    },
    {
      "epoch": 1.3063829787234043,
      "grad_norm": 1.2382508516311646,
      "learning_rate": 0.00011573849878934626,
      "loss": 1.6737,
      "step": 921
    },
    {
      "epoch": 1.3078014184397162,
      "grad_norm": 1.1222894191741943,
      "learning_rate": 0.00011564164648910411,
      "loss": 1.5186,
      "step": 922
    },
    {
      "epoch": 1.3092198581560284,
      "grad_norm": 1.1041353940963745,
      "learning_rate": 0.00011554479418886199,
      "loss": 1.6019,
      "step": 923
    },
    {
      "epoch": 1.3106382978723405,
      "grad_norm": 1.207726001739502,
      "learning_rate": 0.00011544794188861987,
      "loss": 1.7618,
      "step": 924
    },
    {
      "epoch": 1.3120567375886525,
      "grad_norm": 1.1176342964172363,
      "learning_rate": 0.00011535108958837772,
      "loss": 1.4716,
      "step": 925
    },
    {
      "epoch": 1.3134751773049644,
      "grad_norm": 1.090982437133789,
      "learning_rate": 0.0001152542372881356,
      "loss": 1.6561,
      "step": 926
    },
    {
      "epoch": 1.3148936170212766,
      "grad_norm": 1.2275443077087402,
      "learning_rate": 0.00011515738498789346,
      "loss": 1.8344,
      "step": 927
    },
    {
      "epoch": 1.3163120567375888,
      "grad_norm": 1.203176736831665,
      "learning_rate": 0.00011506053268765134,
      "loss": 1.7922,
      "step": 928
    },
    {
      "epoch": 1.3177304964539007,
      "grad_norm": 1.186428427696228,
      "learning_rate": 0.00011496368038740922,
      "loss": 1.5429,
      "step": 929
    },
    {
      "epoch": 1.3191489361702127,
      "grad_norm": 1.2277957201004028,
      "learning_rate": 0.00011486682808716707,
      "loss": 1.7031,
      "step": 930
    },
    {
      "epoch": 1.3205673758865248,
      "grad_norm": 1.174821376800537,
      "learning_rate": 0.00011476997578692495,
      "loss": 1.5746,
      "step": 931
    },
    {
      "epoch": 1.321985815602837,
      "grad_norm": 1.1172395944595337,
      "learning_rate": 0.00011467312348668283,
      "loss": 1.5025,
      "step": 932
    },
    {
      "epoch": 1.323404255319149,
      "grad_norm": 1.2259165048599243,
      "learning_rate": 0.00011457627118644068,
      "loss": 1.8143,
      "step": 933
    },
    {
      "epoch": 1.324822695035461,
      "grad_norm": 1.2798879146575928,
      "learning_rate": 0.00011447941888619855,
      "loss": 1.8829,
      "step": 934
    },
    {
      "epoch": 1.326241134751773,
      "grad_norm": 2.020556688308716,
      "learning_rate": 0.00011438256658595643,
      "loss": 1.698,
      "step": 935
    },
    {
      "epoch": 1.327659574468085,
      "grad_norm": 1.2863777875900269,
      "learning_rate": 0.00011428571428571428,
      "loss": 1.9202,
      "step": 936
    },
    {
      "epoch": 1.3290780141843972,
      "grad_norm": 1.1320306062698364,
      "learning_rate": 0.00011418886198547216,
      "loss": 1.5284,
      "step": 937
    },
    {
      "epoch": 1.3304964539007091,
      "grad_norm": 1.1341561079025269,
      "learning_rate": 0.00011409200968523004,
      "loss": 1.4473,
      "step": 938
    },
    {
      "epoch": 1.3319148936170213,
      "grad_norm": 1.25192391872406,
      "learning_rate": 0.00011399515738498789,
      "loss": 1.6659,
      "step": 939
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 1.1707910299301147,
      "learning_rate": 0.00011389830508474577,
      "loss": 1.7806,
      "step": 940
    },
    {
      "epoch": 1.3347517730496454,
      "grad_norm": 1.0474827289581299,
      "learning_rate": 0.00011380145278450365,
      "loss": 1.4623,
      "step": 941
    },
    {
      "epoch": 1.3361702127659574,
      "grad_norm": 1.2655093669891357,
      "learning_rate": 0.0001137046004842615,
      "loss": 1.8148,
      "step": 942
    },
    {
      "epoch": 1.3375886524822695,
      "grad_norm": 1.229114294052124,
      "learning_rate": 0.00011360774818401938,
      "loss": 1.6801,
      "step": 943
    },
    {
      "epoch": 1.3390070921985815,
      "grad_norm": 1.1582350730895996,
      "learning_rate": 0.00011351089588377724,
      "loss": 1.6184,
      "step": 944
    },
    {
      "epoch": 1.3404255319148937,
      "grad_norm": 1.1487945318222046,
      "learning_rate": 0.00011341404358353512,
      "loss": 1.6317,
      "step": 945
    },
    {
      "epoch": 1.3418439716312056,
      "grad_norm": 1.2071768045425415,
      "learning_rate": 0.00011331719128329298,
      "loss": 1.6888,
      "step": 946
    },
    {
      "epoch": 1.3432624113475178,
      "grad_norm": 1.239540696144104,
      "learning_rate": 0.00011322033898305085,
      "loss": 1.6647,
      "step": 947
    },
    {
      "epoch": 1.3446808510638297,
      "grad_norm": 1.2201781272888184,
      "learning_rate": 0.00011312348668280873,
      "loss": 1.661,
      "step": 948
    },
    {
      "epoch": 1.346099290780142,
      "grad_norm": 1.113709807395935,
      "learning_rate": 0.0001130266343825666,
      "loss": 1.576,
      "step": 949
    },
    {
      "epoch": 1.3475177304964538,
      "grad_norm": 1.2019202709197998,
      "learning_rate": 0.00011292978208232446,
      "loss": 1.6173,
      "step": 950
    },
    {
      "epoch": 1.348936170212766,
      "grad_norm": 1.1095081567764282,
      "learning_rate": 0.00011283292978208233,
      "loss": 1.7291,
      "step": 951
    },
    {
      "epoch": 1.350354609929078,
      "grad_norm": 1.1851284503936768,
      "learning_rate": 0.00011273607748184021,
      "loss": 1.7803,
      "step": 952
    },
    {
      "epoch": 1.3517730496453901,
      "grad_norm": 1.0814930200576782,
      "learning_rate": 0.00011263922518159806,
      "loss": 1.6586,
      "step": 953
    },
    {
      "epoch": 1.353191489361702,
      "grad_norm": 1.1287448406219482,
      "learning_rate": 0.00011254237288135594,
      "loss": 1.5438,
      "step": 954
    },
    {
      "epoch": 1.3546099290780143,
      "grad_norm": 1.1824431419372559,
      "learning_rate": 0.00011244552058111382,
      "loss": 1.5447,
      "step": 955
    },
    {
      "epoch": 1.3560283687943262,
      "grad_norm": 1.1342796087265015,
      "learning_rate": 0.00011234866828087167,
      "loss": 1.7099,
      "step": 956
    },
    {
      "epoch": 1.3574468085106384,
      "grad_norm": 1.1588134765625,
      "learning_rate": 0.00011225181598062955,
      "loss": 1.6343,
      "step": 957
    },
    {
      "epoch": 1.3588652482269503,
      "grad_norm": 1.146774411201477,
      "learning_rate": 0.0001121549636803874,
      "loss": 1.8391,
      "step": 958
    },
    {
      "epoch": 1.3602836879432625,
      "grad_norm": 1.1741163730621338,
      "learning_rate": 0.00011205811138014528,
      "loss": 1.4939,
      "step": 959
    },
    {
      "epoch": 1.3617021276595744,
      "grad_norm": 1.1536725759506226,
      "learning_rate": 0.00011196125907990315,
      "loss": 1.658,
      "step": 960
    },
    {
      "epoch": 1.3631205673758866,
      "grad_norm": 1.1312150955200195,
      "learning_rate": 0.00011186440677966102,
      "loss": 1.7353,
      "step": 961
    },
    {
      "epoch": 1.3645390070921986,
      "grad_norm": 1.1812450885772705,
      "learning_rate": 0.00011176755447941888,
      "loss": 1.6056,
      "step": 962
    },
    {
      "epoch": 1.3659574468085105,
      "grad_norm": 1.2007774114608765,
      "learning_rate": 0.00011167070217917676,
      "loss": 1.673,
      "step": 963
    },
    {
      "epoch": 1.3673758865248227,
      "grad_norm": 1.2752985954284668,
      "learning_rate": 0.00011157384987893463,
      "loss": 1.9037,
      "step": 964
    },
    {
      "epoch": 1.3687943262411348,
      "grad_norm": 1.2208536863327026,
      "learning_rate": 0.0001114769975786925,
      "loss": 1.6925,
      "step": 965
    },
    {
      "epoch": 1.3702127659574468,
      "grad_norm": 1.2808160781860352,
      "learning_rate": 0.00011138014527845038,
      "loss": 1.6853,
      "step": 966
    },
    {
      "epoch": 1.3716312056737587,
      "grad_norm": 1.089027762413025,
      "learning_rate": 0.00011128329297820823,
      "loss": 1.6555,
      "step": 967
    },
    {
      "epoch": 1.373049645390071,
      "grad_norm": 1.1047166585922241,
      "learning_rate": 0.00011118644067796611,
      "loss": 1.4978,
      "step": 968
    },
    {
      "epoch": 1.374468085106383,
      "grad_norm": 1.0739012956619263,
      "learning_rate": 0.00011108958837772399,
      "loss": 1.6007,
      "step": 969
    },
    {
      "epoch": 1.375886524822695,
      "grad_norm": 1.179026484489441,
      "learning_rate": 0.00011099273607748184,
      "loss": 1.6669,
      "step": 970
    },
    {
      "epoch": 1.377304964539007,
      "grad_norm": 1.1046103239059448,
      "learning_rate": 0.00011089588377723972,
      "loss": 1.5983,
      "step": 971
    },
    {
      "epoch": 1.3787234042553191,
      "grad_norm": 1.1961252689361572,
      "learning_rate": 0.0001107990314769976,
      "loss": 1.5383,
      "step": 972
    },
    {
      "epoch": 1.3801418439716313,
      "grad_norm": 1.1818937063217163,
      "learning_rate": 0.00011070217917675545,
      "loss": 1.6442,
      "step": 973
    },
    {
      "epoch": 1.3815602836879433,
      "grad_norm": 1.2215759754180908,
      "learning_rate": 0.00011060532687651333,
      "loss": 1.9487,
      "step": 974
    },
    {
      "epoch": 1.3829787234042552,
      "grad_norm": 1.4783482551574707,
      "learning_rate": 0.00011050847457627118,
      "loss": 1.4912,
      "step": 975
    },
    {
      "epoch": 1.3843971631205674,
      "grad_norm": 1.1202548742294312,
      "learning_rate": 0.00011041162227602906,
      "loss": 1.4729,
      "step": 976
    },
    {
      "epoch": 1.3858156028368795,
      "grad_norm": 1.1596672534942627,
      "learning_rate": 0.00011031476997578693,
      "loss": 1.6297,
      "step": 977
    },
    {
      "epoch": 1.3872340425531915,
      "grad_norm": 1.1592689752578735,
      "learning_rate": 0.00011021791767554478,
      "loss": 1.6549,
      "step": 978
    },
    {
      "epoch": 1.3886524822695034,
      "grad_norm": 1.1292668581008911,
      "learning_rate": 0.00011012106537530266,
      "loss": 1.6048,
      "step": 979
    },
    {
      "epoch": 1.3900709219858156,
      "grad_norm": 1.0825718641281128,
      "learning_rate": 0.00011002421307506054,
      "loss": 1.5129,
      "step": 980
    },
    {
      "epoch": 1.3914893617021278,
      "grad_norm": 1.2114427089691162,
      "learning_rate": 0.0001099273607748184,
      "loss": 1.823,
      "step": 981
    },
    {
      "epoch": 1.3929078014184397,
      "grad_norm": 1.1911460161209106,
      "learning_rate": 0.00010983050847457627,
      "loss": 1.7533,
      "step": 982
    },
    {
      "epoch": 1.3943262411347517,
      "grad_norm": 1.221181869506836,
      "learning_rate": 0.00010973365617433415,
      "loss": 1.7017,
      "step": 983
    },
    {
      "epoch": 1.3957446808510638,
      "grad_norm": 1.1635750532150269,
      "learning_rate": 0.00010963680387409201,
      "loss": 1.7917,
      "step": 984
    },
    {
      "epoch": 1.397163120567376,
      "grad_norm": 1.235533595085144,
      "learning_rate": 0.00010953995157384989,
      "loss": 1.8506,
      "step": 985
    },
    {
      "epoch": 1.398581560283688,
      "grad_norm": 1.1691476106643677,
      "learning_rate": 0.00010944309927360777,
      "loss": 1.8524,
      "step": 986
    },
    {
      "epoch": 1.4,
      "grad_norm": 1.0987571477890015,
      "learning_rate": 0.00010934624697336562,
      "loss": 1.5169,
      "step": 987
    },
    {
      "epoch": 1.401418439716312,
      "grad_norm": 1.0995463132858276,
      "learning_rate": 0.0001092493946731235,
      "loss": 1.6282,
      "step": 988
    },
    {
      "epoch": 1.4028368794326243,
      "grad_norm": 1.2586760520935059,
      "learning_rate": 0.00010915254237288135,
      "loss": 1.8451,
      "step": 989
    },
    {
      "epoch": 1.4042553191489362,
      "grad_norm": 1.1941345930099487,
      "learning_rate": 0.00010905569007263923,
      "loss": 1.9499,
      "step": 990
    },
    {
      "epoch": 1.4056737588652481,
      "grad_norm": 1.048651933670044,
      "learning_rate": 0.0001089588377723971,
      "loss": 1.5519,
      "step": 991
    },
    {
      "epoch": 1.4070921985815603,
      "grad_norm": 1.0933752059936523,
      "learning_rate": 0.00010886198547215496,
      "loss": 1.3748,
      "step": 992
    },
    {
      "epoch": 1.4085106382978723,
      "grad_norm": 1.1557165384292603,
      "learning_rate": 0.00010876513317191283,
      "loss": 1.6281,
      "step": 993
    },
    {
      "epoch": 1.4099290780141844,
      "grad_norm": 1.192756175994873,
      "learning_rate": 0.00010866828087167071,
      "loss": 1.4983,
      "step": 994
    },
    {
      "epoch": 1.4113475177304964,
      "grad_norm": 1.102967381477356,
      "learning_rate": 0.00010857142857142856,
      "loss": 1.8108,
      "step": 995
    },
    {
      "epoch": 1.4127659574468086,
      "grad_norm": 1.0901373624801636,
      "learning_rate": 0.00010847457627118644,
      "loss": 1.5046,
      "step": 996
    },
    {
      "epoch": 1.4141843971631205,
      "grad_norm": 1.1881366968154907,
      "learning_rate": 0.00010837772397094432,
      "loss": 1.6548,
      "step": 997
    },
    {
      "epoch": 1.4156028368794327,
      "grad_norm": 1.064683437347412,
      "learning_rate": 0.00010828087167070217,
      "loss": 1.3942,
      "step": 998
    },
    {
      "epoch": 1.4170212765957446,
      "grad_norm": 1.1951991319656372,
      "learning_rate": 0.00010818401937046005,
      "loss": 1.5399,
      "step": 999
    },
    {
      "epoch": 1.4184397163120568,
      "grad_norm": 1.143358826637268,
      "learning_rate": 0.00010808716707021793,
      "loss": 1.4712,
      "step": 1000
    },
    {
      "epoch": 1.4184397163120568,
      "eval_loss": 1.7848999500274658,
      "eval_runtime": 95.6905,
      "eval_samples_per_second": 14.735,
      "eval_steps_per_second": 7.368,
      "step": 1000
    }
  ],
  "logging_steps": 1,
  "max_steps": 2115,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2633414954227200.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
