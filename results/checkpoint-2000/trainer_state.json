{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.8368794326241136,
  "eval_steps": 500,
  "global_step": 2000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0014184397163120568,
      "grad_norm": 4.663888931274414,
      "learning_rate": 0.0,
      "loss": 3.363,
      "step": 1
    },
    {
      "epoch": 0.0028368794326241137,
      "grad_norm": 4.326501846313477,
      "learning_rate": 4.000000000000001e-06,
      "loss": 3.1448,
      "step": 2
    },
    {
      "epoch": 0.00425531914893617,
      "grad_norm": 5.150259494781494,
      "learning_rate": 8.000000000000001e-06,
      "loss": 3.1434,
      "step": 3
    },
    {
      "epoch": 0.005673758865248227,
      "grad_norm": 5.390714645385742,
      "learning_rate": 1.2e-05,
      "loss": 3.3289,
      "step": 4
    },
    {
      "epoch": 0.0070921985815602835,
      "grad_norm": 4.865074634552002,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 3.0366,
      "step": 5
    },
    {
      "epoch": 0.00851063829787234,
      "grad_norm": 4.287224769592285,
      "learning_rate": 2e-05,
      "loss": 3.1406,
      "step": 6
    },
    {
      "epoch": 0.009929078014184398,
      "grad_norm": 4.218326568603516,
      "learning_rate": 2.4e-05,
      "loss": 3.1917,
      "step": 7
    },
    {
      "epoch": 0.011347517730496455,
      "grad_norm": 3.5314183235168457,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 2.882,
      "step": 8
    },
    {
      "epoch": 0.01276595744680851,
      "grad_norm": 3.696972370147705,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 2.8056,
      "step": 9
    },
    {
      "epoch": 0.014184397163120567,
      "grad_norm": 3.214474678039551,
      "learning_rate": 3.6e-05,
      "loss": 3.2135,
      "step": 10
    },
    {
      "epoch": 0.015602836879432624,
      "grad_norm": 3.480194330215454,
      "learning_rate": 4e-05,
      "loss": 2.6706,
      "step": 11
    },
    {
      "epoch": 0.01702127659574468,
      "grad_norm": 3.2293241024017334,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 2.577,
      "step": 12
    },
    {
      "epoch": 0.018439716312056736,
      "grad_norm": 3.4259519577026367,
      "learning_rate": 4.8e-05,
      "loss": 2.8119,
      "step": 13
    },
    {
      "epoch": 0.019858156028368795,
      "grad_norm": 2.966158151626587,
      "learning_rate": 5.2000000000000004e-05,
      "loss": 2.6788,
      "step": 14
    },
    {
      "epoch": 0.02127659574468085,
      "grad_norm": 3.072213888168335,
      "learning_rate": 5.6000000000000006e-05,
      "loss": 2.3474,
      "step": 15
    },
    {
      "epoch": 0.02269503546099291,
      "grad_norm": 3.135129690170288,
      "learning_rate": 6e-05,
      "loss": 2.2081,
      "step": 16
    },
    {
      "epoch": 0.024113475177304965,
      "grad_norm": 2.8877358436584473,
      "learning_rate": 6.400000000000001e-05,
      "loss": 2.486,
      "step": 17
    },
    {
      "epoch": 0.02553191489361702,
      "grad_norm": 1.3276922702789307,
      "learning_rate": 6.800000000000001e-05,
      "loss": 2.277,
      "step": 18
    },
    {
      "epoch": 0.02695035460992908,
      "grad_norm": 1.734344482421875,
      "learning_rate": 7.2e-05,
      "loss": 2.3077,
      "step": 19
    },
    {
      "epoch": 0.028368794326241134,
      "grad_norm": 1.927562952041626,
      "learning_rate": 7.6e-05,
      "loss": 2.2205,
      "step": 20
    },
    {
      "epoch": 0.029787234042553193,
      "grad_norm": 1.658927321434021,
      "learning_rate": 8e-05,
      "loss": 2.2476,
      "step": 21
    },
    {
      "epoch": 0.031205673758865248,
      "grad_norm": 1.7990249395370483,
      "learning_rate": 8.4e-05,
      "loss": 2.4956,
      "step": 22
    },
    {
      "epoch": 0.032624113475177303,
      "grad_norm": 1.2179522514343262,
      "learning_rate": 8.800000000000001e-05,
      "loss": 2.3486,
      "step": 23
    },
    {
      "epoch": 0.03404255319148936,
      "grad_norm": 1.2065980434417725,
      "learning_rate": 9.200000000000001e-05,
      "loss": 2.1772,
      "step": 24
    },
    {
      "epoch": 0.03546099290780142,
      "grad_norm": 1.3824200630187988,
      "learning_rate": 9.6e-05,
      "loss": 2.215,
      "step": 25
    },
    {
      "epoch": 0.03687943262411347,
      "grad_norm": 1.2284038066864014,
      "learning_rate": 0.0001,
      "loss": 2.1544,
      "step": 26
    },
    {
      "epoch": 0.03829787234042553,
      "grad_norm": 1.2294367551803589,
      "learning_rate": 0.00010400000000000001,
      "loss": 2.1664,
      "step": 27
    },
    {
      "epoch": 0.03971631205673759,
      "grad_norm": 1.2366228103637695,
      "learning_rate": 0.00010800000000000001,
      "loss": 2.06,
      "step": 28
    },
    {
      "epoch": 0.04113475177304964,
      "grad_norm": 1.154578685760498,
      "learning_rate": 0.00011200000000000001,
      "loss": 2.1996,
      "step": 29
    },
    {
      "epoch": 0.0425531914893617,
      "grad_norm": 1.2831920385360718,
      "learning_rate": 0.000116,
      "loss": 1.9245,
      "step": 30
    },
    {
      "epoch": 0.04397163120567376,
      "grad_norm": 1.2155778408050537,
      "learning_rate": 0.00012,
      "loss": 2.0609,
      "step": 31
    },
    {
      "epoch": 0.04539007092198582,
      "grad_norm": 1.2330687046051025,
      "learning_rate": 0.000124,
      "loss": 2.1224,
      "step": 32
    },
    {
      "epoch": 0.04680851063829787,
      "grad_norm": 1.2099401950836182,
      "learning_rate": 0.00012800000000000002,
      "loss": 2.122,
      "step": 33
    },
    {
      "epoch": 0.04822695035460993,
      "grad_norm": 1.0961740016937256,
      "learning_rate": 0.000132,
      "loss": 2.0189,
      "step": 34
    },
    {
      "epoch": 0.04964539007092199,
      "grad_norm": 1.2576420307159424,
      "learning_rate": 0.00013600000000000003,
      "loss": 2.0353,
      "step": 35
    },
    {
      "epoch": 0.05106382978723404,
      "grad_norm": 1.195496678352356,
      "learning_rate": 0.00014,
      "loss": 2.0332,
      "step": 36
    },
    {
      "epoch": 0.0524822695035461,
      "grad_norm": 1.218315839767456,
      "learning_rate": 0.000144,
      "loss": 2.0501,
      "step": 37
    },
    {
      "epoch": 0.05390070921985816,
      "grad_norm": 1.1647756099700928,
      "learning_rate": 0.000148,
      "loss": 2.2139,
      "step": 38
    },
    {
      "epoch": 0.05531914893617021,
      "grad_norm": 1.20754873752594,
      "learning_rate": 0.000152,
      "loss": 2.2591,
      "step": 39
    },
    {
      "epoch": 0.05673758865248227,
      "grad_norm": 1.0971544981002808,
      "learning_rate": 0.00015600000000000002,
      "loss": 2.273,
      "step": 40
    },
    {
      "epoch": 0.05815602836879433,
      "grad_norm": 1.2460731267929077,
      "learning_rate": 0.00016,
      "loss": 2.2102,
      "step": 41
    },
    {
      "epoch": 0.059574468085106386,
      "grad_norm": 1.1251200437545776,
      "learning_rate": 0.000164,
      "loss": 2.1836,
      "step": 42
    },
    {
      "epoch": 0.06099290780141844,
      "grad_norm": 1.149972677230835,
      "learning_rate": 0.000168,
      "loss": 1.8653,
      "step": 43
    },
    {
      "epoch": 0.062411347517730496,
      "grad_norm": 1.097387671470642,
      "learning_rate": 0.000172,
      "loss": 2.2198,
      "step": 44
    },
    {
      "epoch": 0.06382978723404255,
      "grad_norm": 1.1107244491577148,
      "learning_rate": 0.00017600000000000002,
      "loss": 2.1135,
      "step": 45
    },
    {
      "epoch": 0.06524822695035461,
      "grad_norm": 1.0508993864059448,
      "learning_rate": 0.00018,
      "loss": 1.9834,
      "step": 46
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 1.1061515808105469,
      "learning_rate": 0.00018400000000000003,
      "loss": 2.0214,
      "step": 47
    },
    {
      "epoch": 0.06808510638297872,
      "grad_norm": 1.055652141571045,
      "learning_rate": 0.000188,
      "loss": 2.0456,
      "step": 48
    },
    {
      "epoch": 0.06950354609929078,
      "grad_norm": 1.0564075708389282,
      "learning_rate": 0.000192,
      "loss": 1.8642,
      "step": 49
    },
    {
      "epoch": 0.07092198581560284,
      "grad_norm": 1.03203547000885,
      "learning_rate": 0.000196,
      "loss": 1.714,
      "step": 50
    },
    {
      "epoch": 0.07234042553191489,
      "grad_norm": 1.1104342937469482,
      "learning_rate": 0.0002,
      "loss": 2.0922,
      "step": 51
    },
    {
      "epoch": 0.07375886524822695,
      "grad_norm": 1.0160233974456787,
      "learning_rate": 0.0001999031476997579,
      "loss": 1.9706,
      "step": 52
    },
    {
      "epoch": 0.075177304964539,
      "grad_norm": 0.9867855310440063,
      "learning_rate": 0.00019980629539951574,
      "loss": 1.9446,
      "step": 53
    },
    {
      "epoch": 0.07659574468085106,
      "grad_norm": 1.0777437686920166,
      "learning_rate": 0.00019970944309927362,
      "loss": 1.8683,
      "step": 54
    },
    {
      "epoch": 0.07801418439716312,
      "grad_norm": 1.0760366916656494,
      "learning_rate": 0.0001996125907990315,
      "loss": 1.8404,
      "step": 55
    },
    {
      "epoch": 0.07943262411347518,
      "grad_norm": 1.1468721628189087,
      "learning_rate": 0.00019951573849878935,
      "loss": 1.9124,
      "step": 56
    },
    {
      "epoch": 0.08085106382978724,
      "grad_norm": 1.0046629905700684,
      "learning_rate": 0.00019941888619854722,
      "loss": 1.9508,
      "step": 57
    },
    {
      "epoch": 0.08226950354609928,
      "grad_norm": 1.1077072620391846,
      "learning_rate": 0.0001993220338983051,
      "loss": 1.966,
      "step": 58
    },
    {
      "epoch": 0.08368794326241134,
      "grad_norm": 1.161176085472107,
      "learning_rate": 0.00019922518159806295,
      "loss": 1.7889,
      "step": 59
    },
    {
      "epoch": 0.0851063829787234,
      "grad_norm": 1.0874756574630737,
      "learning_rate": 0.00019912832929782083,
      "loss": 1.9527,
      "step": 60
    },
    {
      "epoch": 0.08652482269503546,
      "grad_norm": 1.1886991262435913,
      "learning_rate": 0.00019903147699757868,
      "loss": 1.846,
      "step": 61
    },
    {
      "epoch": 0.08794326241134752,
      "grad_norm": 0.9369897246360779,
      "learning_rate": 0.00019893462469733656,
      "loss": 1.6584,
      "step": 62
    },
    {
      "epoch": 0.08936170212765958,
      "grad_norm": 1.0621529817581177,
      "learning_rate": 0.00019883777239709444,
      "loss": 1.8865,
      "step": 63
    },
    {
      "epoch": 0.09078014184397164,
      "grad_norm": 0.9763602614402771,
      "learning_rate": 0.0001987409200968523,
      "loss": 1.8334,
      "step": 64
    },
    {
      "epoch": 0.09219858156028368,
      "grad_norm": 1.0840765237808228,
      "learning_rate": 0.00019864406779661017,
      "loss": 2.0674,
      "step": 65
    },
    {
      "epoch": 0.09361702127659574,
      "grad_norm": 0.9588376879692078,
      "learning_rate": 0.00019854721549636805,
      "loss": 1.8821,
      "step": 66
    },
    {
      "epoch": 0.0950354609929078,
      "grad_norm": 0.9782182574272156,
      "learning_rate": 0.0001984503631961259,
      "loss": 1.8329,
      "step": 67
    },
    {
      "epoch": 0.09645390070921986,
      "grad_norm": 1.0978199243545532,
      "learning_rate": 0.00019835351089588377,
      "loss": 1.7789,
      "step": 68
    },
    {
      "epoch": 0.09787234042553192,
      "grad_norm": 1.2051382064819336,
      "learning_rate": 0.00019825665859564165,
      "loss": 1.8647,
      "step": 69
    },
    {
      "epoch": 0.09929078014184398,
      "grad_norm": 1.075136661529541,
      "learning_rate": 0.00019815980629539953,
      "loss": 1.8156,
      "step": 70
    },
    {
      "epoch": 0.10070921985815603,
      "grad_norm": 1.114687204360962,
      "learning_rate": 0.0001980629539951574,
      "loss": 2.0969,
      "step": 71
    },
    {
      "epoch": 0.10212765957446808,
      "grad_norm": 1.0886708498001099,
      "learning_rate": 0.00019796610169491526,
      "loss": 1.8735,
      "step": 72
    },
    {
      "epoch": 0.10354609929078014,
      "grad_norm": 1.0000802278518677,
      "learning_rate": 0.00019786924939467314,
      "loss": 1.7809,
      "step": 73
    },
    {
      "epoch": 0.1049645390070922,
      "grad_norm": 1.0024892091751099,
      "learning_rate": 0.00019777239709443102,
      "loss": 1.8654,
      "step": 74
    },
    {
      "epoch": 0.10638297872340426,
      "grad_norm": 1.0471751689910889,
      "learning_rate": 0.00019767554479418887,
      "loss": 1.8414,
      "step": 75
    },
    {
      "epoch": 0.10780141843971631,
      "grad_norm": 1.0259408950805664,
      "learning_rate": 0.00019757869249394675,
      "loss": 1.8887,
      "step": 76
    },
    {
      "epoch": 0.10921985815602837,
      "grad_norm": 1.0418591499328613,
      "learning_rate": 0.00019748184019370462,
      "loss": 1.9078,
      "step": 77
    },
    {
      "epoch": 0.11063829787234042,
      "grad_norm": 1.106501579284668,
      "learning_rate": 0.00019738498789346247,
      "loss": 1.9529,
      "step": 78
    },
    {
      "epoch": 0.11205673758865248,
      "grad_norm": 1.2001898288726807,
      "learning_rate": 0.00019728813559322035,
      "loss": 1.8873,
      "step": 79
    },
    {
      "epoch": 0.11347517730496454,
      "grad_norm": 1.1186670064926147,
      "learning_rate": 0.00019719128329297823,
      "loss": 1.9769,
      "step": 80
    },
    {
      "epoch": 0.1148936170212766,
      "grad_norm": 1.0681841373443604,
      "learning_rate": 0.00019709443099273608,
      "loss": 1.932,
      "step": 81
    },
    {
      "epoch": 0.11631205673758865,
      "grad_norm": 1.085942029953003,
      "learning_rate": 0.00019699757869249396,
      "loss": 1.9425,
      "step": 82
    },
    {
      "epoch": 0.11773049645390071,
      "grad_norm": 1.0608960390090942,
      "learning_rate": 0.00019690072639225184,
      "loss": 1.8147,
      "step": 83
    },
    {
      "epoch": 0.11914893617021277,
      "grad_norm": 0.9925984144210815,
      "learning_rate": 0.0001968038740920097,
      "loss": 1.6996,
      "step": 84
    },
    {
      "epoch": 0.12056737588652482,
      "grad_norm": 1.0561087131500244,
      "learning_rate": 0.00019670702179176757,
      "loss": 1.9734,
      "step": 85
    },
    {
      "epoch": 0.12198581560283688,
      "grad_norm": 0.9507638812065125,
      "learning_rate": 0.00019661016949152545,
      "loss": 1.8695,
      "step": 86
    },
    {
      "epoch": 0.12340425531914893,
      "grad_norm": 1.1066452264785767,
      "learning_rate": 0.0001965133171912833,
      "loss": 1.8306,
      "step": 87
    },
    {
      "epoch": 0.12482269503546099,
      "grad_norm": 1.409095287322998,
      "learning_rate": 0.00019641646489104117,
      "loss": 1.9217,
      "step": 88
    },
    {
      "epoch": 0.12624113475177304,
      "grad_norm": 1.0073235034942627,
      "learning_rate": 0.00019631961259079905,
      "loss": 1.871,
      "step": 89
    },
    {
      "epoch": 0.1276595744680851,
      "grad_norm": 1.1069413423538208,
      "learning_rate": 0.0001962227602905569,
      "loss": 2.2092,
      "step": 90
    },
    {
      "epoch": 0.12907801418439716,
      "grad_norm": 0.9206241369247437,
      "learning_rate": 0.00019612590799031478,
      "loss": 1.5993,
      "step": 91
    },
    {
      "epoch": 0.13049645390070921,
      "grad_norm": 0.971748948097229,
      "learning_rate": 0.00019602905569007263,
      "loss": 1.9343,
      "step": 92
    },
    {
      "epoch": 0.13191489361702127,
      "grad_norm": 0.9606374502182007,
      "learning_rate": 0.0001959322033898305,
      "loss": 1.988,
      "step": 93
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 0.992902934551239,
      "learning_rate": 0.0001958353510895884,
      "loss": 1.6357,
      "step": 94
    },
    {
      "epoch": 0.1347517730496454,
      "grad_norm": 0.991494357585907,
      "learning_rate": 0.00019573849878934624,
      "loss": 1.929,
      "step": 95
    },
    {
      "epoch": 0.13617021276595745,
      "grad_norm": 0.9323453903198242,
      "learning_rate": 0.00019564164648910412,
      "loss": 1.9671,
      "step": 96
    },
    {
      "epoch": 0.1375886524822695,
      "grad_norm": 1.1381146907806396,
      "learning_rate": 0.000195544794188862,
      "loss": 1.6889,
      "step": 97
    },
    {
      "epoch": 0.13900709219858157,
      "grad_norm": 0.942345380783081,
      "learning_rate": 0.00019544794188861985,
      "loss": 1.8009,
      "step": 98
    },
    {
      "epoch": 0.14042553191489363,
      "grad_norm": 1.1215462684631348,
      "learning_rate": 0.00019535108958837773,
      "loss": 1.9217,
      "step": 99
    },
    {
      "epoch": 0.14184397163120568,
      "grad_norm": 1.0809898376464844,
      "learning_rate": 0.0001952542372881356,
      "loss": 2.0826,
      "step": 100
    },
    {
      "epoch": 0.14326241134751774,
      "grad_norm": 0.9221402406692505,
      "learning_rate": 0.00019515738498789345,
      "loss": 1.8363,
      "step": 101
    },
    {
      "epoch": 0.14468085106382977,
      "grad_norm": 1.1425656080245972,
      "learning_rate": 0.00019506053268765133,
      "loss": 1.9013,
      "step": 102
    },
    {
      "epoch": 0.14609929078014183,
      "grad_norm": 0.9840394258499146,
      "learning_rate": 0.0001949636803874092,
      "loss": 1.8085,
      "step": 103
    },
    {
      "epoch": 0.1475177304964539,
      "grad_norm": 1.1152970790863037,
      "learning_rate": 0.00019486682808716706,
      "loss": 2.0811,
      "step": 104
    },
    {
      "epoch": 0.14893617021276595,
      "grad_norm": 0.9925697445869446,
      "learning_rate": 0.00019476997578692494,
      "loss": 1.7898,
      "step": 105
    },
    {
      "epoch": 0.150354609929078,
      "grad_norm": 0.9788102507591248,
      "learning_rate": 0.00019467312348668282,
      "loss": 1.8498,
      "step": 106
    },
    {
      "epoch": 0.15177304964539007,
      "grad_norm": 1.0104581117630005,
      "learning_rate": 0.0001945762711864407,
      "loss": 1.7161,
      "step": 107
    },
    {
      "epoch": 0.15319148936170213,
      "grad_norm": 1.0371237993240356,
      "learning_rate": 0.00019447941888619857,
      "loss": 2.1101,
      "step": 108
    },
    {
      "epoch": 0.15460992907801419,
      "grad_norm": 1.0925321578979492,
      "learning_rate": 0.00019438256658595643,
      "loss": 2.0603,
      "step": 109
    },
    {
      "epoch": 0.15602836879432624,
      "grad_norm": 0.9612079858779907,
      "learning_rate": 0.0001942857142857143,
      "loss": 1.8853,
      "step": 110
    },
    {
      "epoch": 0.1574468085106383,
      "grad_norm": 1.045314073562622,
      "learning_rate": 0.00019418886198547218,
      "loss": 1.715,
      "step": 111
    },
    {
      "epoch": 0.15886524822695036,
      "grad_norm": 0.9515319466590881,
      "learning_rate": 0.00019409200968523003,
      "loss": 1.9728,
      "step": 112
    },
    {
      "epoch": 0.16028368794326242,
      "grad_norm": 0.9624865055084229,
      "learning_rate": 0.0001939951573849879,
      "loss": 1.9609,
      "step": 113
    },
    {
      "epoch": 0.16170212765957448,
      "grad_norm": 1.0090194940567017,
      "learning_rate": 0.0001938983050847458,
      "loss": 1.9138,
      "step": 114
    },
    {
      "epoch": 0.16312056737588654,
      "grad_norm": 0.997510552406311,
      "learning_rate": 0.00019380145278450364,
      "loss": 1.8258,
      "step": 115
    },
    {
      "epoch": 0.16453900709219857,
      "grad_norm": 1.1058714389801025,
      "learning_rate": 0.00019370460048426152,
      "loss": 1.9574,
      "step": 116
    },
    {
      "epoch": 0.16595744680851063,
      "grad_norm": 0.9852263927459717,
      "learning_rate": 0.0001936077481840194,
      "loss": 1.9388,
      "step": 117
    },
    {
      "epoch": 0.1673758865248227,
      "grad_norm": 1.0362639427185059,
      "learning_rate": 0.00019351089588377725,
      "loss": 1.9979,
      "step": 118
    },
    {
      "epoch": 0.16879432624113475,
      "grad_norm": 0.9582896828651428,
      "learning_rate": 0.00019341404358353513,
      "loss": 1.9451,
      "step": 119
    },
    {
      "epoch": 0.1702127659574468,
      "grad_norm": 1.2235652208328247,
      "learning_rate": 0.000193317191283293,
      "loss": 1.9965,
      "step": 120
    },
    {
      "epoch": 0.17163120567375886,
      "grad_norm": 0.9667586088180542,
      "learning_rate": 0.00019322033898305085,
      "loss": 2.0728,
      "step": 121
    },
    {
      "epoch": 0.17304964539007092,
      "grad_norm": 0.9890563488006592,
      "learning_rate": 0.00019312348668280873,
      "loss": 1.9984,
      "step": 122
    },
    {
      "epoch": 0.17446808510638298,
      "grad_norm": 0.9136565923690796,
      "learning_rate": 0.00019302663438256658,
      "loss": 1.9968,
      "step": 123
    },
    {
      "epoch": 0.17588652482269504,
      "grad_norm": 1.1284838914871216,
      "learning_rate": 0.00019292978208232446,
      "loss": 2.0538,
      "step": 124
    },
    {
      "epoch": 0.1773049645390071,
      "grad_norm": 0.9669015407562256,
      "learning_rate": 0.00019283292978208234,
      "loss": 1.9323,
      "step": 125
    },
    {
      "epoch": 0.17872340425531916,
      "grad_norm": 0.9495101571083069,
      "learning_rate": 0.0001927360774818402,
      "loss": 1.9049,
      "step": 126
    },
    {
      "epoch": 0.18014184397163122,
      "grad_norm": 0.9509551525115967,
      "learning_rate": 0.00019263922518159807,
      "loss": 1.9258,
      "step": 127
    },
    {
      "epoch": 0.18156028368794327,
      "grad_norm": 0.9739115834236145,
      "learning_rate": 0.00019254237288135595,
      "loss": 1.7723,
      "step": 128
    },
    {
      "epoch": 0.1829787234042553,
      "grad_norm": 0.9413913488388062,
      "learning_rate": 0.0001924455205811138,
      "loss": 1.8601,
      "step": 129
    },
    {
      "epoch": 0.18439716312056736,
      "grad_norm": 1.0929319858551025,
      "learning_rate": 0.00019234866828087168,
      "loss": 2.0404,
      "step": 130
    },
    {
      "epoch": 0.18581560283687942,
      "grad_norm": 0.9712840914726257,
      "learning_rate": 0.00019225181598062955,
      "loss": 1.6331,
      "step": 131
    },
    {
      "epoch": 0.18723404255319148,
      "grad_norm": 1.0864912271499634,
      "learning_rate": 0.0001921549636803874,
      "loss": 1.8071,
      "step": 132
    },
    {
      "epoch": 0.18865248226950354,
      "grad_norm": 1.011907696723938,
      "learning_rate": 0.00019205811138014528,
      "loss": 1.7877,
      "step": 133
    },
    {
      "epoch": 0.1900709219858156,
      "grad_norm": 1.0717408657073975,
      "learning_rate": 0.00019196125907990316,
      "loss": 2.0539,
      "step": 134
    },
    {
      "epoch": 0.19148936170212766,
      "grad_norm": 0.9419696927070618,
      "learning_rate": 0.000191864406779661,
      "loss": 1.6857,
      "step": 135
    },
    {
      "epoch": 0.19290780141843972,
      "grad_norm": 1.008816123008728,
      "learning_rate": 0.0001917675544794189,
      "loss": 1.7538,
      "step": 136
    },
    {
      "epoch": 0.19432624113475178,
      "grad_norm": 0.954567551612854,
      "learning_rate": 0.00019167070217917677,
      "loss": 1.7457,
      "step": 137
    },
    {
      "epoch": 0.19574468085106383,
      "grad_norm": 1.0287903547286987,
      "learning_rate": 0.00019157384987893462,
      "loss": 1.8841,
      "step": 138
    },
    {
      "epoch": 0.1971631205673759,
      "grad_norm": 1.0585139989852905,
      "learning_rate": 0.0001914769975786925,
      "loss": 1.831,
      "step": 139
    },
    {
      "epoch": 0.19858156028368795,
      "grad_norm": 0.8999666571617126,
      "learning_rate": 0.00019138014527845035,
      "loss": 1.7679,
      "step": 140
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.9993060231208801,
      "learning_rate": 0.00019128329297820823,
      "loss": 1.7601,
      "step": 141
    },
    {
      "epoch": 0.20141843971631207,
      "grad_norm": 1.0174119472503662,
      "learning_rate": 0.0001911864406779661,
      "loss": 2.0544,
      "step": 142
    },
    {
      "epoch": 0.2028368794326241,
      "grad_norm": 0.9215055704116821,
      "learning_rate": 0.00019108958837772398,
      "loss": 1.7231,
      "step": 143
    },
    {
      "epoch": 0.20425531914893616,
      "grad_norm": 1.06234610080719,
      "learning_rate": 0.00019099273607748186,
      "loss": 1.9472,
      "step": 144
    },
    {
      "epoch": 0.20567375886524822,
      "grad_norm": 0.9426120519638062,
      "learning_rate": 0.0001908958837772397,
      "loss": 1.8451,
      "step": 145
    },
    {
      "epoch": 0.20709219858156028,
      "grad_norm": 0.9534973502159119,
      "learning_rate": 0.0001907990314769976,
      "loss": 1.9874,
      "step": 146
    },
    {
      "epoch": 0.20851063829787234,
      "grad_norm": 0.9999266266822815,
      "learning_rate": 0.00019070217917675547,
      "loss": 1.9135,
      "step": 147
    },
    {
      "epoch": 0.2099290780141844,
      "grad_norm": 0.9851370453834534,
      "learning_rate": 0.00019060532687651335,
      "loss": 1.9419,
      "step": 148
    },
    {
      "epoch": 0.21134751773049645,
      "grad_norm": 1.1135103702545166,
      "learning_rate": 0.0001905084745762712,
      "loss": 2.2241,
      "step": 149
    },
    {
      "epoch": 0.2127659574468085,
      "grad_norm": 1.024118423461914,
      "learning_rate": 0.00019041162227602908,
      "loss": 1.8521,
      "step": 150
    },
    {
      "epoch": 0.21418439716312057,
      "grad_norm": 0.9475961923599243,
      "learning_rate": 0.00019031476997578695,
      "loss": 1.8281,
      "step": 151
    },
    {
      "epoch": 0.21560283687943263,
      "grad_norm": 1.0830246210098267,
      "learning_rate": 0.0001902179176755448,
      "loss": 2.0724,
      "step": 152
    },
    {
      "epoch": 0.2170212765957447,
      "grad_norm": 1.0670086145401,
      "learning_rate": 0.00019012106537530268,
      "loss": 2.0665,
      "step": 153
    },
    {
      "epoch": 0.21843971631205675,
      "grad_norm": 0.9611037969589233,
      "learning_rate": 0.00019002421307506053,
      "loss": 1.8794,
      "step": 154
    },
    {
      "epoch": 0.2198581560283688,
      "grad_norm": 0.9807606935501099,
      "learning_rate": 0.0001899273607748184,
      "loss": 1.7585,
      "step": 155
    },
    {
      "epoch": 0.22127659574468084,
      "grad_norm": 0.89825040102005,
      "learning_rate": 0.0001898305084745763,
      "loss": 1.715,
      "step": 156
    },
    {
      "epoch": 0.2226950354609929,
      "grad_norm": 1.0388842821121216,
      "learning_rate": 0.00018973365617433414,
      "loss": 2.0931,
      "step": 157
    },
    {
      "epoch": 0.22411347517730495,
      "grad_norm": 0.9821796417236328,
      "learning_rate": 0.00018963680387409202,
      "loss": 1.8478,
      "step": 158
    },
    {
      "epoch": 0.225531914893617,
      "grad_norm": 0.9460282921791077,
      "learning_rate": 0.0001895399515738499,
      "loss": 1.913,
      "step": 159
    },
    {
      "epoch": 0.22695035460992907,
      "grad_norm": 0.9254499673843384,
      "learning_rate": 0.00018944309927360775,
      "loss": 1.9416,
      "step": 160
    },
    {
      "epoch": 0.22836879432624113,
      "grad_norm": 0.980393648147583,
      "learning_rate": 0.00018934624697336563,
      "loss": 1.8052,
      "step": 161
    },
    {
      "epoch": 0.2297872340425532,
      "grad_norm": 0.9918868541717529,
      "learning_rate": 0.0001892493946731235,
      "loss": 1.9824,
      "step": 162
    },
    {
      "epoch": 0.23120567375886525,
      "grad_norm": 0.9574214816093445,
      "learning_rate": 0.00018915254237288136,
      "loss": 1.7779,
      "step": 163
    },
    {
      "epoch": 0.2326241134751773,
      "grad_norm": 0.955186128616333,
      "learning_rate": 0.00018905569007263923,
      "loss": 1.7754,
      "step": 164
    },
    {
      "epoch": 0.23404255319148937,
      "grad_norm": 1.004132866859436,
      "learning_rate": 0.0001889588377723971,
      "loss": 1.883,
      "step": 165
    },
    {
      "epoch": 0.23546099290780143,
      "grad_norm": 0.8997734785079956,
      "learning_rate": 0.00018886198547215496,
      "loss": 2.0024,
      "step": 166
    },
    {
      "epoch": 0.23687943262411348,
      "grad_norm": 0.9180673956871033,
      "learning_rate": 0.00018876513317191284,
      "loss": 1.8712,
      "step": 167
    },
    {
      "epoch": 0.23829787234042554,
      "grad_norm": 1.0329842567443848,
      "learning_rate": 0.00018866828087167072,
      "loss": 2.1347,
      "step": 168
    },
    {
      "epoch": 0.2397163120567376,
      "grad_norm": 1.0167003870010376,
      "learning_rate": 0.00018857142857142857,
      "loss": 1.8339,
      "step": 169
    },
    {
      "epoch": 0.24113475177304963,
      "grad_norm": 1.0670790672302246,
      "learning_rate": 0.00018847457627118645,
      "loss": 1.9222,
      "step": 170
    },
    {
      "epoch": 0.2425531914893617,
      "grad_norm": 0.9621996879577637,
      "learning_rate": 0.0001883777239709443,
      "loss": 1.9693,
      "step": 171
    },
    {
      "epoch": 0.24397163120567375,
      "grad_norm": 0.928979754447937,
      "learning_rate": 0.00018828087167070218,
      "loss": 1.737,
      "step": 172
    },
    {
      "epoch": 0.2453900709219858,
      "grad_norm": 1.0045229196548462,
      "learning_rate": 0.00018818401937046006,
      "loss": 1.9461,
      "step": 173
    },
    {
      "epoch": 0.24680851063829787,
      "grad_norm": 1.0096064805984497,
      "learning_rate": 0.0001880871670702179,
      "loss": 1.8946,
      "step": 174
    },
    {
      "epoch": 0.24822695035460993,
      "grad_norm": 1.0389715433120728,
      "learning_rate": 0.00018799031476997578,
      "loss": 1.9128,
      "step": 175
    },
    {
      "epoch": 0.24964539007092199,
      "grad_norm": 0.999501645565033,
      "learning_rate": 0.00018789346246973366,
      "loss": 1.8569,
      "step": 176
    },
    {
      "epoch": 0.251063829787234,
      "grad_norm": 0.9583911895751953,
      "learning_rate": 0.00018779661016949151,
      "loss": 1.8501,
      "step": 177
    },
    {
      "epoch": 0.2524822695035461,
      "grad_norm": 0.9775153994560242,
      "learning_rate": 0.0001876997578692494,
      "loss": 1.6771,
      "step": 178
    },
    {
      "epoch": 0.25390070921985813,
      "grad_norm": 0.9665774703025818,
      "learning_rate": 0.00018760290556900727,
      "loss": 1.8818,
      "step": 179
    },
    {
      "epoch": 0.2553191489361702,
      "grad_norm": 0.9724550247192383,
      "learning_rate": 0.00018750605326876515,
      "loss": 2.1773,
      "step": 180
    },
    {
      "epoch": 0.25673758865248225,
      "grad_norm": 1.1561572551727295,
      "learning_rate": 0.00018740920096852303,
      "loss": 1.8608,
      "step": 181
    },
    {
      "epoch": 0.2581560283687943,
      "grad_norm": 1.0202478170394897,
      "learning_rate": 0.00018731234866828088,
      "loss": 1.7356,
      "step": 182
    },
    {
      "epoch": 0.25957446808510637,
      "grad_norm": 1.0430041551589966,
      "learning_rate": 0.00018721549636803876,
      "loss": 1.9362,
      "step": 183
    },
    {
      "epoch": 0.26099290780141843,
      "grad_norm": 1.038509488105774,
      "learning_rate": 0.00018711864406779663,
      "loss": 2.1047,
      "step": 184
    },
    {
      "epoch": 0.2624113475177305,
      "grad_norm": 1.0352694988250732,
      "learning_rate": 0.00018702179176755448,
      "loss": 1.9668,
      "step": 185
    },
    {
      "epoch": 0.26382978723404255,
      "grad_norm": 1.0145736932754517,
      "learning_rate": 0.00018692493946731236,
      "loss": 1.9073,
      "step": 186
    },
    {
      "epoch": 0.2652482269503546,
      "grad_norm": 0.985048770904541,
      "learning_rate": 0.00018682808716707024,
      "loss": 1.7562,
      "step": 187
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 1.057384967803955,
      "learning_rate": 0.0001867312348668281,
      "loss": 1.8624,
      "step": 188
    },
    {
      "epoch": 0.2680851063829787,
      "grad_norm": 0.9623408317565918,
      "learning_rate": 0.00018663438256658597,
      "loss": 1.7815,
      "step": 189
    },
    {
      "epoch": 0.2695035460992908,
      "grad_norm": 1.0034300088882446,
      "learning_rate": 0.00018653753026634385,
      "loss": 1.9532,
      "step": 190
    },
    {
      "epoch": 0.27092198581560284,
      "grad_norm": 0.9395196437835693,
      "learning_rate": 0.0001864406779661017,
      "loss": 1.6993,
      "step": 191
    },
    {
      "epoch": 0.2723404255319149,
      "grad_norm": 0.9910885095596313,
      "learning_rate": 0.00018634382566585958,
      "loss": 1.7409,
      "step": 192
    },
    {
      "epoch": 0.27375886524822696,
      "grad_norm": 0.9712793827056885,
      "learning_rate": 0.00018624697336561746,
      "loss": 1.7994,
      "step": 193
    },
    {
      "epoch": 0.275177304964539,
      "grad_norm": 0.9951639771461487,
      "learning_rate": 0.0001861501210653753,
      "loss": 1.9165,
      "step": 194
    },
    {
      "epoch": 0.2765957446808511,
      "grad_norm": 0.9464163184165955,
      "learning_rate": 0.00018605326876513318,
      "loss": 1.8001,
      "step": 195
    },
    {
      "epoch": 0.27801418439716313,
      "grad_norm": 1.0139267444610596,
      "learning_rate": 0.00018595641646489106,
      "loss": 1.8521,
      "step": 196
    },
    {
      "epoch": 0.2794326241134752,
      "grad_norm": 0.9855507016181946,
      "learning_rate": 0.00018585956416464891,
      "loss": 1.9984,
      "step": 197
    },
    {
      "epoch": 0.28085106382978725,
      "grad_norm": 1.098187804222107,
      "learning_rate": 0.0001857627118644068,
      "loss": 1.9804,
      "step": 198
    },
    {
      "epoch": 0.2822695035460993,
      "grad_norm": 0.8774276971817017,
      "learning_rate": 0.00018566585956416467,
      "loss": 1.8172,
      "step": 199
    },
    {
      "epoch": 0.28368794326241137,
      "grad_norm": 0.9513270258903503,
      "learning_rate": 0.00018556900726392252,
      "loss": 2.0358,
      "step": 200
    },
    {
      "epoch": 0.2851063829787234,
      "grad_norm": 1.068496823310852,
      "learning_rate": 0.0001854721549636804,
      "loss": 1.589,
      "step": 201
    },
    {
      "epoch": 0.2865248226950355,
      "grad_norm": 0.9528117179870605,
      "learning_rate": 0.00018537530266343825,
      "loss": 1.7981,
      "step": 202
    },
    {
      "epoch": 0.28794326241134754,
      "grad_norm": 0.9149752259254456,
      "learning_rate": 0.00018527845036319613,
      "loss": 1.7106,
      "step": 203
    },
    {
      "epoch": 0.28936170212765955,
      "grad_norm": 1.0679677724838257,
      "learning_rate": 0.000185181598062954,
      "loss": 1.7816,
      "step": 204
    },
    {
      "epoch": 0.2907801418439716,
      "grad_norm": 1.0009958744049072,
      "learning_rate": 0.00018508474576271186,
      "loss": 1.9823,
      "step": 205
    },
    {
      "epoch": 0.29219858156028367,
      "grad_norm": 0.9693969488143921,
      "learning_rate": 0.00018498789346246974,
      "loss": 1.7922,
      "step": 206
    },
    {
      "epoch": 0.2936170212765957,
      "grad_norm": 0.8737604022026062,
      "learning_rate": 0.00018489104116222761,
      "loss": 1.7473,
      "step": 207
    },
    {
      "epoch": 0.2950354609929078,
      "grad_norm": 0.9264819622039795,
      "learning_rate": 0.00018479418886198546,
      "loss": 1.8496,
      "step": 208
    },
    {
      "epoch": 0.29645390070921984,
      "grad_norm": 0.9392140507698059,
      "learning_rate": 0.00018469733656174334,
      "loss": 1.9292,
      "step": 209
    },
    {
      "epoch": 0.2978723404255319,
      "grad_norm": 0.9021729230880737,
      "learning_rate": 0.00018460048426150122,
      "loss": 1.771,
      "step": 210
    },
    {
      "epoch": 0.29929078014184396,
      "grad_norm": 0.9156889319419861,
      "learning_rate": 0.00018450363196125907,
      "loss": 1.7914,
      "step": 211
    },
    {
      "epoch": 0.300709219858156,
      "grad_norm": 0.9815962314605713,
      "learning_rate": 0.00018440677966101695,
      "loss": 2.0523,
      "step": 212
    },
    {
      "epoch": 0.3021276595744681,
      "grad_norm": 0.9518104195594788,
      "learning_rate": 0.00018430992736077483,
      "loss": 1.8942,
      "step": 213
    },
    {
      "epoch": 0.30354609929078014,
      "grad_norm": 1.049188256263733,
      "learning_rate": 0.00018421307506053268,
      "loss": 2.2277,
      "step": 214
    },
    {
      "epoch": 0.3049645390070922,
      "grad_norm": 0.9291843175888062,
      "learning_rate": 0.00018411622276029056,
      "loss": 1.9009,
      "step": 215
    },
    {
      "epoch": 0.30638297872340425,
      "grad_norm": 1.0086026191711426,
      "learning_rate": 0.00018401937046004844,
      "loss": 1.8147,
      "step": 216
    },
    {
      "epoch": 0.3078014184397163,
      "grad_norm": 1.0855687856674194,
      "learning_rate": 0.0001839225181598063,
      "loss": 1.6274,
      "step": 217
    },
    {
      "epoch": 0.30921985815602837,
      "grad_norm": 1.01848304271698,
      "learning_rate": 0.00018382566585956416,
      "loss": 1.9065,
      "step": 218
    },
    {
      "epoch": 0.31063829787234043,
      "grad_norm": 1.115381121635437,
      "learning_rate": 0.00018372881355932204,
      "loss": 1.8363,
      "step": 219
    },
    {
      "epoch": 0.3120567375886525,
      "grad_norm": 1.0715506076812744,
      "learning_rate": 0.00018363196125907992,
      "loss": 1.8608,
      "step": 220
    },
    {
      "epoch": 0.31347517730496455,
      "grad_norm": 0.9929397702217102,
      "learning_rate": 0.0001835351089588378,
      "loss": 1.7621,
      "step": 221
    },
    {
      "epoch": 0.3148936170212766,
      "grad_norm": 1.007621169090271,
      "learning_rate": 0.00018343825665859565,
      "loss": 1.7274,
      "step": 222
    },
    {
      "epoch": 0.31631205673758866,
      "grad_norm": 1.038918137550354,
      "learning_rate": 0.00018334140435835353,
      "loss": 2.0179,
      "step": 223
    },
    {
      "epoch": 0.3177304964539007,
      "grad_norm": 0.9554470777511597,
      "learning_rate": 0.0001832445520581114,
      "loss": 1.8415,
      "step": 224
    },
    {
      "epoch": 0.3191489361702128,
      "grad_norm": 0.9952170848846436,
      "learning_rate": 0.00018314769975786926,
      "loss": 1.8044,
      "step": 225
    },
    {
      "epoch": 0.32056737588652484,
      "grad_norm": 0.9729833006858826,
      "learning_rate": 0.00018305084745762714,
      "loss": 1.8347,
      "step": 226
    },
    {
      "epoch": 0.3219858156028369,
      "grad_norm": 1.0922369956970215,
      "learning_rate": 0.000182953995157385,
      "loss": 2.1871,
      "step": 227
    },
    {
      "epoch": 0.32340425531914896,
      "grad_norm": 1.0044426918029785,
      "learning_rate": 0.00018285714285714286,
      "loss": 1.8984,
      "step": 228
    },
    {
      "epoch": 0.324822695035461,
      "grad_norm": 0.9098760485649109,
      "learning_rate": 0.00018276029055690074,
      "loss": 1.8968,
      "step": 229
    },
    {
      "epoch": 0.3262411347517731,
      "grad_norm": 0.9837214350700378,
      "learning_rate": 0.00018266343825665862,
      "loss": 2.0426,
      "step": 230
    },
    {
      "epoch": 0.3276595744680851,
      "grad_norm": 0.9324159026145935,
      "learning_rate": 0.00018256658595641647,
      "loss": 1.9843,
      "step": 231
    },
    {
      "epoch": 0.32907801418439714,
      "grad_norm": 1.0140657424926758,
      "learning_rate": 0.00018246973365617435,
      "loss": 1.9063,
      "step": 232
    },
    {
      "epoch": 0.3304964539007092,
      "grad_norm": 0.9005314111709595,
      "learning_rate": 0.0001823728813559322,
      "loss": 1.8591,
      "step": 233
    },
    {
      "epoch": 0.33191489361702126,
      "grad_norm": 0.8580217957496643,
      "learning_rate": 0.00018227602905569008,
      "loss": 1.5464,
      "step": 234
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.8500463366508484,
      "learning_rate": 0.00018217917675544796,
      "loss": 1.7719,
      "step": 235
    },
    {
      "epoch": 0.3347517730496454,
      "grad_norm": 0.9767171144485474,
      "learning_rate": 0.0001820823244552058,
      "loss": 2.0819,
      "step": 236
    },
    {
      "epoch": 0.33617021276595743,
      "grad_norm": 0.8938849568367004,
      "learning_rate": 0.00018198547215496369,
      "loss": 1.8295,
      "step": 237
    },
    {
      "epoch": 0.3375886524822695,
      "grad_norm": 0.9511204361915588,
      "learning_rate": 0.00018188861985472156,
      "loss": 1.7266,
      "step": 238
    },
    {
      "epoch": 0.33900709219858155,
      "grad_norm": 0.9522839188575745,
      "learning_rate": 0.00018179176755447942,
      "loss": 1.772,
      "step": 239
    },
    {
      "epoch": 0.3404255319148936,
      "grad_norm": 0.9642924666404724,
      "learning_rate": 0.0001816949152542373,
      "loss": 2.1366,
      "step": 240
    },
    {
      "epoch": 0.34184397163120567,
      "grad_norm": 0.9406882524490356,
      "learning_rate": 0.00018159806295399517,
      "loss": 2.0296,
      "step": 241
    },
    {
      "epoch": 0.3432624113475177,
      "grad_norm": 0.967184841632843,
      "learning_rate": 0.00018150121065375302,
      "loss": 1.8968,
      "step": 242
    },
    {
      "epoch": 0.3446808510638298,
      "grad_norm": 0.893243134021759,
      "learning_rate": 0.0001814043583535109,
      "loss": 1.7,
      "step": 243
    },
    {
      "epoch": 0.34609929078014184,
      "grad_norm": 0.9362541437149048,
      "learning_rate": 0.00018130750605326878,
      "loss": 1.9237,
      "step": 244
    },
    {
      "epoch": 0.3475177304964539,
      "grad_norm": 0.9011443257331848,
      "learning_rate": 0.00018121065375302663,
      "loss": 1.7482,
      "step": 245
    },
    {
      "epoch": 0.34893617021276596,
      "grad_norm": 0.9381369948387146,
      "learning_rate": 0.0001811138014527845,
      "loss": 2.0529,
      "step": 246
    },
    {
      "epoch": 0.350354609929078,
      "grad_norm": 0.9607241153717041,
      "learning_rate": 0.00018101694915254239,
      "loss": 1.7078,
      "step": 247
    },
    {
      "epoch": 0.3517730496453901,
      "grad_norm": 1.008245825767517,
      "learning_rate": 0.00018092009685230024,
      "loss": 1.6759,
      "step": 248
    },
    {
      "epoch": 0.35319148936170214,
      "grad_norm": 0.9571892023086548,
      "learning_rate": 0.00018082324455205812,
      "loss": 2.0354,
      "step": 249
    },
    {
      "epoch": 0.3546099290780142,
      "grad_norm": 0.9735412001609802,
      "learning_rate": 0.00018072639225181597,
      "loss": 1.6914,
      "step": 250
    },
    {
      "epoch": 0.35602836879432626,
      "grad_norm": 1.0446652173995972,
      "learning_rate": 0.00018062953995157384,
      "loss": 1.8721,
      "step": 251
    },
    {
      "epoch": 0.3574468085106383,
      "grad_norm": 1.3239846229553223,
      "learning_rate": 0.00018053268765133172,
      "loss": 1.9019,
      "step": 252
    },
    {
      "epoch": 0.3588652482269504,
      "grad_norm": 0.9775428175926208,
      "learning_rate": 0.0001804358353510896,
      "loss": 1.6349,
      "step": 253
    },
    {
      "epoch": 0.36028368794326243,
      "grad_norm": 1.0675644874572754,
      "learning_rate": 0.00018033898305084748,
      "loss": 2.192,
      "step": 254
    },
    {
      "epoch": 0.3617021276595745,
      "grad_norm": 0.9878771305084229,
      "learning_rate": 0.00018024213075060533,
      "loss": 1.7361,
      "step": 255
    },
    {
      "epoch": 0.36312056737588655,
      "grad_norm": 0.8959107995033264,
      "learning_rate": 0.0001801452784503632,
      "loss": 1.6891,
      "step": 256
    },
    {
      "epoch": 0.3645390070921986,
      "grad_norm": 0.9647307991981506,
      "learning_rate": 0.00018004842615012109,
      "loss": 1.7116,
      "step": 257
    },
    {
      "epoch": 0.3659574468085106,
      "grad_norm": 1.0258516073226929,
      "learning_rate": 0.00017995157384987896,
      "loss": 2.0333,
      "step": 258
    },
    {
      "epoch": 0.36737588652482267,
      "grad_norm": 0.904725193977356,
      "learning_rate": 0.00017985472154963681,
      "loss": 1.765,
      "step": 259
    },
    {
      "epoch": 0.36879432624113473,
      "grad_norm": 0.9768916964530945,
      "learning_rate": 0.0001797578692493947,
      "loss": 1.8871,
      "step": 260
    },
    {
      "epoch": 0.3702127659574468,
      "grad_norm": 0.9332278966903687,
      "learning_rate": 0.00017966101694915257,
      "loss": 1.8291,
      "step": 261
    },
    {
      "epoch": 0.37163120567375885,
      "grad_norm": 0.978183925151825,
      "learning_rate": 0.00017956416464891042,
      "loss": 1.8143,
      "step": 262
    },
    {
      "epoch": 0.3730496453900709,
      "grad_norm": 0.9541735649108887,
      "learning_rate": 0.0001794673123486683,
      "loss": 1.8159,
      "step": 263
    },
    {
      "epoch": 0.37446808510638296,
      "grad_norm": 0.9887608885765076,
      "learning_rate": 0.00017937046004842615,
      "loss": 1.9049,
      "step": 264
    },
    {
      "epoch": 0.375886524822695,
      "grad_norm": 0.9812231659889221,
      "learning_rate": 0.00017927360774818403,
      "loss": 1.8132,
      "step": 265
    },
    {
      "epoch": 0.3773049645390071,
      "grad_norm": 0.9784431457519531,
      "learning_rate": 0.0001791767554479419,
      "loss": 1.8101,
      "step": 266
    },
    {
      "epoch": 0.37872340425531914,
      "grad_norm": 0.9982566833496094,
      "learning_rate": 0.00017907990314769976,
      "loss": 1.7921,
      "step": 267
    },
    {
      "epoch": 0.3801418439716312,
      "grad_norm": 0.9074917435646057,
      "learning_rate": 0.00017898305084745764,
      "loss": 1.9321,
      "step": 268
    },
    {
      "epoch": 0.38156028368794326,
      "grad_norm": 1.0277540683746338,
      "learning_rate": 0.00017888619854721551,
      "loss": 2.126,
      "step": 269
    },
    {
      "epoch": 0.3829787234042553,
      "grad_norm": 0.8828142285346985,
      "learning_rate": 0.00017878934624697337,
      "loss": 1.5719,
      "step": 270
    },
    {
      "epoch": 0.3843971631205674,
      "grad_norm": 0.9607075452804565,
      "learning_rate": 0.00017869249394673124,
      "loss": 1.7884,
      "step": 271
    },
    {
      "epoch": 0.38581560283687943,
      "grad_norm": 0.9126862287521362,
      "learning_rate": 0.00017859564164648912,
      "loss": 1.9482,
      "step": 272
    },
    {
      "epoch": 0.3872340425531915,
      "grad_norm": 0.9614412188529968,
      "learning_rate": 0.00017849878934624697,
      "loss": 1.7178,
      "step": 273
    },
    {
      "epoch": 0.38865248226950355,
      "grad_norm": 0.9119043350219727,
      "learning_rate": 0.00017840193704600485,
      "loss": 1.8499,
      "step": 274
    },
    {
      "epoch": 0.3900709219858156,
      "grad_norm": 1.0984479188919067,
      "learning_rate": 0.00017830508474576273,
      "loss": 1.759,
      "step": 275
    },
    {
      "epoch": 0.39148936170212767,
      "grad_norm": 0.9465433955192566,
      "learning_rate": 0.00017820823244552058,
      "loss": 1.807,
      "step": 276
    },
    {
      "epoch": 0.39290780141843973,
      "grad_norm": 0.9199845194816589,
      "learning_rate": 0.00017811138014527846,
      "loss": 1.8849,
      "step": 277
    },
    {
      "epoch": 0.3943262411347518,
      "grad_norm": 0.9319270849227905,
      "learning_rate": 0.00017801452784503634,
      "loss": 1.859,
      "step": 278
    },
    {
      "epoch": 0.39574468085106385,
      "grad_norm": 0.949927031993866,
      "learning_rate": 0.0001779176755447942,
      "loss": 1.9239,
      "step": 279
    },
    {
      "epoch": 0.3971631205673759,
      "grad_norm": 0.9235413074493408,
      "learning_rate": 0.00017782082324455207,
      "loss": 1.8989,
      "step": 280
    },
    {
      "epoch": 0.39858156028368796,
      "grad_norm": 0.943297266960144,
      "learning_rate": 0.00017772397094430992,
      "loss": 1.8911,
      "step": 281
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.9669685959815979,
      "learning_rate": 0.0001776271186440678,
      "loss": 1.8185,
      "step": 282
    },
    {
      "epoch": 0.4014184397163121,
      "grad_norm": 0.9418421387672424,
      "learning_rate": 0.00017753026634382567,
      "loss": 1.8185,
      "step": 283
    },
    {
      "epoch": 0.40283687943262414,
      "grad_norm": 1.0247584581375122,
      "learning_rate": 0.00017743341404358352,
      "loss": 1.8827,
      "step": 284
    },
    {
      "epoch": 0.40425531914893614,
      "grad_norm": 1.0367324352264404,
      "learning_rate": 0.0001773365617433414,
      "loss": 1.9937,
      "step": 285
    },
    {
      "epoch": 0.4056737588652482,
      "grad_norm": 1.0447474718093872,
      "learning_rate": 0.00017723970944309928,
      "loss": 1.924,
      "step": 286
    },
    {
      "epoch": 0.40709219858156026,
      "grad_norm": 0.9520232677459717,
      "learning_rate": 0.00017714285714285713,
      "loss": 1.936,
      "step": 287
    },
    {
      "epoch": 0.4085106382978723,
      "grad_norm": 0.9429634213447571,
      "learning_rate": 0.000177046004842615,
      "loss": 1.9339,
      "step": 288
    },
    {
      "epoch": 0.4099290780141844,
      "grad_norm": 0.9821912050247192,
      "learning_rate": 0.0001769491525423729,
      "loss": 2.0034,
      "step": 289
    },
    {
      "epoch": 0.41134751773049644,
      "grad_norm": 0.9924930930137634,
      "learning_rate": 0.00017685230024213077,
      "loss": 1.8511,
      "step": 290
    },
    {
      "epoch": 0.4127659574468085,
      "grad_norm": 1.0038831233978271,
      "learning_rate": 0.00017675544794188862,
      "loss": 2.1056,
      "step": 291
    },
    {
      "epoch": 0.41418439716312055,
      "grad_norm": 0.9564592838287354,
      "learning_rate": 0.0001766585956416465,
      "loss": 1.8863,
      "step": 292
    },
    {
      "epoch": 0.4156028368794326,
      "grad_norm": 0.9334070086479187,
      "learning_rate": 0.00017656174334140437,
      "loss": 1.8718,
      "step": 293
    },
    {
      "epoch": 0.41702127659574467,
      "grad_norm": 0.8720489144325256,
      "learning_rate": 0.00017646489104116225,
      "loss": 1.8468,
      "step": 294
    },
    {
      "epoch": 0.41843971631205673,
      "grad_norm": 0.8848581314086914,
      "learning_rate": 0.00017636803874092013,
      "loss": 1.8084,
      "step": 295
    },
    {
      "epoch": 0.4198581560283688,
      "grad_norm": 0.9114629626274109,
      "learning_rate": 0.00017627118644067798,
      "loss": 1.9487,
      "step": 296
    },
    {
      "epoch": 0.42127659574468085,
      "grad_norm": 0.9775903820991516,
      "learning_rate": 0.00017617433414043586,
      "loss": 1.7691,
      "step": 297
    },
    {
      "epoch": 0.4226950354609929,
      "grad_norm": 1.0030083656311035,
      "learning_rate": 0.0001760774818401937,
      "loss": 1.7996,
      "step": 298
    },
    {
      "epoch": 0.42411347517730497,
      "grad_norm": 0.9292450547218323,
      "learning_rate": 0.0001759806295399516,
      "loss": 1.7146,
      "step": 299
    },
    {
      "epoch": 0.425531914893617,
      "grad_norm": 0.9759061336517334,
      "learning_rate": 0.00017588377723970947,
      "loss": 1.7757,
      "step": 300
    },
    {
      "epoch": 0.4269503546099291,
      "grad_norm": 0.9845147132873535,
      "learning_rate": 0.00017578692493946732,
      "loss": 1.768,
      "step": 301
    },
    {
      "epoch": 0.42836879432624114,
      "grad_norm": 0.9506266713142395,
      "learning_rate": 0.0001756900726392252,
      "loss": 2.0084,
      "step": 302
    },
    {
      "epoch": 0.4297872340425532,
      "grad_norm": 0.9350481033325195,
      "learning_rate": 0.00017559322033898307,
      "loss": 1.9765,
      "step": 303
    },
    {
      "epoch": 0.43120567375886526,
      "grad_norm": 0.9896640181541443,
      "learning_rate": 0.00017549636803874092,
      "loss": 1.893,
      "step": 304
    },
    {
      "epoch": 0.4326241134751773,
      "grad_norm": 0.9888864159584045,
      "learning_rate": 0.0001753995157384988,
      "loss": 1.88,
      "step": 305
    },
    {
      "epoch": 0.4340425531914894,
      "grad_norm": 0.937113881111145,
      "learning_rate": 0.00017530266343825668,
      "loss": 1.8432,
      "step": 306
    },
    {
      "epoch": 0.43546099290780144,
      "grad_norm": 0.9765783548355103,
      "learning_rate": 0.00017520581113801453,
      "loss": 1.7804,
      "step": 307
    },
    {
      "epoch": 0.4368794326241135,
      "grad_norm": 0.9332955479621887,
      "learning_rate": 0.0001751089588377724,
      "loss": 1.8956,
      "step": 308
    },
    {
      "epoch": 0.43829787234042555,
      "grad_norm": 0.9328266382217407,
      "learning_rate": 0.0001750121065375303,
      "loss": 1.782,
      "step": 309
    },
    {
      "epoch": 0.4397163120567376,
      "grad_norm": 0.9297088980674744,
      "learning_rate": 0.00017491525423728814,
      "loss": 1.805,
      "step": 310
    },
    {
      "epoch": 0.44113475177304967,
      "grad_norm": 0.9776032567024231,
      "learning_rate": 0.00017481840193704602,
      "loss": 1.8934,
      "step": 311
    },
    {
      "epoch": 0.4425531914893617,
      "grad_norm": 0.9452140927314758,
      "learning_rate": 0.00017472154963680387,
      "loss": 1.8798,
      "step": 312
    },
    {
      "epoch": 0.44397163120567373,
      "grad_norm": 1.030831217765808,
      "learning_rate": 0.00017462469733656175,
      "loss": 1.7181,
      "step": 313
    },
    {
      "epoch": 0.4453900709219858,
      "grad_norm": 0.8747674226760864,
      "learning_rate": 0.00017452784503631962,
      "loss": 1.6531,
      "step": 314
    },
    {
      "epoch": 0.44680851063829785,
      "grad_norm": 0.9434899687767029,
      "learning_rate": 0.00017443099273607747,
      "loss": 1.7733,
      "step": 315
    },
    {
      "epoch": 0.4482269503546099,
      "grad_norm": 0.9592564105987549,
      "learning_rate": 0.00017433414043583535,
      "loss": 2.0461,
      "step": 316
    },
    {
      "epoch": 0.44964539007092197,
      "grad_norm": 0.9413037300109863,
      "learning_rate": 0.00017423728813559323,
      "loss": 1.9054,
      "step": 317
    },
    {
      "epoch": 0.451063829787234,
      "grad_norm": 0.9602166414260864,
      "learning_rate": 0.00017414043583535108,
      "loss": 1.8483,
      "step": 318
    },
    {
      "epoch": 0.4524822695035461,
      "grad_norm": 1.0252346992492676,
      "learning_rate": 0.00017404358353510896,
      "loss": 1.8019,
      "step": 319
    },
    {
      "epoch": 0.45390070921985815,
      "grad_norm": 0.9301417469978333,
      "learning_rate": 0.00017394673123486684,
      "loss": 1.7285,
      "step": 320
    },
    {
      "epoch": 0.4553191489361702,
      "grad_norm": 0.9384927153587341,
      "learning_rate": 0.0001738498789346247,
      "loss": 1.6223,
      "step": 321
    },
    {
      "epoch": 0.45673758865248226,
      "grad_norm": 0.9198197722434998,
      "learning_rate": 0.00017375302663438257,
      "loss": 1.7974,
      "step": 322
    },
    {
      "epoch": 0.4581560283687943,
      "grad_norm": 0.9289765954017639,
      "learning_rate": 0.00017365617433414045,
      "loss": 1.6964,
      "step": 323
    },
    {
      "epoch": 0.4595744680851064,
      "grad_norm": 0.9340206384658813,
      "learning_rate": 0.0001735593220338983,
      "loss": 1.9346,
      "step": 324
    },
    {
      "epoch": 0.46099290780141844,
      "grad_norm": 1.032168984413147,
      "learning_rate": 0.00017346246973365617,
      "loss": 1.8882,
      "step": 325
    },
    {
      "epoch": 0.4624113475177305,
      "grad_norm": 1.043074607849121,
      "learning_rate": 0.00017336561743341405,
      "loss": 1.5286,
      "step": 326
    },
    {
      "epoch": 0.46382978723404256,
      "grad_norm": 0.9749189615249634,
      "learning_rate": 0.00017326876513317193,
      "loss": 1.9693,
      "step": 327
    },
    {
      "epoch": 0.4652482269503546,
      "grad_norm": 0.9572228789329529,
      "learning_rate": 0.00017317191283292978,
      "loss": 1.8084,
      "step": 328
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 1.0045562982559204,
      "learning_rate": 0.00017307506053268766,
      "loss": 1.7183,
      "step": 329
    },
    {
      "epoch": 0.46808510638297873,
      "grad_norm": 1.1227355003356934,
      "learning_rate": 0.00017297820823244554,
      "loss": 1.9009,
      "step": 330
    },
    {
      "epoch": 0.4695035460992908,
      "grad_norm": 1.0158571004867554,
      "learning_rate": 0.00017288135593220342,
      "loss": 1.9807,
      "step": 331
    },
    {
      "epoch": 0.47092198581560285,
      "grad_norm": 0.9610460996627808,
      "learning_rate": 0.00017278450363196127,
      "loss": 1.7399,
      "step": 332
    },
    {
      "epoch": 0.4723404255319149,
      "grad_norm": 0.8958942294120789,
      "learning_rate": 0.00017268765133171915,
      "loss": 1.7994,
      "step": 333
    },
    {
      "epoch": 0.47375886524822697,
      "grad_norm": 0.9412689805030823,
      "learning_rate": 0.00017259079903147702,
      "loss": 2.0594,
      "step": 334
    },
    {
      "epoch": 0.475177304964539,
      "grad_norm": 0.9631602764129639,
      "learning_rate": 0.00017249394673123487,
      "loss": 1.7864,
      "step": 335
    },
    {
      "epoch": 0.4765957446808511,
      "grad_norm": 1.1445562839508057,
      "learning_rate": 0.00017239709443099275,
      "loss": 1.5906,
      "step": 336
    },
    {
      "epoch": 0.47801418439716314,
      "grad_norm": 0.9993076920509338,
      "learning_rate": 0.00017230024213075063,
      "loss": 1.6411,
      "step": 337
    },
    {
      "epoch": 0.4794326241134752,
      "grad_norm": 1.0374830961227417,
      "learning_rate": 0.00017220338983050848,
      "loss": 1.842,
      "step": 338
    },
    {
      "epoch": 0.4808510638297872,
      "grad_norm": 0.973639726638794,
      "learning_rate": 0.00017210653753026636,
      "loss": 1.7878,
      "step": 339
    },
    {
      "epoch": 0.48226950354609927,
      "grad_norm": 0.9438994526863098,
      "learning_rate": 0.00017200968523002424,
      "loss": 2.0517,
      "step": 340
    },
    {
      "epoch": 0.4836879432624113,
      "grad_norm": 0.9525969624519348,
      "learning_rate": 0.0001719128329297821,
      "loss": 1.9102,
      "step": 341
    },
    {
      "epoch": 0.4851063829787234,
      "grad_norm": 0.9234567284584045,
      "learning_rate": 0.00017181598062953997,
      "loss": 1.932,
      "step": 342
    },
    {
      "epoch": 0.48652482269503544,
      "grad_norm": 0.9590464234352112,
      "learning_rate": 0.00017171912832929782,
      "loss": 1.7139,
      "step": 343
    },
    {
      "epoch": 0.4879432624113475,
      "grad_norm": 0.9586837291717529,
      "learning_rate": 0.0001716222760290557,
      "loss": 1.9188,
      "step": 344
    },
    {
      "epoch": 0.48936170212765956,
      "grad_norm": 0.9036353826522827,
      "learning_rate": 0.00017152542372881357,
      "loss": 1.6881,
      "step": 345
    },
    {
      "epoch": 0.4907801418439716,
      "grad_norm": 0.9529016017913818,
      "learning_rate": 0.00017142857142857143,
      "loss": 1.8478,
      "step": 346
    },
    {
      "epoch": 0.4921985815602837,
      "grad_norm": 1.007252812385559,
      "learning_rate": 0.0001713317191283293,
      "loss": 1.8538,
      "step": 347
    },
    {
      "epoch": 0.49361702127659574,
      "grad_norm": 0.945681631565094,
      "learning_rate": 0.00017123486682808718,
      "loss": 1.9226,
      "step": 348
    },
    {
      "epoch": 0.4950354609929078,
      "grad_norm": 0.9263474941253662,
      "learning_rate": 0.00017113801452784503,
      "loss": 1.7189,
      "step": 349
    },
    {
      "epoch": 0.49645390070921985,
      "grad_norm": 0.9747632741928101,
      "learning_rate": 0.0001710411622276029,
      "loss": 1.8628,
      "step": 350
    },
    {
      "epoch": 0.4978723404255319,
      "grad_norm": 0.9846658706665039,
      "learning_rate": 0.0001709443099273608,
      "loss": 1.9134,
      "step": 351
    },
    {
      "epoch": 0.49929078014184397,
      "grad_norm": 0.9113063812255859,
      "learning_rate": 0.00017084745762711864,
      "loss": 1.899,
      "step": 352
    },
    {
      "epoch": 0.500709219858156,
      "grad_norm": 0.9714037179946899,
      "learning_rate": 0.00017075060532687652,
      "loss": 1.9107,
      "step": 353
    },
    {
      "epoch": 0.502127659574468,
      "grad_norm": 0.9537795186042786,
      "learning_rate": 0.0001706537530266344,
      "loss": 1.8053,
      "step": 354
    },
    {
      "epoch": 0.5035460992907801,
      "grad_norm": 0.9485891461372375,
      "learning_rate": 0.00017055690072639225,
      "loss": 1.7612,
      "step": 355
    },
    {
      "epoch": 0.5049645390070922,
      "grad_norm": 0.9222022891044617,
      "learning_rate": 0.00017046004842615013,
      "loss": 1.8346,
      "step": 356
    },
    {
      "epoch": 0.5063829787234042,
      "grad_norm": 0.9114431142807007,
      "learning_rate": 0.000170363196125908,
      "loss": 1.6909,
      "step": 357
    },
    {
      "epoch": 0.5078014184397163,
      "grad_norm": 0.9674013257026672,
      "learning_rate": 0.00017026634382566585,
      "loss": 1.8031,
      "step": 358
    },
    {
      "epoch": 0.5092198581560283,
      "grad_norm": 0.9337841272354126,
      "learning_rate": 0.00017016949152542373,
      "loss": 1.7164,
      "step": 359
    },
    {
      "epoch": 0.5106382978723404,
      "grad_norm": 0.9948791861534119,
      "learning_rate": 0.00017007263922518158,
      "loss": 1.7063,
      "step": 360
    },
    {
      "epoch": 0.5120567375886524,
      "grad_norm": 1.0778685808181763,
      "learning_rate": 0.00016997578692493946,
      "loss": 2.0027,
      "step": 361
    },
    {
      "epoch": 0.5134751773049645,
      "grad_norm": 0.9692330360412598,
      "learning_rate": 0.00016987893462469734,
      "loss": 1.7529,
      "step": 362
    },
    {
      "epoch": 0.5148936170212766,
      "grad_norm": 1.045678734779358,
      "learning_rate": 0.00016978208232445522,
      "loss": 1.908,
      "step": 363
    },
    {
      "epoch": 0.5163120567375886,
      "grad_norm": 1.0215644836425781,
      "learning_rate": 0.00016968523002421307,
      "loss": 2.0764,
      "step": 364
    },
    {
      "epoch": 0.5177304964539007,
      "grad_norm": 0.9777445793151855,
      "learning_rate": 0.00016958837772397095,
      "loss": 1.871,
      "step": 365
    },
    {
      "epoch": 0.5191489361702127,
      "grad_norm": 0.9745745062828064,
      "learning_rate": 0.00016949152542372882,
      "loss": 1.6008,
      "step": 366
    },
    {
      "epoch": 0.5205673758865248,
      "grad_norm": 1.0185343027114868,
      "learning_rate": 0.0001693946731234867,
      "loss": 1.9607,
      "step": 367
    },
    {
      "epoch": 0.5219858156028369,
      "grad_norm": 0.9472505450248718,
      "learning_rate": 0.00016929782082324458,
      "loss": 1.861,
      "step": 368
    },
    {
      "epoch": 0.5234042553191489,
      "grad_norm": 1.0330811738967896,
      "learning_rate": 0.00016920096852300243,
      "loss": 2.0062,
      "step": 369
    },
    {
      "epoch": 0.524822695035461,
      "grad_norm": 0.931767463684082,
      "learning_rate": 0.0001691041162227603,
      "loss": 1.7943,
      "step": 370
    },
    {
      "epoch": 0.526241134751773,
      "grad_norm": 0.8553069829940796,
      "learning_rate": 0.0001690072639225182,
      "loss": 1.7324,
      "step": 371
    },
    {
      "epoch": 0.5276595744680851,
      "grad_norm": 0.8727834224700928,
      "learning_rate": 0.00016891041162227604,
      "loss": 1.7278,
      "step": 372
    },
    {
      "epoch": 0.5290780141843971,
      "grad_norm": 0.8997536301612854,
      "learning_rate": 0.00016881355932203392,
      "loss": 1.8374,
      "step": 373
    },
    {
      "epoch": 0.5304964539007092,
      "grad_norm": 0.9161466360092163,
      "learning_rate": 0.0001687167070217918,
      "loss": 1.8076,
      "step": 374
    },
    {
      "epoch": 0.5319148936170213,
      "grad_norm": 1.0687780380249023,
      "learning_rate": 0.00016861985472154965,
      "loss": 1.5887,
      "step": 375
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 0.8630894422531128,
      "learning_rate": 0.00016852300242130752,
      "loss": 1.7742,
      "step": 376
    },
    {
      "epoch": 0.5347517730496454,
      "grad_norm": 0.998538076877594,
      "learning_rate": 0.00016842615012106538,
      "loss": 1.7454,
      "step": 377
    },
    {
      "epoch": 0.5361702127659574,
      "grad_norm": 0.9754890203475952,
      "learning_rate": 0.00016832929782082325,
      "loss": 1.8326,
      "step": 378
    },
    {
      "epoch": 0.5375886524822695,
      "grad_norm": 0.9351779222488403,
      "learning_rate": 0.00016823244552058113,
      "loss": 1.8356,
      "step": 379
    },
    {
      "epoch": 0.5390070921985816,
      "grad_norm": 0.8755729794502258,
      "learning_rate": 0.00016813559322033898,
      "loss": 1.6331,
      "step": 380
    },
    {
      "epoch": 0.5404255319148936,
      "grad_norm": 1.0290228128433228,
      "learning_rate": 0.00016803874092009686,
      "loss": 2.1401,
      "step": 381
    },
    {
      "epoch": 0.5418439716312057,
      "grad_norm": 0.9561594724655151,
      "learning_rate": 0.00016794188861985474,
      "loss": 1.7978,
      "step": 382
    },
    {
      "epoch": 0.5432624113475177,
      "grad_norm": 0.9548543691635132,
      "learning_rate": 0.0001678450363196126,
      "loss": 1.8855,
      "step": 383
    },
    {
      "epoch": 0.5446808510638298,
      "grad_norm": 0.9470059871673584,
      "learning_rate": 0.00016774818401937047,
      "loss": 1.7313,
      "step": 384
    },
    {
      "epoch": 0.5460992907801419,
      "grad_norm": 0.9736037254333496,
      "learning_rate": 0.00016765133171912835,
      "loss": 1.8428,
      "step": 385
    },
    {
      "epoch": 0.5475177304964539,
      "grad_norm": 0.957306981086731,
      "learning_rate": 0.0001675544794188862,
      "loss": 2.0756,
      "step": 386
    },
    {
      "epoch": 0.548936170212766,
      "grad_norm": 0.9309136867523193,
      "learning_rate": 0.00016745762711864408,
      "loss": 1.8929,
      "step": 387
    },
    {
      "epoch": 0.550354609929078,
      "grad_norm": 0.8406240940093994,
      "learning_rate": 0.00016736077481840195,
      "loss": 1.8103,
      "step": 388
    },
    {
      "epoch": 0.5517730496453901,
      "grad_norm": 0.8818572163581848,
      "learning_rate": 0.0001672639225181598,
      "loss": 1.9621,
      "step": 389
    },
    {
      "epoch": 0.5531914893617021,
      "grad_norm": 1.0767971277236938,
      "learning_rate": 0.00016716707021791768,
      "loss": 1.8136,
      "step": 390
    },
    {
      "epoch": 0.5546099290780142,
      "grad_norm": 0.9289748668670654,
      "learning_rate": 0.00016707021791767553,
      "loss": 1.7687,
      "step": 391
    },
    {
      "epoch": 0.5560283687943263,
      "grad_norm": 0.962541937828064,
      "learning_rate": 0.0001669733656174334,
      "loss": 1.8763,
      "step": 392
    },
    {
      "epoch": 0.5574468085106383,
      "grad_norm": 0.9504688382148743,
      "learning_rate": 0.0001668765133171913,
      "loss": 2.0939,
      "step": 393
    },
    {
      "epoch": 0.5588652482269504,
      "grad_norm": 0.9227321147918701,
      "learning_rate": 0.00016677966101694914,
      "loss": 1.8242,
      "step": 394
    },
    {
      "epoch": 0.5602836879432624,
      "grad_norm": 0.9383020401000977,
      "learning_rate": 0.00016668280871670702,
      "loss": 2.0296,
      "step": 395
    },
    {
      "epoch": 0.5617021276595745,
      "grad_norm": 0.9648865461349487,
      "learning_rate": 0.0001665859564164649,
      "loss": 1.8459,
      "step": 396
    },
    {
      "epoch": 0.5631205673758866,
      "grad_norm": 0.8519643545150757,
      "learning_rate": 0.00016648910411622275,
      "loss": 1.6578,
      "step": 397
    },
    {
      "epoch": 0.5645390070921986,
      "grad_norm": 0.900329053401947,
      "learning_rate": 0.00016639225181598063,
      "loss": 1.8174,
      "step": 398
    },
    {
      "epoch": 0.5659574468085107,
      "grad_norm": 0.8840930461883545,
      "learning_rate": 0.0001662953995157385,
      "loss": 1.6711,
      "step": 399
    },
    {
      "epoch": 0.5673758865248227,
      "grad_norm": 0.9421043992042542,
      "learning_rate": 0.00016619854721549638,
      "loss": 1.9039,
      "step": 400
    },
    {
      "epoch": 0.5687943262411348,
      "grad_norm": 0.9165019989013672,
      "learning_rate": 0.00016610169491525423,
      "loss": 1.8825,
      "step": 401
    },
    {
      "epoch": 0.5702127659574469,
      "grad_norm": 0.9477689862251282,
      "learning_rate": 0.0001660048426150121,
      "loss": 1.7967,
      "step": 402
    },
    {
      "epoch": 0.5716312056737589,
      "grad_norm": 0.9842367172241211,
      "learning_rate": 0.00016590799031477,
      "loss": 1.812,
      "step": 403
    },
    {
      "epoch": 0.573049645390071,
      "grad_norm": 0.9256036281585693,
      "learning_rate": 0.00016581113801452787,
      "loss": 1.7424,
      "step": 404
    },
    {
      "epoch": 0.574468085106383,
      "grad_norm": 0.9276215434074402,
      "learning_rate": 0.00016571428571428575,
      "loss": 1.9102,
      "step": 405
    },
    {
      "epoch": 0.5758865248226951,
      "grad_norm": 0.9226938486099243,
      "learning_rate": 0.0001656174334140436,
      "loss": 1.7025,
      "step": 406
    },
    {
      "epoch": 0.577304964539007,
      "grad_norm": 0.8902654051780701,
      "learning_rate": 0.00016552058111380148,
      "loss": 1.7183,
      "step": 407
    },
    {
      "epoch": 0.5787234042553191,
      "grad_norm": 0.9309189319610596,
      "learning_rate": 0.00016542372881355933,
      "loss": 1.7781,
      "step": 408
    },
    {
      "epoch": 0.5801418439716312,
      "grad_norm": 0.947432279586792,
      "learning_rate": 0.0001653268765133172,
      "loss": 1.7323,
      "step": 409
    },
    {
      "epoch": 0.5815602836879432,
      "grad_norm": 0.9832228422164917,
      "learning_rate": 0.00016523002421307508,
      "loss": 1.9145,
      "step": 410
    },
    {
      "epoch": 0.5829787234042553,
      "grad_norm": 0.9246309995651245,
      "learning_rate": 0.00016513317191283293,
      "loss": 1.6524,
      "step": 411
    },
    {
      "epoch": 0.5843971631205673,
      "grad_norm": 0.9460448622703552,
      "learning_rate": 0.0001650363196125908,
      "loss": 1.7523,
      "step": 412
    },
    {
      "epoch": 0.5858156028368794,
      "grad_norm": 1.0051263570785522,
      "learning_rate": 0.0001649394673123487,
      "loss": 1.8572,
      "step": 413
    },
    {
      "epoch": 0.5872340425531914,
      "grad_norm": 0.9661255478858948,
      "learning_rate": 0.00016484261501210654,
      "loss": 1.8508,
      "step": 414
    },
    {
      "epoch": 0.5886524822695035,
      "grad_norm": 0.9462977647781372,
      "learning_rate": 0.00016474576271186442,
      "loss": 1.8244,
      "step": 415
    },
    {
      "epoch": 0.5900709219858156,
      "grad_norm": 1.021762490272522,
      "learning_rate": 0.0001646489104116223,
      "loss": 1.797,
      "step": 416
    },
    {
      "epoch": 0.5914893617021276,
      "grad_norm": 0.9398526549339294,
      "learning_rate": 0.00016455205811138015,
      "loss": 1.9082,
      "step": 417
    },
    {
      "epoch": 0.5929078014184397,
      "grad_norm": 0.9971896409988403,
      "learning_rate": 0.00016445520581113803,
      "loss": 2.1014,
      "step": 418
    },
    {
      "epoch": 0.5943262411347517,
      "grad_norm": 0.9796718955039978,
      "learning_rate": 0.0001643583535108959,
      "loss": 1.7606,
      "step": 419
    },
    {
      "epoch": 0.5957446808510638,
      "grad_norm": 1.0000969171524048,
      "learning_rate": 0.00016426150121065376,
      "loss": 1.9288,
      "step": 420
    },
    {
      "epoch": 0.5971631205673759,
      "grad_norm": 0.9603239297866821,
      "learning_rate": 0.00016416464891041163,
      "loss": 1.9338,
      "step": 421
    },
    {
      "epoch": 0.5985815602836879,
      "grad_norm": 1.0116310119628906,
      "learning_rate": 0.00016406779661016948,
      "loss": 1.6369,
      "step": 422
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.9344016909599304,
      "learning_rate": 0.00016397094430992736,
      "loss": 1.8784,
      "step": 423
    },
    {
      "epoch": 0.601418439716312,
      "grad_norm": 0.9857602715492249,
      "learning_rate": 0.00016387409200968524,
      "loss": 1.8524,
      "step": 424
    },
    {
      "epoch": 0.6028368794326241,
      "grad_norm": 0.9478918313980103,
      "learning_rate": 0.0001637772397094431,
      "loss": 1.779,
      "step": 425
    },
    {
      "epoch": 0.6042553191489362,
      "grad_norm": 0.9827153086662292,
      "learning_rate": 0.00016368038740920097,
      "loss": 1.8628,
      "step": 426
    },
    {
      "epoch": 0.6056737588652482,
      "grad_norm": 0.9552390575408936,
      "learning_rate": 0.00016358353510895885,
      "loss": 1.9878,
      "step": 427
    },
    {
      "epoch": 0.6070921985815603,
      "grad_norm": 0.8602179884910583,
      "learning_rate": 0.0001634866828087167,
      "loss": 1.5751,
      "step": 428
    },
    {
      "epoch": 0.6085106382978723,
      "grad_norm": 1.0037013292312622,
      "learning_rate": 0.00016338983050847458,
      "loss": 1.9663,
      "step": 429
    },
    {
      "epoch": 0.6099290780141844,
      "grad_norm": 0.9661442637443542,
      "learning_rate": 0.00016329297820823246,
      "loss": 1.6912,
      "step": 430
    },
    {
      "epoch": 0.6113475177304964,
      "grad_norm": 0.9556114673614502,
      "learning_rate": 0.0001631961259079903,
      "loss": 1.7937,
      "step": 431
    },
    {
      "epoch": 0.6127659574468085,
      "grad_norm": 1.1967833042144775,
      "learning_rate": 0.00016309927360774818,
      "loss": 1.938,
      "step": 432
    },
    {
      "epoch": 0.6141843971631206,
      "grad_norm": 0.9502696990966797,
      "learning_rate": 0.00016300242130750606,
      "loss": 1.7276,
      "step": 433
    },
    {
      "epoch": 0.6156028368794326,
      "grad_norm": 1.0521098375320435,
      "learning_rate": 0.0001629055690072639,
      "loss": 1.8143,
      "step": 434
    },
    {
      "epoch": 0.6170212765957447,
      "grad_norm": 0.9686905145645142,
      "learning_rate": 0.0001628087167070218,
      "loss": 1.9932,
      "step": 435
    },
    {
      "epoch": 0.6184397163120567,
      "grad_norm": 0.9943152070045471,
      "learning_rate": 0.00016271186440677967,
      "loss": 1.9851,
      "step": 436
    },
    {
      "epoch": 0.6198581560283688,
      "grad_norm": 0.9060384631156921,
      "learning_rate": 0.00016261501210653752,
      "loss": 1.8452,
      "step": 437
    },
    {
      "epoch": 0.6212765957446809,
      "grad_norm": 0.9493729472160339,
      "learning_rate": 0.0001625181598062954,
      "loss": 1.6709,
      "step": 438
    },
    {
      "epoch": 0.6226950354609929,
      "grad_norm": 0.9582311511039734,
      "learning_rate": 0.00016242130750605328,
      "loss": 1.8994,
      "step": 439
    },
    {
      "epoch": 0.624113475177305,
      "grad_norm": 1.0585215091705322,
      "learning_rate": 0.00016232445520581116,
      "loss": 1.8092,
      "step": 440
    },
    {
      "epoch": 0.625531914893617,
      "grad_norm": 1.0869824886322021,
      "learning_rate": 0.00016222760290556903,
      "loss": 1.8234,
      "step": 441
    },
    {
      "epoch": 0.6269503546099291,
      "grad_norm": 0.9643338918685913,
      "learning_rate": 0.00016213075060532688,
      "loss": 1.7018,
      "step": 442
    },
    {
      "epoch": 0.6283687943262412,
      "grad_norm": 0.9336432218551636,
      "learning_rate": 0.00016203389830508476,
      "loss": 1.686,
      "step": 443
    },
    {
      "epoch": 0.6297872340425532,
      "grad_norm": 1.0179376602172852,
      "learning_rate": 0.00016193704600484264,
      "loss": 1.9509,
      "step": 444
    },
    {
      "epoch": 0.6312056737588653,
      "grad_norm": 0.9448987245559692,
      "learning_rate": 0.0001618401937046005,
      "loss": 1.8221,
      "step": 445
    },
    {
      "epoch": 0.6326241134751773,
      "grad_norm": 1.063069462776184,
      "learning_rate": 0.00016174334140435837,
      "loss": 2.2294,
      "step": 446
    },
    {
      "epoch": 0.6340425531914894,
      "grad_norm": 0.9483571648597717,
      "learning_rate": 0.00016164648910411625,
      "loss": 1.9025,
      "step": 447
    },
    {
      "epoch": 0.6354609929078014,
      "grad_norm": 0.9159568548202515,
      "learning_rate": 0.0001615496368038741,
      "loss": 1.8817,
      "step": 448
    },
    {
      "epoch": 0.6368794326241135,
      "grad_norm": 0.9895548820495605,
      "learning_rate": 0.00016145278450363198,
      "loss": 1.7253,
      "step": 449
    },
    {
      "epoch": 0.6382978723404256,
      "grad_norm": 0.957243025302887,
      "learning_rate": 0.00016135593220338985,
      "loss": 1.8266,
      "step": 450
    },
    {
      "epoch": 0.6397163120567376,
      "grad_norm": 0.9528331756591797,
      "learning_rate": 0.0001612590799031477,
      "loss": 1.8683,
      "step": 451
    },
    {
      "epoch": 0.6411347517730497,
      "grad_norm": 0.9599409103393555,
      "learning_rate": 0.00016116222760290558,
      "loss": 1.6851,
      "step": 452
    },
    {
      "epoch": 0.6425531914893617,
      "grad_norm": 0.9517663717269897,
      "learning_rate": 0.00016106537530266344,
      "loss": 1.8819,
      "step": 453
    },
    {
      "epoch": 0.6439716312056738,
      "grad_norm": 0.9752060174942017,
      "learning_rate": 0.0001609685230024213,
      "loss": 1.6375,
      "step": 454
    },
    {
      "epoch": 0.6453900709219859,
      "grad_norm": 0.9666942358016968,
      "learning_rate": 0.0001608716707021792,
      "loss": 2.0449,
      "step": 455
    },
    {
      "epoch": 0.6468085106382979,
      "grad_norm": 0.9405897259712219,
      "learning_rate": 0.00016077481840193704,
      "loss": 1.6178,
      "step": 456
    },
    {
      "epoch": 0.64822695035461,
      "grad_norm": 0.9592404961585999,
      "learning_rate": 0.00016067796610169492,
      "loss": 1.8184,
      "step": 457
    },
    {
      "epoch": 0.649645390070922,
      "grad_norm": 0.9113569855690002,
      "learning_rate": 0.0001605811138014528,
      "loss": 1.67,
      "step": 458
    },
    {
      "epoch": 0.6510638297872341,
      "grad_norm": 1.0250040292739868,
      "learning_rate": 0.00016048426150121065,
      "loss": 1.7273,
      "step": 459
    },
    {
      "epoch": 0.6524822695035462,
      "grad_norm": 1.0332226753234863,
      "learning_rate": 0.00016038740920096853,
      "loss": 1.8214,
      "step": 460
    },
    {
      "epoch": 0.6539007092198581,
      "grad_norm": 0.8301089406013489,
      "learning_rate": 0.0001602905569007264,
      "loss": 1.725,
      "step": 461
    },
    {
      "epoch": 0.6553191489361702,
      "grad_norm": 0.9999291300773621,
      "learning_rate": 0.00016019370460048426,
      "loss": 1.8431,
      "step": 462
    },
    {
      "epoch": 0.6567375886524822,
      "grad_norm": 0.9815519452095032,
      "learning_rate": 0.00016009685230024213,
      "loss": 1.7325,
      "step": 463
    },
    {
      "epoch": 0.6581560283687943,
      "grad_norm": 1.0464285612106323,
      "learning_rate": 0.00016,
      "loss": 1.742,
      "step": 464
    },
    {
      "epoch": 0.6595744680851063,
      "grad_norm": 0.9548895955085754,
      "learning_rate": 0.00015990314769975786,
      "loss": 1.6047,
      "step": 465
    },
    {
      "epoch": 0.6609929078014184,
      "grad_norm": 1.084294319152832,
      "learning_rate": 0.00015980629539951574,
      "loss": 1.9552,
      "step": 466
    },
    {
      "epoch": 0.6624113475177305,
      "grad_norm": 1.0143967866897583,
      "learning_rate": 0.00015970944309927362,
      "loss": 1.8402,
      "step": 467
    },
    {
      "epoch": 0.6638297872340425,
      "grad_norm": 0.9333505630493164,
      "learning_rate": 0.00015961259079903147,
      "loss": 1.7068,
      "step": 468
    },
    {
      "epoch": 0.6652482269503546,
      "grad_norm": 0.9322513937950134,
      "learning_rate": 0.00015951573849878935,
      "loss": 1.7812,
      "step": 469
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.994925320148468,
      "learning_rate": 0.0001594188861985472,
      "loss": 1.7499,
      "step": 470
    },
    {
      "epoch": 0.6680851063829787,
      "grad_norm": 0.9555068612098694,
      "learning_rate": 0.00015932203389830508,
      "loss": 1.8724,
      "step": 471
    },
    {
      "epoch": 0.6695035460992907,
      "grad_norm": 0.9105766415596008,
      "learning_rate": 0.00015922518159806296,
      "loss": 1.6967,
      "step": 472
    },
    {
      "epoch": 0.6709219858156028,
      "grad_norm": 0.974273145198822,
      "learning_rate": 0.00015912832929782083,
      "loss": 1.7501,
      "step": 473
    },
    {
      "epoch": 0.6723404255319149,
      "grad_norm": 0.9696586728096008,
      "learning_rate": 0.00015903147699757869,
      "loss": 1.6895,
      "step": 474
    },
    {
      "epoch": 0.6737588652482269,
      "grad_norm": 0.9671431183815002,
      "learning_rate": 0.00015893462469733656,
      "loss": 1.8288,
      "step": 475
    },
    {
      "epoch": 0.675177304964539,
      "grad_norm": 0.9355440735816956,
      "learning_rate": 0.00015883777239709444,
      "loss": 1.9129,
      "step": 476
    },
    {
      "epoch": 0.676595744680851,
      "grad_norm": 0.9853194355964661,
      "learning_rate": 0.00015874092009685232,
      "loss": 1.9956,
      "step": 477
    },
    {
      "epoch": 0.6780141843971631,
      "grad_norm": 0.9325653910636902,
      "learning_rate": 0.0001586440677966102,
      "loss": 1.5878,
      "step": 478
    },
    {
      "epoch": 0.6794326241134752,
      "grad_norm": 0.9293085932731628,
      "learning_rate": 0.00015854721549636805,
      "loss": 1.6628,
      "step": 479
    },
    {
      "epoch": 0.6808510638297872,
      "grad_norm": 0.9750685095787048,
      "learning_rate": 0.00015845036319612593,
      "loss": 1.7832,
      "step": 480
    },
    {
      "epoch": 0.6822695035460993,
      "grad_norm": 0.9032379984855652,
      "learning_rate": 0.0001583535108958838,
      "loss": 1.8512,
      "step": 481
    },
    {
      "epoch": 0.6836879432624113,
      "grad_norm": 0.8798975944519043,
      "learning_rate": 0.00015825665859564166,
      "loss": 1.7611,
      "step": 482
    },
    {
      "epoch": 0.6851063829787234,
      "grad_norm": 0.9648714661598206,
      "learning_rate": 0.00015815980629539953,
      "loss": 1.8787,
      "step": 483
    },
    {
      "epoch": 0.6865248226950355,
      "grad_norm": 1.0370577573776245,
      "learning_rate": 0.0001580629539951574,
      "loss": 1.8954,
      "step": 484
    },
    {
      "epoch": 0.6879432624113475,
      "grad_norm": 0.8941919803619385,
      "learning_rate": 0.00015796610169491526,
      "loss": 1.6513,
      "step": 485
    },
    {
      "epoch": 0.6893617021276596,
      "grad_norm": 1.005692958831787,
      "learning_rate": 0.00015786924939467314,
      "loss": 1.815,
      "step": 486
    },
    {
      "epoch": 0.6907801418439716,
      "grad_norm": 0.8727826476097107,
      "learning_rate": 0.000157772397094431,
      "loss": 1.8058,
      "step": 487
    },
    {
      "epoch": 0.6921985815602837,
      "grad_norm": 0.8884201645851135,
      "learning_rate": 0.00015767554479418887,
      "loss": 1.9065,
      "step": 488
    },
    {
      "epoch": 0.6936170212765957,
      "grad_norm": 0.8967852592468262,
      "learning_rate": 0.00015757869249394675,
      "loss": 1.7835,
      "step": 489
    },
    {
      "epoch": 0.6950354609929078,
      "grad_norm": 0.9886108636856079,
      "learning_rate": 0.0001574818401937046,
      "loss": 1.868,
      "step": 490
    },
    {
      "epoch": 0.6964539007092199,
      "grad_norm": 1.0012317895889282,
      "learning_rate": 0.00015738498789346248,
      "loss": 2.0529,
      "step": 491
    },
    {
      "epoch": 0.6978723404255319,
      "grad_norm": 0.9667900204658508,
      "learning_rate": 0.00015728813559322036,
      "loss": 1.9801,
      "step": 492
    },
    {
      "epoch": 0.699290780141844,
      "grad_norm": 0.9269720315933228,
      "learning_rate": 0.0001571912832929782,
      "loss": 2.0074,
      "step": 493
    },
    {
      "epoch": 0.700709219858156,
      "grad_norm": 0.9457390904426575,
      "learning_rate": 0.00015709443099273609,
      "loss": 1.7077,
      "step": 494
    },
    {
      "epoch": 0.7021276595744681,
      "grad_norm": 0.929481029510498,
      "learning_rate": 0.00015699757869249396,
      "loss": 1.7801,
      "step": 495
    },
    {
      "epoch": 0.7035460992907802,
      "grad_norm": 1.0903468132019043,
      "learning_rate": 0.00015690072639225181,
      "loss": 1.8324,
      "step": 496
    },
    {
      "epoch": 0.7049645390070922,
      "grad_norm": 0.886135995388031,
      "learning_rate": 0.0001568038740920097,
      "loss": 1.6575,
      "step": 497
    },
    {
      "epoch": 0.7063829787234043,
      "grad_norm": 0.9447641372680664,
      "learning_rate": 0.00015670702179176757,
      "loss": 1.8172,
      "step": 498
    },
    {
      "epoch": 0.7078014184397163,
      "grad_norm": 0.9351337552070618,
      "learning_rate": 0.00015661016949152542,
      "loss": 1.866,
      "step": 499
    },
    {
      "epoch": 0.7092198581560284,
      "grad_norm": 0.9051510095596313,
      "learning_rate": 0.0001565133171912833,
      "loss": 1.6929,
      "step": 500
    },
    {
      "epoch": 0.7092198581560284,
      "eval_loss": 1.806251049041748,
      "eval_runtime": 91.737,
      "eval_samples_per_second": 15.37,
      "eval_steps_per_second": 7.685,
      "step": 500
    },
    {
      "epoch": 0.7106382978723405,
      "grad_norm": 0.9373800158500671,
      "learning_rate": 0.00015641646489104115,
      "loss": 1.7382,
      "step": 501
    },
    {
      "epoch": 0.7120567375886525,
      "grad_norm": 1.0061854124069214,
      "learning_rate": 0.00015631961259079903,
      "loss": 1.9958,
      "step": 502
    },
    {
      "epoch": 0.7134751773049646,
      "grad_norm": 0.9167385697364807,
      "learning_rate": 0.0001562227602905569,
      "loss": 1.7058,
      "step": 503
    },
    {
      "epoch": 0.7148936170212766,
      "grad_norm": 0.9798259139060974,
      "learning_rate": 0.00015612590799031476,
      "loss": 2.0648,
      "step": 504
    },
    {
      "epoch": 0.7163120567375887,
      "grad_norm": 0.8581452965736389,
      "learning_rate": 0.00015602905569007264,
      "loss": 1.6057,
      "step": 505
    },
    {
      "epoch": 0.7177304964539007,
      "grad_norm": 0.8883776664733887,
      "learning_rate": 0.00015593220338983051,
      "loss": 1.7381,
      "step": 506
    },
    {
      "epoch": 0.7191489361702128,
      "grad_norm": 0.9018328785896301,
      "learning_rate": 0.00015583535108958837,
      "loss": 2.0363,
      "step": 507
    },
    {
      "epoch": 0.7205673758865249,
      "grad_norm": 0.9636991024017334,
      "learning_rate": 0.00015573849878934624,
      "loss": 1.8643,
      "step": 508
    },
    {
      "epoch": 0.7219858156028369,
      "grad_norm": 0.9437087178230286,
      "learning_rate": 0.00015564164648910412,
      "loss": 1.8585,
      "step": 509
    },
    {
      "epoch": 0.723404255319149,
      "grad_norm": 0.9153167009353638,
      "learning_rate": 0.000155544794188862,
      "loss": 1.6232,
      "step": 510
    },
    {
      "epoch": 0.724822695035461,
      "grad_norm": 0.9500200748443604,
      "learning_rate": 0.00015544794188861985,
      "loss": 1.6023,
      "step": 511
    },
    {
      "epoch": 0.7262411347517731,
      "grad_norm": 1.0050026178359985,
      "learning_rate": 0.00015535108958837773,
      "loss": 1.911,
      "step": 512
    },
    {
      "epoch": 0.7276595744680852,
      "grad_norm": 1.0488208532333374,
      "learning_rate": 0.0001552542372881356,
      "loss": 2.0252,
      "step": 513
    },
    {
      "epoch": 0.7290780141843972,
      "grad_norm": 1.0257785320281982,
      "learning_rate": 0.00015515738498789349,
      "loss": 1.7381,
      "step": 514
    },
    {
      "epoch": 0.7304964539007093,
      "grad_norm": 1.1056292057037354,
      "learning_rate": 0.00015506053268765134,
      "loss": 1.7445,
      "step": 515
    },
    {
      "epoch": 0.7319148936170212,
      "grad_norm": 0.9302806854248047,
      "learning_rate": 0.00015496368038740921,
      "loss": 1.7969,
      "step": 516
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 1.0347586870193481,
      "learning_rate": 0.0001548668280871671,
      "loss": 1.8761,
      "step": 517
    },
    {
      "epoch": 0.7347517730496453,
      "grad_norm": 1.0026211738586426,
      "learning_rate": 0.00015476997578692494,
      "loss": 1.7041,
      "step": 518
    },
    {
      "epoch": 0.7361702127659574,
      "grad_norm": 1.0126824378967285,
      "learning_rate": 0.00015467312348668282,
      "loss": 1.9945,
      "step": 519
    },
    {
      "epoch": 0.7375886524822695,
      "grad_norm": 0.9673506021499634,
      "learning_rate": 0.0001545762711864407,
      "loss": 1.8969,
      "step": 520
    },
    {
      "epoch": 0.7390070921985815,
      "grad_norm": 0.9812307357788086,
      "learning_rate": 0.00015447941888619855,
      "loss": 1.9236,
      "step": 521
    },
    {
      "epoch": 0.7404255319148936,
      "grad_norm": 0.9484560489654541,
      "learning_rate": 0.00015438256658595643,
      "loss": 1.8147,
      "step": 522
    },
    {
      "epoch": 0.7418439716312056,
      "grad_norm": 0.9468274712562561,
      "learning_rate": 0.0001542857142857143,
      "loss": 1.8016,
      "step": 523
    },
    {
      "epoch": 0.7432624113475177,
      "grad_norm": 0.9180728793144226,
      "learning_rate": 0.00015418886198547216,
      "loss": 1.8935,
      "step": 524
    },
    {
      "epoch": 0.7446808510638298,
      "grad_norm": 0.9321813583374023,
      "learning_rate": 0.00015409200968523004,
      "loss": 1.9893,
      "step": 525
    },
    {
      "epoch": 0.7460992907801418,
      "grad_norm": 1.008211374282837,
      "learning_rate": 0.00015399515738498791,
      "loss": 1.9819,
      "step": 526
    },
    {
      "epoch": 0.7475177304964539,
      "grad_norm": 0.9611465334892273,
      "learning_rate": 0.00015389830508474577,
      "loss": 1.8028,
      "step": 527
    },
    {
      "epoch": 0.7489361702127659,
      "grad_norm": 1.06980299949646,
      "learning_rate": 0.00015380145278450364,
      "loss": 1.8331,
      "step": 528
    },
    {
      "epoch": 0.750354609929078,
      "grad_norm": 0.9951017498970032,
      "learning_rate": 0.00015370460048426152,
      "loss": 1.733,
      "step": 529
    },
    {
      "epoch": 0.75177304964539,
      "grad_norm": 0.9143504500389099,
      "learning_rate": 0.00015360774818401937,
      "loss": 1.9987,
      "step": 530
    },
    {
      "epoch": 0.7531914893617021,
      "grad_norm": 1.035732626914978,
      "learning_rate": 0.00015351089588377725,
      "loss": 1.8822,
      "step": 531
    },
    {
      "epoch": 0.7546099290780142,
      "grad_norm": 0.9665756225585938,
      "learning_rate": 0.0001534140435835351,
      "loss": 1.5895,
      "step": 532
    },
    {
      "epoch": 0.7560283687943262,
      "grad_norm": 0.9881098866462708,
      "learning_rate": 0.00015331719128329298,
      "loss": 1.6994,
      "step": 533
    },
    {
      "epoch": 0.7574468085106383,
      "grad_norm": 0.9169169068336487,
      "learning_rate": 0.00015322033898305086,
      "loss": 1.8721,
      "step": 534
    },
    {
      "epoch": 0.7588652482269503,
      "grad_norm": 1.005684494972229,
      "learning_rate": 0.0001531234866828087,
      "loss": 1.7963,
      "step": 535
    },
    {
      "epoch": 0.7602836879432624,
      "grad_norm": 0.9482589364051819,
      "learning_rate": 0.0001530266343825666,
      "loss": 1.8089,
      "step": 536
    },
    {
      "epoch": 0.7617021276595745,
      "grad_norm": 1.0248510837554932,
      "learning_rate": 0.00015292978208232447,
      "loss": 1.6966,
      "step": 537
    },
    {
      "epoch": 0.7631205673758865,
      "grad_norm": 0.9485064148902893,
      "learning_rate": 0.00015283292978208232,
      "loss": 1.5582,
      "step": 538
    },
    {
      "epoch": 0.7645390070921986,
      "grad_norm": 0.9537084102630615,
      "learning_rate": 0.0001527360774818402,
      "loss": 1.6908,
      "step": 539
    },
    {
      "epoch": 0.7659574468085106,
      "grad_norm": 0.9340246915817261,
      "learning_rate": 0.00015263922518159807,
      "loss": 1.8275,
      "step": 540
    },
    {
      "epoch": 0.7673758865248227,
      "grad_norm": 1.0218333005905151,
      "learning_rate": 0.00015254237288135592,
      "loss": 2.0469,
      "step": 541
    },
    {
      "epoch": 0.7687943262411348,
      "grad_norm": 0.93470299243927,
      "learning_rate": 0.0001524455205811138,
      "loss": 1.7582,
      "step": 542
    },
    {
      "epoch": 0.7702127659574468,
      "grad_norm": 0.9361689686775208,
      "learning_rate": 0.00015234866828087168,
      "loss": 1.7492,
      "step": 543
    },
    {
      "epoch": 0.7716312056737589,
      "grad_norm": 1.0975946187973022,
      "learning_rate": 0.00015225181598062953,
      "loss": 1.9672,
      "step": 544
    },
    {
      "epoch": 0.7730496453900709,
      "grad_norm": 1.0108466148376465,
      "learning_rate": 0.0001521549636803874,
      "loss": 1.9047,
      "step": 545
    },
    {
      "epoch": 0.774468085106383,
      "grad_norm": 1.0511740446090698,
      "learning_rate": 0.0001520581113801453,
      "loss": 1.9796,
      "step": 546
    },
    {
      "epoch": 0.775886524822695,
      "grad_norm": 0.9763832688331604,
      "learning_rate": 0.00015196125907990314,
      "loss": 1.7714,
      "step": 547
    },
    {
      "epoch": 0.7773049645390071,
      "grad_norm": 0.938847005367279,
      "learning_rate": 0.00015186440677966102,
      "loss": 1.8183,
      "step": 548
    },
    {
      "epoch": 0.7787234042553192,
      "grad_norm": 0.8810025453567505,
      "learning_rate": 0.0001517675544794189,
      "loss": 1.7477,
      "step": 549
    },
    {
      "epoch": 0.7801418439716312,
      "grad_norm": 0.9073086380958557,
      "learning_rate": 0.00015167070217917677,
      "loss": 1.7315,
      "step": 550
    },
    {
      "epoch": 0.7815602836879433,
      "grad_norm": 0.9286612868309021,
      "learning_rate": 0.00015157384987893465,
      "loss": 1.7977,
      "step": 551
    },
    {
      "epoch": 0.7829787234042553,
      "grad_norm": 0.9796596765518188,
      "learning_rate": 0.0001514769975786925,
      "loss": 2.112,
      "step": 552
    },
    {
      "epoch": 0.7843971631205674,
      "grad_norm": 0.9236878156661987,
      "learning_rate": 0.00015138014527845038,
      "loss": 1.9134,
      "step": 553
    },
    {
      "epoch": 0.7858156028368795,
      "grad_norm": 0.9988068342208862,
      "learning_rate": 0.00015128329297820826,
      "loss": 1.8176,
      "step": 554
    },
    {
      "epoch": 0.7872340425531915,
      "grad_norm": 0.9496425986289978,
      "learning_rate": 0.0001511864406779661,
      "loss": 1.7943,
      "step": 555
    },
    {
      "epoch": 0.7886524822695036,
      "grad_norm": 0.9448116421699524,
      "learning_rate": 0.000151089588377724,
      "loss": 1.8102,
      "step": 556
    },
    {
      "epoch": 0.7900709219858156,
      "grad_norm": 0.9122583866119385,
      "learning_rate": 0.00015099273607748186,
      "loss": 1.7606,
      "step": 557
    },
    {
      "epoch": 0.7914893617021277,
      "grad_norm": 0.8941739201545715,
      "learning_rate": 0.00015089588377723972,
      "loss": 1.873,
      "step": 558
    },
    {
      "epoch": 0.7929078014184398,
      "grad_norm": 0.9107263684272766,
      "learning_rate": 0.0001507990314769976,
      "loss": 1.6111,
      "step": 559
    },
    {
      "epoch": 0.7943262411347518,
      "grad_norm": 0.9208337068557739,
      "learning_rate": 0.00015070217917675547,
      "loss": 1.7329,
      "step": 560
    },
    {
      "epoch": 0.7957446808510639,
      "grad_norm": 0.9259641170501709,
      "learning_rate": 0.00015060532687651332,
      "loss": 1.8765,
      "step": 561
    },
    {
      "epoch": 0.7971631205673759,
      "grad_norm": 1.0317492485046387,
      "learning_rate": 0.0001505084745762712,
      "loss": 1.8607,
      "step": 562
    },
    {
      "epoch": 0.798581560283688,
      "grad_norm": 0.9023297429084778,
      "learning_rate": 0.00015041162227602908,
      "loss": 1.489,
      "step": 563
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.8855882883071899,
      "learning_rate": 0.00015031476997578693,
      "loss": 1.773,
      "step": 564
    },
    {
      "epoch": 0.8014184397163121,
      "grad_norm": 0.9644176959991455,
      "learning_rate": 0.0001502179176755448,
      "loss": 1.8337,
      "step": 565
    },
    {
      "epoch": 0.8028368794326242,
      "grad_norm": 0.9933615922927856,
      "learning_rate": 0.00015012106537530266,
      "loss": 1.8721,
      "step": 566
    },
    {
      "epoch": 0.8042553191489362,
      "grad_norm": 0.9730909466743469,
      "learning_rate": 0.00015002421307506054,
      "loss": 1.5937,
      "step": 567
    },
    {
      "epoch": 0.8056737588652483,
      "grad_norm": 0.9592195749282837,
      "learning_rate": 0.00014992736077481842,
      "loss": 1.9941,
      "step": 568
    },
    {
      "epoch": 0.8070921985815603,
      "grad_norm": 0.9991093277931213,
      "learning_rate": 0.00014983050847457627,
      "loss": 1.8707,
      "step": 569
    },
    {
      "epoch": 0.8085106382978723,
      "grad_norm": 0.9750028252601624,
      "learning_rate": 0.00014973365617433414,
      "loss": 1.8025,
      "step": 570
    },
    {
      "epoch": 0.8099290780141843,
      "grad_norm": 1.0412169694900513,
      "learning_rate": 0.00014963680387409202,
      "loss": 1.7719,
      "step": 571
    },
    {
      "epoch": 0.8113475177304964,
      "grad_norm": 1.0398106575012207,
      "learning_rate": 0.00014953995157384987,
      "loss": 1.661,
      "step": 572
    },
    {
      "epoch": 0.8127659574468085,
      "grad_norm": 0.8933267593383789,
      "learning_rate": 0.00014944309927360775,
      "loss": 1.8424,
      "step": 573
    },
    {
      "epoch": 0.8141843971631205,
      "grad_norm": 0.9653945565223694,
      "learning_rate": 0.00014934624697336563,
      "loss": 1.713,
      "step": 574
    },
    {
      "epoch": 0.8156028368794326,
      "grad_norm": 1.0151563882827759,
      "learning_rate": 0.00014924939467312348,
      "loss": 1.6218,
      "step": 575
    },
    {
      "epoch": 0.8170212765957446,
      "grad_norm": 0.9580346941947937,
      "learning_rate": 0.00014915254237288136,
      "loss": 1.8652,
      "step": 576
    },
    {
      "epoch": 0.8184397163120567,
      "grad_norm": 1.0817276239395142,
      "learning_rate": 0.00014905569007263924,
      "loss": 1.9472,
      "step": 577
    },
    {
      "epoch": 0.8198581560283688,
      "grad_norm": 0.9456907510757446,
      "learning_rate": 0.0001489588377723971,
      "loss": 1.7959,
      "step": 578
    },
    {
      "epoch": 0.8212765957446808,
      "grad_norm": 0.9884796738624573,
      "learning_rate": 0.00014886198547215497,
      "loss": 1.8121,
      "step": 579
    },
    {
      "epoch": 0.8226950354609929,
      "grad_norm": 1.0423191785812378,
      "learning_rate": 0.00014876513317191282,
      "loss": 1.7294,
      "step": 580
    },
    {
      "epoch": 0.8241134751773049,
      "grad_norm": 0.8762698769569397,
      "learning_rate": 0.0001486682808716707,
      "loss": 1.8848,
      "step": 581
    },
    {
      "epoch": 0.825531914893617,
      "grad_norm": 0.882957398891449,
      "learning_rate": 0.00014857142857142857,
      "loss": 1.6479,
      "step": 582
    },
    {
      "epoch": 0.826950354609929,
      "grad_norm": 0.9469701647758484,
      "learning_rate": 0.00014847457627118645,
      "loss": 1.5607,
      "step": 583
    },
    {
      "epoch": 0.8283687943262411,
      "grad_norm": 0.9412171840667725,
      "learning_rate": 0.0001483777239709443,
      "loss": 1.8733,
      "step": 584
    },
    {
      "epoch": 0.8297872340425532,
      "grad_norm": 1.0088787078857422,
      "learning_rate": 0.00014828087167070218,
      "loss": 1.8975,
      "step": 585
    },
    {
      "epoch": 0.8312056737588652,
      "grad_norm": 1.1250557899475098,
      "learning_rate": 0.00014818401937046006,
      "loss": 1.8192,
      "step": 586
    },
    {
      "epoch": 0.8326241134751773,
      "grad_norm": 0.9558066129684448,
      "learning_rate": 0.00014808716707021794,
      "loss": 1.7909,
      "step": 587
    },
    {
      "epoch": 0.8340425531914893,
      "grad_norm": 0.979161262512207,
      "learning_rate": 0.0001479903147699758,
      "loss": 1.8969,
      "step": 588
    },
    {
      "epoch": 0.8354609929078014,
      "grad_norm": 1.0074257850646973,
      "learning_rate": 0.00014789346246973367,
      "loss": 2.0733,
      "step": 589
    },
    {
      "epoch": 0.8368794326241135,
      "grad_norm": 0.9141056537628174,
      "learning_rate": 0.00014779661016949154,
      "loss": 1.7769,
      "step": 590
    },
    {
      "epoch": 0.8382978723404255,
      "grad_norm": 0.8834106922149658,
      "learning_rate": 0.00014769975786924942,
      "loss": 1.6314,
      "step": 591
    },
    {
      "epoch": 0.8397163120567376,
      "grad_norm": 0.9422737956047058,
      "learning_rate": 0.00014760290556900727,
      "loss": 1.8027,
      "step": 592
    },
    {
      "epoch": 0.8411347517730496,
      "grad_norm": 0.8941066265106201,
      "learning_rate": 0.00014750605326876515,
      "loss": 1.8671,
      "step": 593
    },
    {
      "epoch": 0.8425531914893617,
      "grad_norm": 0.9444699883460999,
      "learning_rate": 0.00014740920096852303,
      "loss": 2.0242,
      "step": 594
    },
    {
      "epoch": 0.8439716312056738,
      "grad_norm": 0.8961655497550964,
      "learning_rate": 0.00014731234866828088,
      "loss": 1.9358,
      "step": 595
    },
    {
      "epoch": 0.8453900709219858,
      "grad_norm": 0.8932142853736877,
      "learning_rate": 0.00014721549636803876,
      "loss": 2.0009,
      "step": 596
    },
    {
      "epoch": 0.8468085106382979,
      "grad_norm": 0.9080725908279419,
      "learning_rate": 0.0001471186440677966,
      "loss": 1.702,
      "step": 597
    },
    {
      "epoch": 0.8482269503546099,
      "grad_norm": 0.9869876503944397,
      "learning_rate": 0.0001470217917675545,
      "loss": 1.8054,
      "step": 598
    },
    {
      "epoch": 0.849645390070922,
      "grad_norm": 0.9595502614974976,
      "learning_rate": 0.00014692493946731237,
      "loss": 1.854,
      "step": 599
    },
    {
      "epoch": 0.851063829787234,
      "grad_norm": 0.975337028503418,
      "learning_rate": 0.00014682808716707022,
      "loss": 2.0526,
      "step": 600
    },
    {
      "epoch": 0.8524822695035461,
      "grad_norm": 0.9461711645126343,
      "learning_rate": 0.0001467312348668281,
      "loss": 1.7712,
      "step": 601
    },
    {
      "epoch": 0.8539007092198582,
      "grad_norm": 1.002305507659912,
      "learning_rate": 0.00014663438256658597,
      "loss": 1.9774,
      "step": 602
    },
    {
      "epoch": 0.8553191489361702,
      "grad_norm": 0.9175607562065125,
      "learning_rate": 0.00014653753026634382,
      "loss": 1.7826,
      "step": 603
    },
    {
      "epoch": 0.8567375886524823,
      "grad_norm": 0.9470594525337219,
      "learning_rate": 0.0001464406779661017,
      "loss": 1.805,
      "step": 604
    },
    {
      "epoch": 0.8581560283687943,
      "grad_norm": 0.9285657405853271,
      "learning_rate": 0.00014634382566585958,
      "loss": 1.8489,
      "step": 605
    },
    {
      "epoch": 0.8595744680851064,
      "grad_norm": 1.013704776763916,
      "learning_rate": 0.00014624697336561743,
      "loss": 1.7888,
      "step": 606
    },
    {
      "epoch": 0.8609929078014185,
      "grad_norm": 1.061592936515808,
      "learning_rate": 0.0001461501210653753,
      "loss": 1.8539,
      "step": 607
    },
    {
      "epoch": 0.8624113475177305,
      "grad_norm": 0.9778721928596497,
      "learning_rate": 0.0001460532687651332,
      "loss": 2.0674,
      "step": 608
    },
    {
      "epoch": 0.8638297872340426,
      "grad_norm": 1.0700124502182007,
      "learning_rate": 0.00014595641646489104,
      "loss": 2.0469,
      "step": 609
    },
    {
      "epoch": 0.8652482269503546,
      "grad_norm": 0.932140052318573,
      "learning_rate": 0.00014585956416464892,
      "loss": 1.832,
      "step": 610
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 0.8720954060554504,
      "learning_rate": 0.00014576271186440677,
      "loss": 1.5422,
      "step": 611
    },
    {
      "epoch": 0.8680851063829788,
      "grad_norm": 0.9799209833145142,
      "learning_rate": 0.00014566585956416465,
      "loss": 1.8203,
      "step": 612
    },
    {
      "epoch": 0.8695035460992908,
      "grad_norm": 0.9990918636322021,
      "learning_rate": 0.00014556900726392252,
      "loss": 1.7772,
      "step": 613
    },
    {
      "epoch": 0.8709219858156029,
      "grad_norm": 0.9366308450698853,
      "learning_rate": 0.00014547215496368038,
      "loss": 1.9041,
      "step": 614
    },
    {
      "epoch": 0.8723404255319149,
      "grad_norm": 0.9561300277709961,
      "learning_rate": 0.00014537530266343825,
      "loss": 2.0094,
      "step": 615
    },
    {
      "epoch": 0.873758865248227,
      "grad_norm": 0.9853776097297668,
      "learning_rate": 0.00014527845036319613,
      "loss": 1.853,
      "step": 616
    },
    {
      "epoch": 0.875177304964539,
      "grad_norm": 0.9627854228019714,
      "learning_rate": 0.00014518159806295398,
      "loss": 1.7166,
      "step": 617
    },
    {
      "epoch": 0.8765957446808511,
      "grad_norm": 0.9175072908401489,
      "learning_rate": 0.00014508474576271186,
      "loss": 1.9452,
      "step": 618
    },
    {
      "epoch": 0.8780141843971632,
      "grad_norm": 0.9186393022537231,
      "learning_rate": 0.00014498789346246974,
      "loss": 1.9466,
      "step": 619
    },
    {
      "epoch": 0.8794326241134752,
      "grad_norm": 0.975797712802887,
      "learning_rate": 0.0001448910411622276,
      "loss": 1.8509,
      "step": 620
    },
    {
      "epoch": 0.8808510638297873,
      "grad_norm": 0.9033548831939697,
      "learning_rate": 0.00014479418886198547,
      "loss": 1.6784,
      "step": 621
    },
    {
      "epoch": 0.8822695035460993,
      "grad_norm": 2.0145516395568848,
      "learning_rate": 0.00014469733656174335,
      "loss": 1.9831,
      "step": 622
    },
    {
      "epoch": 0.8836879432624114,
      "grad_norm": 0.9748150110244751,
      "learning_rate": 0.00014460048426150122,
      "loss": 1.8112,
      "step": 623
    },
    {
      "epoch": 0.8851063829787233,
      "grad_norm": 0.928269624710083,
      "learning_rate": 0.0001445036319612591,
      "loss": 1.8436,
      "step": 624
    },
    {
      "epoch": 0.8865248226950354,
      "grad_norm": 1.0261791944503784,
      "learning_rate": 0.00014440677966101695,
      "loss": 1.6131,
      "step": 625
    },
    {
      "epoch": 0.8879432624113475,
      "grad_norm": 0.9150677919387817,
      "learning_rate": 0.00014430992736077483,
      "loss": 1.711,
      "step": 626
    },
    {
      "epoch": 0.8893617021276595,
      "grad_norm": 0.9695219993591309,
      "learning_rate": 0.0001442130750605327,
      "loss": 1.6629,
      "step": 627
    },
    {
      "epoch": 0.8907801418439716,
      "grad_norm": 1.0402165651321411,
      "learning_rate": 0.00014411622276029056,
      "loss": 1.8659,
      "step": 628
    },
    {
      "epoch": 0.8921985815602836,
      "grad_norm": 0.9994091391563416,
      "learning_rate": 0.00014401937046004844,
      "loss": 1.8511,
      "step": 629
    },
    {
      "epoch": 0.8936170212765957,
      "grad_norm": 0.9416429996490479,
      "learning_rate": 0.00014392251815980632,
      "loss": 1.541,
      "step": 630
    },
    {
      "epoch": 0.8950354609929078,
      "grad_norm": 0.9779669642448425,
      "learning_rate": 0.00014382566585956417,
      "loss": 1.4849,
      "step": 631
    },
    {
      "epoch": 0.8964539007092198,
      "grad_norm": 0.9192597270011902,
      "learning_rate": 0.00014372881355932205,
      "loss": 1.7296,
      "step": 632
    },
    {
      "epoch": 0.8978723404255319,
      "grad_norm": 0.9987861514091492,
      "learning_rate": 0.00014363196125907992,
      "loss": 1.7969,
      "step": 633
    },
    {
      "epoch": 0.8992907801418439,
      "grad_norm": 0.9664397239685059,
      "learning_rate": 0.00014353510895883778,
      "loss": 1.5772,
      "step": 634
    },
    {
      "epoch": 0.900709219858156,
      "grad_norm": 0.9543797969818115,
      "learning_rate": 0.00014343825665859565,
      "loss": 1.9208,
      "step": 635
    },
    {
      "epoch": 0.902127659574468,
      "grad_norm": 1.030147910118103,
      "learning_rate": 0.00014334140435835353,
      "loss": 1.8075,
      "step": 636
    },
    {
      "epoch": 0.9035460992907801,
      "grad_norm": 1.0873552560806274,
      "learning_rate": 0.00014324455205811138,
      "loss": 1.8447,
      "step": 637
    },
    {
      "epoch": 0.9049645390070922,
      "grad_norm": 0.9692791104316711,
      "learning_rate": 0.00014314769975786926,
      "loss": 1.7809,
      "step": 638
    },
    {
      "epoch": 0.9063829787234042,
      "grad_norm": 0.918589174747467,
      "learning_rate": 0.00014305084745762714,
      "loss": 1.8654,
      "step": 639
    },
    {
      "epoch": 0.9078014184397163,
      "grad_norm": 0.9659599661827087,
      "learning_rate": 0.000142953995157385,
      "loss": 1.8804,
      "step": 640
    },
    {
      "epoch": 0.9092198581560283,
      "grad_norm": 0.9876123666763306,
      "learning_rate": 0.00014285714285714287,
      "loss": 1.9563,
      "step": 641
    },
    {
      "epoch": 0.9106382978723404,
      "grad_norm": 1.0985149145126343,
      "learning_rate": 0.00014276029055690075,
      "loss": 1.9155,
      "step": 642
    },
    {
      "epoch": 0.9120567375886525,
      "grad_norm": 0.9651275277137756,
      "learning_rate": 0.0001426634382566586,
      "loss": 1.7785,
      "step": 643
    },
    {
      "epoch": 0.9134751773049645,
      "grad_norm": 0.9180082082748413,
      "learning_rate": 0.00014256658595641648,
      "loss": 1.8405,
      "step": 644
    },
    {
      "epoch": 0.9148936170212766,
      "grad_norm": 1.0275384187698364,
      "learning_rate": 0.00014246973365617433,
      "loss": 2.048,
      "step": 645
    },
    {
      "epoch": 0.9163120567375886,
      "grad_norm": 1.0340267419815063,
      "learning_rate": 0.0001423728813559322,
      "loss": 1.6341,
      "step": 646
    },
    {
      "epoch": 0.9177304964539007,
      "grad_norm": 0.858508288860321,
      "learning_rate": 0.00014227602905569008,
      "loss": 1.5195,
      "step": 647
    },
    {
      "epoch": 0.9191489361702128,
      "grad_norm": 0.938224732875824,
      "learning_rate": 0.00014217917675544793,
      "loss": 1.6986,
      "step": 648
    },
    {
      "epoch": 0.9205673758865248,
      "grad_norm": 0.9428529739379883,
      "learning_rate": 0.0001420823244552058,
      "loss": 1.7616,
      "step": 649
    },
    {
      "epoch": 0.9219858156028369,
      "grad_norm": 0.984078049659729,
      "learning_rate": 0.0001419854721549637,
      "loss": 1.9531,
      "step": 650
    },
    {
      "epoch": 0.9234042553191489,
      "grad_norm": 0.9620499610900879,
      "learning_rate": 0.00014188861985472154,
      "loss": 1.8108,
      "step": 651
    },
    {
      "epoch": 0.924822695035461,
      "grad_norm": 1.060816764831543,
      "learning_rate": 0.00014179176755447942,
      "loss": 1.8468,
      "step": 652
    },
    {
      "epoch": 0.926241134751773,
      "grad_norm": 0.9061930179595947,
      "learning_rate": 0.0001416949152542373,
      "loss": 1.708,
      "step": 653
    },
    {
      "epoch": 0.9276595744680851,
      "grad_norm": 0.9653571248054504,
      "learning_rate": 0.00014159806295399515,
      "loss": 1.8584,
      "step": 654
    },
    {
      "epoch": 0.9290780141843972,
      "grad_norm": 0.974460244178772,
      "learning_rate": 0.00014150121065375303,
      "loss": 2.0488,
      "step": 655
    },
    {
      "epoch": 0.9304964539007092,
      "grad_norm": 0.9710502028465271,
      "learning_rate": 0.0001414043583535109,
      "loss": 1.9374,
      "step": 656
    },
    {
      "epoch": 0.9319148936170213,
      "grad_norm": 0.9244868159294128,
      "learning_rate": 0.00014130750605326876,
      "loss": 1.7593,
      "step": 657
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 1.024077296257019,
      "learning_rate": 0.00014121065375302663,
      "loss": 1.8946,
      "step": 658
    },
    {
      "epoch": 0.9347517730496454,
      "grad_norm": 0.9225230813026428,
      "learning_rate": 0.0001411138014527845,
      "loss": 1.8369,
      "step": 659
    },
    {
      "epoch": 0.9361702127659575,
      "grad_norm": 0.9084619879722595,
      "learning_rate": 0.0001410169491525424,
      "loss": 1.7481,
      "step": 660
    },
    {
      "epoch": 0.9375886524822695,
      "grad_norm": 0.8650357127189636,
      "learning_rate": 0.00014092009685230027,
      "loss": 1.7555,
      "step": 661
    },
    {
      "epoch": 0.9390070921985816,
      "grad_norm": 0.9914340376853943,
      "learning_rate": 0.00014082324455205812,
      "loss": 1.8704,
      "step": 662
    },
    {
      "epoch": 0.9404255319148936,
      "grad_norm": 0.9047065377235413,
      "learning_rate": 0.000140726392251816,
      "loss": 1.7808,
      "step": 663
    },
    {
      "epoch": 0.9418439716312057,
      "grad_norm": 0.9416099786758423,
      "learning_rate": 0.00014062953995157387,
      "loss": 1.9871,
      "step": 664
    },
    {
      "epoch": 0.9432624113475178,
      "grad_norm": 1.0570451021194458,
      "learning_rate": 0.00014053268765133173,
      "loss": 1.9121,
      "step": 665
    },
    {
      "epoch": 0.9446808510638298,
      "grad_norm": 0.9373465776443481,
      "learning_rate": 0.0001404358353510896,
      "loss": 1.9358,
      "step": 666
    },
    {
      "epoch": 0.9460992907801419,
      "grad_norm": 0.897957444190979,
      "learning_rate": 0.00014033898305084748,
      "loss": 1.6269,
      "step": 667
    },
    {
      "epoch": 0.9475177304964539,
      "grad_norm": 0.9292532205581665,
      "learning_rate": 0.00014024213075060533,
      "loss": 1.8592,
      "step": 668
    },
    {
      "epoch": 0.948936170212766,
      "grad_norm": 1.0342576503753662,
      "learning_rate": 0.0001401452784503632,
      "loss": 1.5566,
      "step": 669
    },
    {
      "epoch": 0.950354609929078,
      "grad_norm": 0.9260356426239014,
      "learning_rate": 0.0001400484261501211,
      "loss": 1.8768,
      "step": 670
    },
    {
      "epoch": 0.9517730496453901,
      "grad_norm": 0.9853015542030334,
      "learning_rate": 0.00013995157384987894,
      "loss": 1.7743,
      "step": 671
    },
    {
      "epoch": 0.9531914893617022,
      "grad_norm": 1.0748721361160278,
      "learning_rate": 0.00013985472154963682,
      "loss": 2.079,
      "step": 672
    },
    {
      "epoch": 0.9546099290780142,
      "grad_norm": 0.9573856592178345,
      "learning_rate": 0.0001397578692493947,
      "loss": 1.675,
      "step": 673
    },
    {
      "epoch": 0.9560283687943263,
      "grad_norm": 0.8905030488967896,
      "learning_rate": 0.00013966101694915255,
      "loss": 1.7418,
      "step": 674
    },
    {
      "epoch": 0.9574468085106383,
      "grad_norm": 0.9220945835113525,
      "learning_rate": 0.00013956416464891043,
      "loss": 1.9879,
      "step": 675
    },
    {
      "epoch": 0.9588652482269504,
      "grad_norm": 1.0226384401321411,
      "learning_rate": 0.00013946731234866828,
      "loss": 2.0695,
      "step": 676
    },
    {
      "epoch": 0.9602836879432625,
      "grad_norm": 0.9441185593605042,
      "learning_rate": 0.00013937046004842615,
      "loss": 1.9575,
      "step": 677
    },
    {
      "epoch": 0.9617021276595744,
      "grad_norm": 0.8684213757514954,
      "learning_rate": 0.00013927360774818403,
      "loss": 1.8215,
      "step": 678
    },
    {
      "epoch": 0.9631205673758865,
      "grad_norm": 0.9086737036705017,
      "learning_rate": 0.00013917675544794188,
      "loss": 1.7301,
      "step": 679
    },
    {
      "epoch": 0.9645390070921985,
      "grad_norm": 0.895923912525177,
      "learning_rate": 0.00013907990314769976,
      "loss": 1.6081,
      "step": 680
    },
    {
      "epoch": 0.9659574468085106,
      "grad_norm": 0.9483720064163208,
      "learning_rate": 0.00013898305084745764,
      "loss": 1.5901,
      "step": 681
    },
    {
      "epoch": 0.9673758865248226,
      "grad_norm": 1.0106722116470337,
      "learning_rate": 0.0001388861985472155,
      "loss": 1.7852,
      "step": 682
    },
    {
      "epoch": 0.9687943262411347,
      "grad_norm": 1.0110578536987305,
      "learning_rate": 0.00013878934624697337,
      "loss": 1.8604,
      "step": 683
    },
    {
      "epoch": 0.9702127659574468,
      "grad_norm": 0.9774700403213501,
      "learning_rate": 0.00013869249394673125,
      "loss": 1.8243,
      "step": 684
    },
    {
      "epoch": 0.9716312056737588,
      "grad_norm": 0.9234907031059265,
      "learning_rate": 0.0001385956416464891,
      "loss": 1.5916,
      "step": 685
    },
    {
      "epoch": 0.9730496453900709,
      "grad_norm": 1.059147596359253,
      "learning_rate": 0.00013849878934624698,
      "loss": 1.843,
      "step": 686
    },
    {
      "epoch": 0.9744680851063829,
      "grad_norm": 0.9501283764839172,
      "learning_rate": 0.00013840193704600485,
      "loss": 1.5896,
      "step": 687
    },
    {
      "epoch": 0.975886524822695,
      "grad_norm": 0.925462007522583,
      "learning_rate": 0.0001383050847457627,
      "loss": 1.8908,
      "step": 688
    },
    {
      "epoch": 0.9773049645390071,
      "grad_norm": 1.0491056442260742,
      "learning_rate": 0.00013820823244552058,
      "loss": 1.7496,
      "step": 689
    },
    {
      "epoch": 0.9787234042553191,
      "grad_norm": 0.9669529795646667,
      "learning_rate": 0.00013811138014527843,
      "loss": 1.8373,
      "step": 690
    },
    {
      "epoch": 0.9801418439716312,
      "grad_norm": 1.0195949077606201,
      "learning_rate": 0.0001380145278450363,
      "loss": 1.9521,
      "step": 691
    },
    {
      "epoch": 0.9815602836879432,
      "grad_norm": 1.0332221984863281,
      "learning_rate": 0.0001379176755447942,
      "loss": 1.8843,
      "step": 692
    },
    {
      "epoch": 0.9829787234042553,
      "grad_norm": 0.9786433577537537,
      "learning_rate": 0.00013782082324455204,
      "loss": 1.6895,
      "step": 693
    },
    {
      "epoch": 0.9843971631205674,
      "grad_norm": 0.9934961199760437,
      "learning_rate": 0.00013772397094430992,
      "loss": 1.9579,
      "step": 694
    },
    {
      "epoch": 0.9858156028368794,
      "grad_norm": 0.856977105140686,
      "learning_rate": 0.0001376271186440678,
      "loss": 1.6905,
      "step": 695
    },
    {
      "epoch": 0.9872340425531915,
      "grad_norm": 0.9279069304466248,
      "learning_rate": 0.00013753026634382568,
      "loss": 1.6489,
      "step": 696
    },
    {
      "epoch": 0.9886524822695035,
      "grad_norm": 0.9529761075973511,
      "learning_rate": 0.00013743341404358355,
      "loss": 1.6892,
      "step": 697
    },
    {
      "epoch": 0.9900709219858156,
      "grad_norm": 1.0541527271270752,
      "learning_rate": 0.0001373365617433414,
      "loss": 1.9965,
      "step": 698
    },
    {
      "epoch": 0.9914893617021276,
      "grad_norm": 0.9821218848228455,
      "learning_rate": 0.00013723970944309928,
      "loss": 1.9619,
      "step": 699
    },
    {
      "epoch": 0.9929078014184397,
      "grad_norm": 0.957706868648529,
      "learning_rate": 0.00013714285714285716,
      "loss": 1.8005,
      "step": 700
    },
    {
      "epoch": 0.9943262411347518,
      "grad_norm": 1.0196388959884644,
      "learning_rate": 0.00013704600484261504,
      "loss": 1.7739,
      "step": 701
    },
    {
      "epoch": 0.9957446808510638,
      "grad_norm": 0.9991929531097412,
      "learning_rate": 0.0001369491525423729,
      "loss": 1.7795,
      "step": 702
    },
    {
      "epoch": 0.9971631205673759,
      "grad_norm": 0.9795322418212891,
      "learning_rate": 0.00013685230024213077,
      "loss": 1.9412,
      "step": 703
    },
    {
      "epoch": 0.9985815602836879,
      "grad_norm": 0.9208971261978149,
      "learning_rate": 0.00013675544794188865,
      "loss": 1.851,
      "step": 704
    },
    {
      "epoch": 1.0,
      "grad_norm": 1.0984488725662231,
      "learning_rate": 0.0001366585956416465,
      "loss": 1.5486,
      "step": 705
    },
    {
      "epoch": 1.001418439716312,
      "grad_norm": 0.8669431805610657,
      "learning_rate": 0.00013656174334140438,
      "loss": 1.5551,
      "step": 706
    },
    {
      "epoch": 1.0028368794326241,
      "grad_norm": 0.9462072849273682,
      "learning_rate": 0.00013646489104116223,
      "loss": 1.4165,
      "step": 707
    },
    {
      "epoch": 1.004255319148936,
      "grad_norm": 0.9423401355743408,
      "learning_rate": 0.0001363680387409201,
      "loss": 1.7539,
      "step": 708
    },
    {
      "epoch": 1.0056737588652482,
      "grad_norm": 0.8739091753959656,
      "learning_rate": 0.00013627118644067798,
      "loss": 1.4029,
      "step": 709
    },
    {
      "epoch": 1.0070921985815602,
      "grad_norm": 0.9132165312767029,
      "learning_rate": 0.00013617433414043583,
      "loss": 1.4626,
      "step": 710
    },
    {
      "epoch": 1.0085106382978724,
      "grad_norm": 0.9093713164329529,
      "learning_rate": 0.0001360774818401937,
      "loss": 1.8219,
      "step": 711
    },
    {
      "epoch": 1.0099290780141843,
      "grad_norm": 0.9057936668395996,
      "learning_rate": 0.0001359806295399516,
      "loss": 1.6615,
      "step": 712
    },
    {
      "epoch": 1.0113475177304965,
      "grad_norm": 0.9246616959571838,
      "learning_rate": 0.00013588377723970944,
      "loss": 1.5746,
      "step": 713
    },
    {
      "epoch": 1.0127659574468084,
      "grad_norm": 1.0594182014465332,
      "learning_rate": 0.00013578692493946732,
      "loss": 1.6838,
      "step": 714
    },
    {
      "epoch": 1.0141843971631206,
      "grad_norm": 1.0186001062393188,
      "learning_rate": 0.0001356900726392252,
      "loss": 1.8892,
      "step": 715
    },
    {
      "epoch": 1.0156028368794325,
      "grad_norm": 0.9838067889213562,
      "learning_rate": 0.00013559322033898305,
      "loss": 1.5421,
      "step": 716
    },
    {
      "epoch": 1.0170212765957447,
      "grad_norm": 0.9654032588005066,
      "learning_rate": 0.00013549636803874093,
      "loss": 1.6379,
      "step": 717
    },
    {
      "epoch": 1.0184397163120567,
      "grad_norm": 0.9566038250923157,
      "learning_rate": 0.0001353995157384988,
      "loss": 1.5206,
      "step": 718
    },
    {
      "epoch": 1.0198581560283688,
      "grad_norm": 1.0163044929504395,
      "learning_rate": 0.00013530266343825666,
      "loss": 1.7308,
      "step": 719
    },
    {
      "epoch": 1.0212765957446808,
      "grad_norm": 1.0072609186172485,
      "learning_rate": 0.00013520581113801453,
      "loss": 1.5444,
      "step": 720
    },
    {
      "epoch": 1.022695035460993,
      "grad_norm": 0.954924464225769,
      "learning_rate": 0.00013510895883777239,
      "loss": 1.557,
      "step": 721
    },
    {
      "epoch": 1.0241134751773049,
      "grad_norm": 1.01553213596344,
      "learning_rate": 0.00013501210653753026,
      "loss": 1.5773,
      "step": 722
    },
    {
      "epoch": 1.025531914893617,
      "grad_norm": 1.0861798524856567,
      "learning_rate": 0.00013491525423728814,
      "loss": 1.8833,
      "step": 723
    },
    {
      "epoch": 1.026950354609929,
      "grad_norm": 1.0201469659805298,
      "learning_rate": 0.000134818401937046,
      "loss": 1.6846,
      "step": 724
    },
    {
      "epoch": 1.0283687943262412,
      "grad_norm": 1.0009260177612305,
      "learning_rate": 0.00013472154963680387,
      "loss": 1.6597,
      "step": 725
    },
    {
      "epoch": 1.0297872340425531,
      "grad_norm": 0.9549226760864258,
      "learning_rate": 0.00013462469733656175,
      "loss": 1.5257,
      "step": 726
    },
    {
      "epoch": 1.0312056737588653,
      "grad_norm": 1.0315558910369873,
      "learning_rate": 0.0001345278450363196,
      "loss": 1.7265,
      "step": 727
    },
    {
      "epoch": 1.0326241134751772,
      "grad_norm": 1.0820245742797852,
      "learning_rate": 0.00013443099273607748,
      "loss": 1.3508,
      "step": 728
    },
    {
      "epoch": 1.0340425531914894,
      "grad_norm": 0.9885953664779663,
      "learning_rate": 0.00013433414043583536,
      "loss": 1.8148,
      "step": 729
    },
    {
      "epoch": 1.0354609929078014,
      "grad_norm": 0.99950110912323,
      "learning_rate": 0.0001342372881355932,
      "loss": 1.5989,
      "step": 730
    },
    {
      "epoch": 1.0368794326241135,
      "grad_norm": 1.0264092683792114,
      "learning_rate": 0.00013414043583535109,
      "loss": 1.5121,
      "step": 731
    },
    {
      "epoch": 1.0382978723404255,
      "grad_norm": 0.9720777273178101,
      "learning_rate": 0.00013404358353510896,
      "loss": 1.5167,
      "step": 732
    },
    {
      "epoch": 1.0397163120567376,
      "grad_norm": 0.9726420044898987,
      "learning_rate": 0.00013394673123486684,
      "loss": 1.5827,
      "step": 733
    },
    {
      "epoch": 1.0411347517730496,
      "grad_norm": 1.089019775390625,
      "learning_rate": 0.00013384987893462472,
      "loss": 1.6869,
      "step": 734
    },
    {
      "epoch": 1.0425531914893618,
      "grad_norm": 1.0289783477783203,
      "learning_rate": 0.00013375302663438257,
      "loss": 1.5885,
      "step": 735
    },
    {
      "epoch": 1.0439716312056737,
      "grad_norm": 1.0663580894470215,
      "learning_rate": 0.00013365617433414045,
      "loss": 1.641,
      "step": 736
    },
    {
      "epoch": 1.0453900709219859,
      "grad_norm": 1.0268943309783936,
      "learning_rate": 0.00013355932203389833,
      "loss": 1.5581,
      "step": 737
    },
    {
      "epoch": 1.0468085106382978,
      "grad_norm": 1.0885965824127197,
      "learning_rate": 0.00013346246973365618,
      "loss": 1.6535,
      "step": 738
    },
    {
      "epoch": 1.04822695035461,
      "grad_norm": 1.0613908767700195,
      "learning_rate": 0.00013336561743341406,
      "loss": 1.8136,
      "step": 739
    },
    {
      "epoch": 1.049645390070922,
      "grad_norm": 1.0064640045166016,
      "learning_rate": 0.00013326876513317193,
      "loss": 1.6304,
      "step": 740
    },
    {
      "epoch": 1.0510638297872341,
      "grad_norm": 1.0421061515808105,
      "learning_rate": 0.00013317191283292979,
      "loss": 1.7192,
      "step": 741
    },
    {
      "epoch": 1.052482269503546,
      "grad_norm": 1.0375536680221558,
      "learning_rate": 0.00013307506053268766,
      "loss": 1.6004,
      "step": 742
    },
    {
      "epoch": 1.0539007092198582,
      "grad_norm": 1.040773630142212,
      "learning_rate": 0.00013297820823244554,
      "loss": 1.5675,
      "step": 743
    },
    {
      "epoch": 1.0553191489361702,
      "grad_norm": 1.1050875186920166,
      "learning_rate": 0.0001328813559322034,
      "loss": 1.5882,
      "step": 744
    },
    {
      "epoch": 1.0567375886524824,
      "grad_norm": 1.0099092721939087,
      "learning_rate": 0.00013278450363196127,
      "loss": 1.6221,
      "step": 745
    },
    {
      "epoch": 1.0581560283687943,
      "grad_norm": 0.9932960867881775,
      "learning_rate": 0.00013268765133171915,
      "loss": 1.6655,
      "step": 746
    },
    {
      "epoch": 1.0595744680851065,
      "grad_norm": 1.049930214881897,
      "learning_rate": 0.000132590799031477,
      "loss": 1.4372,
      "step": 747
    },
    {
      "epoch": 1.0609929078014184,
      "grad_norm": 1.054844856262207,
      "learning_rate": 0.00013249394673123488,
      "loss": 1.594,
      "step": 748
    },
    {
      "epoch": 1.0624113475177306,
      "grad_norm": 1.0007810592651367,
      "learning_rate": 0.00013239709443099276,
      "loss": 1.5366,
      "step": 749
    },
    {
      "epoch": 1.0638297872340425,
      "grad_norm": 1.2219514846801758,
      "learning_rate": 0.0001323002421307506,
      "loss": 1.6559,
      "step": 750
    },
    {
      "epoch": 1.0652482269503547,
      "grad_norm": 1.020301103591919,
      "learning_rate": 0.00013220338983050849,
      "loss": 1.7955,
      "step": 751
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 1.0003628730773926,
      "learning_rate": 0.00013210653753026636,
      "loss": 1.697,
      "step": 752
    },
    {
      "epoch": 1.0680851063829788,
      "grad_norm": 1.0451273918151855,
      "learning_rate": 0.00013200968523002421,
      "loss": 1.6042,
      "step": 753
    },
    {
      "epoch": 1.0695035460992908,
      "grad_norm": 1.191812515258789,
      "learning_rate": 0.0001319128329297821,
      "loss": 1.7299,
      "step": 754
    },
    {
      "epoch": 1.070921985815603,
      "grad_norm": 1.0480337142944336,
      "learning_rate": 0.00013181598062953994,
      "loss": 1.7916,
      "step": 755
    },
    {
      "epoch": 1.0723404255319149,
      "grad_norm": 1.090874195098877,
      "learning_rate": 0.00013171912832929782,
      "loss": 1.7273,
      "step": 756
    },
    {
      "epoch": 1.073758865248227,
      "grad_norm": 1.0175896883010864,
      "learning_rate": 0.0001316222760290557,
      "loss": 1.648,
      "step": 757
    },
    {
      "epoch": 1.075177304964539,
      "grad_norm": 1.338780164718628,
      "learning_rate": 0.00013152542372881355,
      "loss": 1.6185,
      "step": 758
    },
    {
      "epoch": 1.076595744680851,
      "grad_norm": 1.1820087432861328,
      "learning_rate": 0.00013142857142857143,
      "loss": 1.5906,
      "step": 759
    },
    {
      "epoch": 1.0780141843971631,
      "grad_norm": 1.0093029737472534,
      "learning_rate": 0.0001313317191283293,
      "loss": 1.6203,
      "step": 760
    },
    {
      "epoch": 1.0794326241134753,
      "grad_norm": 0.9925059676170349,
      "learning_rate": 0.00013123486682808716,
      "loss": 1.6228,
      "step": 761
    },
    {
      "epoch": 1.0808510638297872,
      "grad_norm": 1.1004815101623535,
      "learning_rate": 0.00013113801452784504,
      "loss": 1.7204,
      "step": 762
    },
    {
      "epoch": 1.0822695035460992,
      "grad_norm": 1.026804804801941,
      "learning_rate": 0.00013104116222760291,
      "loss": 1.5004,
      "step": 763
    },
    {
      "epoch": 1.0836879432624114,
      "grad_norm": 1.07239830493927,
      "learning_rate": 0.00013094430992736077,
      "loss": 1.8015,
      "step": 764
    },
    {
      "epoch": 1.0851063829787233,
      "grad_norm": 1.0456151962280273,
      "learning_rate": 0.00013084745762711864,
      "loss": 1.6246,
      "step": 765
    },
    {
      "epoch": 1.0865248226950355,
      "grad_norm": 1.0390042066574097,
      "learning_rate": 0.00013075060532687652,
      "loss": 1.5896,
      "step": 766
    },
    {
      "epoch": 1.0879432624113474,
      "grad_norm": 1.2059139013290405,
      "learning_rate": 0.00013065375302663437,
      "loss": 1.6985,
      "step": 767
    },
    {
      "epoch": 1.0893617021276596,
      "grad_norm": 1.0211321115493774,
      "learning_rate": 0.00013055690072639225,
      "loss": 1.6248,
      "step": 768
    },
    {
      "epoch": 1.0907801418439715,
      "grad_norm": 1.0914887189865112,
      "learning_rate": 0.00013046004842615013,
      "loss": 1.5503,
      "step": 769
    },
    {
      "epoch": 1.0921985815602837,
      "grad_norm": 0.9979832172393799,
      "learning_rate": 0.000130363196125908,
      "loss": 1.6063,
      "step": 770
    },
    {
      "epoch": 1.0936170212765957,
      "grad_norm": 1.163329839706421,
      "learning_rate": 0.00013026634382566586,
      "loss": 1.7596,
      "step": 771
    },
    {
      "epoch": 1.0950354609929078,
      "grad_norm": 0.9895117282867432,
      "learning_rate": 0.00013016949152542374,
      "loss": 1.6197,
      "step": 772
    },
    {
      "epoch": 1.0964539007092198,
      "grad_norm": 1.10126793384552,
      "learning_rate": 0.00013007263922518161,
      "loss": 1.7804,
      "step": 773
    },
    {
      "epoch": 1.097872340425532,
      "grad_norm": 1.0471243858337402,
      "learning_rate": 0.0001299757869249395,
      "loss": 1.3824,
      "step": 774
    },
    {
      "epoch": 1.099290780141844,
      "grad_norm": 1.0833115577697754,
      "learning_rate": 0.00012987893462469734,
      "loss": 1.6021,
      "step": 775
    },
    {
      "epoch": 1.100709219858156,
      "grad_norm": 1.0613868236541748,
      "learning_rate": 0.00012978208232445522,
      "loss": 1.6289,
      "step": 776
    },
    {
      "epoch": 1.102127659574468,
      "grad_norm": 1.1191869974136353,
      "learning_rate": 0.0001296852300242131,
      "loss": 1.5971,
      "step": 777
    },
    {
      "epoch": 1.1035460992907802,
      "grad_norm": 1.0563015937805176,
      "learning_rate": 0.00012958837772397095,
      "loss": 1.6224,
      "step": 778
    },
    {
      "epoch": 1.1049645390070921,
      "grad_norm": 1.1402943134307861,
      "learning_rate": 0.00012949152542372883,
      "loss": 1.6066,
      "step": 779
    },
    {
      "epoch": 1.1063829787234043,
      "grad_norm": 1.0745183229446411,
      "learning_rate": 0.0001293946731234867,
      "loss": 1.4848,
      "step": 780
    },
    {
      "epoch": 1.1078014184397162,
      "grad_norm": 1.1375377178192139,
      "learning_rate": 0.00012929782082324456,
      "loss": 1.6518,
      "step": 781
    },
    {
      "epoch": 1.1092198581560284,
      "grad_norm": 1.0856523513793945,
      "learning_rate": 0.00012920096852300244,
      "loss": 1.6505,
      "step": 782
    },
    {
      "epoch": 1.1106382978723404,
      "grad_norm": 1.0998051166534424,
      "learning_rate": 0.00012910411622276031,
      "loss": 1.6443,
      "step": 783
    },
    {
      "epoch": 1.1120567375886525,
      "grad_norm": 1.0637699365615845,
      "learning_rate": 0.00012900726392251816,
      "loss": 1.6017,
      "step": 784
    },
    {
      "epoch": 1.1134751773049645,
      "grad_norm": 1.1011300086975098,
      "learning_rate": 0.00012891041162227604,
      "loss": 1.6817,
      "step": 785
    },
    {
      "epoch": 1.1148936170212767,
      "grad_norm": 1.0492033958435059,
      "learning_rate": 0.0001288135593220339,
      "loss": 1.6339,
      "step": 786
    },
    {
      "epoch": 1.1163120567375886,
      "grad_norm": 1.0780454874038696,
      "learning_rate": 0.00012871670702179177,
      "loss": 1.6973,
      "step": 787
    },
    {
      "epoch": 1.1177304964539008,
      "grad_norm": 1.0570387840270996,
      "learning_rate": 0.00012861985472154965,
      "loss": 1.6281,
      "step": 788
    },
    {
      "epoch": 1.1191489361702127,
      "grad_norm": 1.108502745628357,
      "learning_rate": 0.0001285230024213075,
      "loss": 1.7035,
      "step": 789
    },
    {
      "epoch": 1.1205673758865249,
      "grad_norm": 1.0517934560775757,
      "learning_rate": 0.00012842615012106538,
      "loss": 1.5869,
      "step": 790
    },
    {
      "epoch": 1.1219858156028368,
      "grad_norm": 1.0766640901565552,
      "learning_rate": 0.00012832929782082326,
      "loss": 1.4739,
      "step": 791
    },
    {
      "epoch": 1.123404255319149,
      "grad_norm": 1.0901997089385986,
      "learning_rate": 0.0001282324455205811,
      "loss": 1.6162,
      "step": 792
    },
    {
      "epoch": 1.124822695035461,
      "grad_norm": 1.116286277770996,
      "learning_rate": 0.000128135593220339,
      "loss": 1.6359,
      "step": 793
    },
    {
      "epoch": 1.1262411347517731,
      "grad_norm": 1.0820567607879639,
      "learning_rate": 0.00012803874092009686,
      "loss": 1.5726,
      "step": 794
    },
    {
      "epoch": 1.127659574468085,
      "grad_norm": 1.0170390605926514,
      "learning_rate": 0.00012794188861985472,
      "loss": 1.537,
      "step": 795
    },
    {
      "epoch": 1.1290780141843972,
      "grad_norm": 1.0629899501800537,
      "learning_rate": 0.0001278450363196126,
      "loss": 1.5736,
      "step": 796
    },
    {
      "epoch": 1.1304964539007092,
      "grad_norm": 1.117285966873169,
      "learning_rate": 0.00012774818401937047,
      "loss": 1.7205,
      "step": 797
    },
    {
      "epoch": 1.1319148936170214,
      "grad_norm": 1.0091588497161865,
      "learning_rate": 0.00012765133171912832,
      "loss": 1.567,
      "step": 798
    },
    {
      "epoch": 1.1333333333333333,
      "grad_norm": 1.0815651416778564,
      "learning_rate": 0.0001275544794188862,
      "loss": 1.6088,
      "step": 799
    },
    {
      "epoch": 1.1347517730496455,
      "grad_norm": 1.0486299991607666,
      "learning_rate": 0.00012745762711864405,
      "loss": 1.5704,
      "step": 800
    },
    {
      "epoch": 1.1361702127659574,
      "grad_norm": 1.2071648836135864,
      "learning_rate": 0.00012736077481840193,
      "loss": 1.6601,
      "step": 801
    },
    {
      "epoch": 1.1375886524822696,
      "grad_norm": 1.1117751598358154,
      "learning_rate": 0.0001272639225181598,
      "loss": 1.7526,
      "step": 802
    },
    {
      "epoch": 1.1390070921985815,
      "grad_norm": 1.1300843954086304,
      "learning_rate": 0.00012716707021791766,
      "loss": 1.9232,
      "step": 803
    },
    {
      "epoch": 1.1404255319148937,
      "grad_norm": 1.1168575286865234,
      "learning_rate": 0.00012707021791767554,
      "loss": 1.4264,
      "step": 804
    },
    {
      "epoch": 1.1418439716312057,
      "grad_norm": 1.0603300333023071,
      "learning_rate": 0.00012697336561743342,
      "loss": 1.5906,
      "step": 805
    },
    {
      "epoch": 1.1432624113475178,
      "grad_norm": 1.142404556274414,
      "learning_rate": 0.0001268765133171913,
      "loss": 1.4126,
      "step": 806
    },
    {
      "epoch": 1.1446808510638298,
      "grad_norm": 1.0878480672836304,
      "learning_rate": 0.00012677966101694917,
      "loss": 1.6963,
      "step": 807
    },
    {
      "epoch": 1.1460992907801417,
      "grad_norm": 1.1728767156600952,
      "learning_rate": 0.00012668280871670702,
      "loss": 1.8231,
      "step": 808
    },
    {
      "epoch": 1.147517730496454,
      "grad_norm": 1.1441761255264282,
      "learning_rate": 0.0001265859564164649,
      "loss": 1.692,
      "step": 809
    },
    {
      "epoch": 1.148936170212766,
      "grad_norm": 1.0993083715438843,
      "learning_rate": 0.00012648910411622278,
      "loss": 1.6154,
      "step": 810
    },
    {
      "epoch": 1.150354609929078,
      "grad_norm": 1.0974794626235962,
      "learning_rate": 0.00012639225181598066,
      "loss": 1.6216,
      "step": 811
    },
    {
      "epoch": 1.15177304964539,
      "grad_norm": 1.1409790515899658,
      "learning_rate": 0.0001262953995157385,
      "loss": 1.6398,
      "step": 812
    },
    {
      "epoch": 1.1531914893617021,
      "grad_norm": 1.0230299234390259,
      "learning_rate": 0.00012619854721549639,
      "loss": 1.5056,
      "step": 813
    },
    {
      "epoch": 1.1546099290780143,
      "grad_norm": 1.2628209590911865,
      "learning_rate": 0.00012610169491525426,
      "loss": 1.8601,
      "step": 814
    },
    {
      "epoch": 1.1560283687943262,
      "grad_norm": 1.1834397315979004,
      "learning_rate": 0.00012600484261501212,
      "loss": 1.8378,
      "step": 815
    },
    {
      "epoch": 1.1574468085106382,
      "grad_norm": 1.1164121627807617,
      "learning_rate": 0.00012590799031477,
      "loss": 1.7314,
      "step": 816
    },
    {
      "epoch": 1.1588652482269504,
      "grad_norm": 1.1394809484481812,
      "learning_rate": 0.00012581113801452784,
      "loss": 1.4417,
      "step": 817
    },
    {
      "epoch": 1.1602836879432625,
      "grad_norm": 1.227352499961853,
      "learning_rate": 0.00012571428571428572,
      "loss": 1.9046,
      "step": 818
    },
    {
      "epoch": 1.1617021276595745,
      "grad_norm": 1.044887900352478,
      "learning_rate": 0.0001256174334140436,
      "loss": 1.5116,
      "step": 819
    },
    {
      "epoch": 1.1631205673758864,
      "grad_norm": 1.1246867179870605,
      "learning_rate": 0.00012552058111380145,
      "loss": 1.6009,
      "step": 820
    },
    {
      "epoch": 1.1645390070921986,
      "grad_norm": 1.1201499700546265,
      "learning_rate": 0.00012542372881355933,
      "loss": 1.5495,
      "step": 821
    },
    {
      "epoch": 1.1659574468085105,
      "grad_norm": 1.162782073020935,
      "learning_rate": 0.0001253268765133172,
      "loss": 1.816,
      "step": 822
    },
    {
      "epoch": 1.1673758865248227,
      "grad_norm": 1.167772889137268,
      "learning_rate": 0.00012523002421307506,
      "loss": 1.7147,
      "step": 823
    },
    {
      "epoch": 1.1687943262411347,
      "grad_norm": 1.1469957828521729,
      "learning_rate": 0.00012513317191283294,
      "loss": 1.5245,
      "step": 824
    },
    {
      "epoch": 1.1702127659574468,
      "grad_norm": 1.1207575798034668,
      "learning_rate": 0.00012503631961259082,
      "loss": 1.662,
      "step": 825
    },
    {
      "epoch": 1.1716312056737588,
      "grad_norm": 1.089674949645996,
      "learning_rate": 0.00012493946731234867,
      "loss": 1.6581,
      "step": 826
    },
    {
      "epoch": 1.173049645390071,
      "grad_norm": 1.1373921632766724,
      "learning_rate": 0.00012484261501210654,
      "loss": 1.6793,
      "step": 827
    },
    {
      "epoch": 1.174468085106383,
      "grad_norm": 1.0678393840789795,
      "learning_rate": 0.00012474576271186442,
      "loss": 1.4781,
      "step": 828
    },
    {
      "epoch": 1.175886524822695,
      "grad_norm": 1.1053776741027832,
      "learning_rate": 0.00012464891041162227,
      "loss": 1.6218,
      "step": 829
    },
    {
      "epoch": 1.177304964539007,
      "grad_norm": 1.1970406770706177,
      "learning_rate": 0.00012455205811138015,
      "loss": 1.8182,
      "step": 830
    },
    {
      "epoch": 1.1787234042553192,
      "grad_norm": 1.108733892440796,
      "learning_rate": 0.00012445520581113803,
      "loss": 1.6454,
      "step": 831
    },
    {
      "epoch": 1.1801418439716311,
      "grad_norm": 1.1200484037399292,
      "learning_rate": 0.00012435835351089588,
      "loss": 1.7415,
      "step": 832
    },
    {
      "epoch": 1.1815602836879433,
      "grad_norm": 1.1099495887756348,
      "learning_rate": 0.00012426150121065376,
      "loss": 1.5547,
      "step": 833
    },
    {
      "epoch": 1.1829787234042553,
      "grad_norm": 1.2226362228393555,
      "learning_rate": 0.0001241646489104116,
      "loss": 1.6778,
      "step": 834
    },
    {
      "epoch": 1.1843971631205674,
      "grad_norm": 1.1625699996948242,
      "learning_rate": 0.0001240677966101695,
      "loss": 1.7189,
      "step": 835
    },
    {
      "epoch": 1.1858156028368794,
      "grad_norm": 1.1172806024551392,
      "learning_rate": 0.00012397094430992737,
      "loss": 1.5602,
      "step": 836
    },
    {
      "epoch": 1.1872340425531915,
      "grad_norm": 1.2000049352645874,
      "learning_rate": 0.00012387409200968522,
      "loss": 1.6432,
      "step": 837
    },
    {
      "epoch": 1.1886524822695035,
      "grad_norm": 1.047294020652771,
      "learning_rate": 0.0001237772397094431,
      "loss": 1.5153,
      "step": 838
    },
    {
      "epoch": 1.1900709219858157,
      "grad_norm": 1.1712678670883179,
      "learning_rate": 0.00012368038740920097,
      "loss": 1.7065,
      "step": 839
    },
    {
      "epoch": 1.1914893617021276,
      "grad_norm": 1.1383200883865356,
      "learning_rate": 0.00012358353510895882,
      "loss": 1.8006,
      "step": 840
    },
    {
      "epoch": 1.1929078014184398,
      "grad_norm": 1.162231683731079,
      "learning_rate": 0.0001234866828087167,
      "loss": 1.7702,
      "step": 841
    },
    {
      "epoch": 1.1943262411347517,
      "grad_norm": 1.1455134153366089,
      "learning_rate": 0.00012338983050847458,
      "loss": 1.5294,
      "step": 842
    },
    {
      "epoch": 1.195744680851064,
      "grad_norm": 1.2015560865402222,
      "learning_rate": 0.00012329297820823246,
      "loss": 1.6117,
      "step": 843
    },
    {
      "epoch": 1.1971631205673758,
      "grad_norm": 1.1155803203582764,
      "learning_rate": 0.0001231961259079903,
      "loss": 1.6747,
      "step": 844
    },
    {
      "epoch": 1.198581560283688,
      "grad_norm": 1.2041141986846924,
      "learning_rate": 0.0001230992736077482,
      "loss": 1.8117,
      "step": 845
    },
    {
      "epoch": 1.2,
      "grad_norm": 1.1454250812530518,
      "learning_rate": 0.00012300242130750607,
      "loss": 1.5233,
      "step": 846
    },
    {
      "epoch": 1.2014184397163121,
      "grad_norm": 1.2244088649749756,
      "learning_rate": 0.00012290556900726394,
      "loss": 1.6077,
      "step": 847
    },
    {
      "epoch": 1.202836879432624,
      "grad_norm": 1.071986436843872,
      "learning_rate": 0.0001228087167070218,
      "loss": 1.4989,
      "step": 848
    },
    {
      "epoch": 1.2042553191489362,
      "grad_norm": 1.2518149614334106,
      "learning_rate": 0.00012271186440677967,
      "loss": 1.567,
      "step": 849
    },
    {
      "epoch": 1.2056737588652482,
      "grad_norm": 1.145087480545044,
      "learning_rate": 0.00012261501210653755,
      "loss": 1.6361,
      "step": 850
    },
    {
      "epoch": 1.2070921985815604,
      "grad_norm": 1.1603142023086548,
      "learning_rate": 0.0001225181598062954,
      "loss": 1.566,
      "step": 851
    },
    {
      "epoch": 1.2085106382978723,
      "grad_norm": 1.1530247926712036,
      "learning_rate": 0.00012242130750605328,
      "loss": 1.6674,
      "step": 852
    },
    {
      "epoch": 1.2099290780141845,
      "grad_norm": 1.0845212936401367,
      "learning_rate": 0.00012232445520581116,
      "loss": 1.7064,
      "step": 853
    },
    {
      "epoch": 1.2113475177304964,
      "grad_norm": 1.1687835454940796,
      "learning_rate": 0.000122227602905569,
      "loss": 1.6285,
      "step": 854
    },
    {
      "epoch": 1.2127659574468086,
      "grad_norm": 1.1240934133529663,
      "learning_rate": 0.0001221307506053269,
      "loss": 1.6136,
      "step": 855
    },
    {
      "epoch": 1.2141843971631205,
      "grad_norm": 1.0865541696548462,
      "learning_rate": 0.00012203389830508477,
      "loss": 1.4974,
      "step": 856
    },
    {
      "epoch": 1.2156028368794327,
      "grad_norm": 1.0967127084732056,
      "learning_rate": 0.00012193704600484262,
      "loss": 1.5726,
      "step": 857
    },
    {
      "epoch": 1.2170212765957447,
      "grad_norm": 1.1025390625,
      "learning_rate": 0.0001218401937046005,
      "loss": 1.5633,
      "step": 858
    },
    {
      "epoch": 1.2184397163120568,
      "grad_norm": 1.2273186445236206,
      "learning_rate": 0.00012174334140435837,
      "loss": 1.8067,
      "step": 859
    },
    {
      "epoch": 1.2198581560283688,
      "grad_norm": 1.1347017288208008,
      "learning_rate": 0.00012164648910411622,
      "loss": 1.7052,
      "step": 860
    },
    {
      "epoch": 1.2212765957446807,
      "grad_norm": 1.1563421487808228,
      "learning_rate": 0.0001215496368038741,
      "loss": 1.4675,
      "step": 861
    },
    {
      "epoch": 1.222695035460993,
      "grad_norm": 1.111360788345337,
      "learning_rate": 0.00012145278450363198,
      "loss": 1.5548,
      "step": 862
    },
    {
      "epoch": 1.224113475177305,
      "grad_norm": 1.181559443473816,
      "learning_rate": 0.00012135593220338983,
      "loss": 1.5412,
      "step": 863
    },
    {
      "epoch": 1.225531914893617,
      "grad_norm": 1.2207701206207275,
      "learning_rate": 0.00012125907990314771,
      "loss": 1.5782,
      "step": 864
    },
    {
      "epoch": 1.226950354609929,
      "grad_norm": 1.1988660097122192,
      "learning_rate": 0.00012116222760290556,
      "loss": 1.8256,
      "step": 865
    },
    {
      "epoch": 1.2283687943262411,
      "grad_norm": 1.193527340888977,
      "learning_rate": 0.00012106537530266344,
      "loss": 1.7381,
      "step": 866
    },
    {
      "epoch": 1.2297872340425533,
      "grad_norm": 1.1142988204956055,
      "learning_rate": 0.00012096852300242132,
      "loss": 1.6554,
      "step": 867
    },
    {
      "epoch": 1.2312056737588652,
      "grad_norm": 1.1546838283538818,
      "learning_rate": 0.00012087167070217917,
      "loss": 1.7093,
      "step": 868
    },
    {
      "epoch": 1.2326241134751772,
      "grad_norm": 1.0819395780563354,
      "learning_rate": 0.00012077481840193705,
      "loss": 1.508,
      "step": 869
    },
    {
      "epoch": 1.2340425531914894,
      "grad_norm": 1.1872516870498657,
      "learning_rate": 0.00012067796610169492,
      "loss": 1.5969,
      "step": 870
    },
    {
      "epoch": 1.2354609929078015,
      "grad_norm": 1.1412676572799683,
      "learning_rate": 0.00012058111380145279,
      "loss": 1.5301,
      "step": 871
    },
    {
      "epoch": 1.2368794326241135,
      "grad_norm": 1.07659912109375,
      "learning_rate": 0.00012048426150121067,
      "loss": 1.6849,
      "step": 872
    },
    {
      "epoch": 1.2382978723404254,
      "grad_norm": 1.0153110027313232,
      "learning_rate": 0.00012038740920096853,
      "loss": 1.4639,
      "step": 873
    },
    {
      "epoch": 1.2397163120567376,
      "grad_norm": 1.050123929977417,
      "learning_rate": 0.0001202905569007264,
      "loss": 1.494,
      "step": 874
    },
    {
      "epoch": 1.2411347517730495,
      "grad_norm": 1.132237434387207,
      "learning_rate": 0.00012019370460048427,
      "loss": 1.3303,
      "step": 875
    },
    {
      "epoch": 1.2425531914893617,
      "grad_norm": 1.2590436935424805,
      "learning_rate": 0.00012009685230024215,
      "loss": 1.7616,
      "step": 876
    },
    {
      "epoch": 1.2439716312056737,
      "grad_norm": 1.1915855407714844,
      "learning_rate": 0.00012,
      "loss": 1.5911,
      "step": 877
    },
    {
      "epoch": 1.2453900709219858,
      "grad_norm": 1.1991710662841797,
      "learning_rate": 0.00011990314769975788,
      "loss": 1.6931,
      "step": 878
    },
    {
      "epoch": 1.2468085106382978,
      "grad_norm": 1.1409502029418945,
      "learning_rate": 0.00011980629539951573,
      "loss": 1.3562,
      "step": 879
    },
    {
      "epoch": 1.24822695035461,
      "grad_norm": 1.1501595973968506,
      "learning_rate": 0.00011970944309927361,
      "loss": 1.4261,
      "step": 880
    },
    {
      "epoch": 1.249645390070922,
      "grad_norm": 1.1364527940750122,
      "learning_rate": 0.00011961259079903149,
      "loss": 1.5007,
      "step": 881
    },
    {
      "epoch": 1.251063829787234,
      "grad_norm": 1.2418267726898193,
      "learning_rate": 0.00011951573849878934,
      "loss": 1.5265,
      "step": 882
    },
    {
      "epoch": 1.252482269503546,
      "grad_norm": 1.1675043106079102,
      "learning_rate": 0.00011941888619854722,
      "loss": 1.6537,
      "step": 883
    },
    {
      "epoch": 1.2539007092198582,
      "grad_norm": 1.2078756093978882,
      "learning_rate": 0.0001193220338983051,
      "loss": 1.4663,
      "step": 884
    },
    {
      "epoch": 1.2553191489361701,
      "grad_norm": 1.1733118295669556,
      "learning_rate": 0.00011922518159806295,
      "loss": 1.7232,
      "step": 885
    },
    {
      "epoch": 1.2567375886524823,
      "grad_norm": 1.1305021047592163,
      "learning_rate": 0.00011912832929782082,
      "loss": 1.7467,
      "step": 886
    },
    {
      "epoch": 1.2581560283687943,
      "grad_norm": 1.1178840398788452,
      "learning_rate": 0.0001190314769975787,
      "loss": 1.4699,
      "step": 887
    },
    {
      "epoch": 1.2595744680851064,
      "grad_norm": 1.1186991930007935,
      "learning_rate": 0.00011893462469733657,
      "loss": 1.6801,
      "step": 888
    },
    {
      "epoch": 1.2609929078014184,
      "grad_norm": 1.2062313556671143,
      "learning_rate": 0.00011883777239709443,
      "loss": 1.8058,
      "step": 889
    },
    {
      "epoch": 1.2624113475177305,
      "grad_norm": 1.242905855178833,
      "learning_rate": 0.00011874092009685231,
      "loss": 1.5943,
      "step": 890
    },
    {
      "epoch": 1.2638297872340425,
      "grad_norm": 1.21308434009552,
      "learning_rate": 0.00011864406779661017,
      "loss": 1.4569,
      "step": 891
    },
    {
      "epoch": 1.2652482269503547,
      "grad_norm": 1.0986233949661255,
      "learning_rate": 0.00011854721549636805,
      "loss": 1.6523,
      "step": 892
    },
    {
      "epoch": 1.2666666666666666,
      "grad_norm": 1.2085695266723633,
      "learning_rate": 0.00011845036319612592,
      "loss": 1.642,
      "step": 893
    },
    {
      "epoch": 1.2680851063829788,
      "grad_norm": 1.212265968322754,
      "learning_rate": 0.00011835351089588378,
      "loss": 1.6045,
      "step": 894
    },
    {
      "epoch": 1.2695035460992907,
      "grad_norm": 1.0752696990966797,
      "learning_rate": 0.00011825665859564166,
      "loss": 1.5124,
      "step": 895
    },
    {
      "epoch": 1.270921985815603,
      "grad_norm": 1.0601632595062256,
      "learning_rate": 0.00011815980629539951,
      "loss": 1.6149,
      "step": 896
    },
    {
      "epoch": 1.2723404255319148,
      "grad_norm": 1.165026068687439,
      "learning_rate": 0.00011806295399515739,
      "loss": 1.6937,
      "step": 897
    },
    {
      "epoch": 1.273758865248227,
      "grad_norm": 1.1537418365478516,
      "learning_rate": 0.00011796610169491527,
      "loss": 1.8067,
      "step": 898
    },
    {
      "epoch": 1.275177304964539,
      "grad_norm": 1.143288016319275,
      "learning_rate": 0.00011786924939467312,
      "loss": 1.6607,
      "step": 899
    },
    {
      "epoch": 1.2765957446808511,
      "grad_norm": 1.1708904504776,
      "learning_rate": 0.000117772397094431,
      "loss": 1.4811,
      "step": 900
    },
    {
      "epoch": 1.278014184397163,
      "grad_norm": 1.1173733472824097,
      "learning_rate": 0.00011767554479418887,
      "loss": 1.4682,
      "step": 901
    },
    {
      "epoch": 1.2794326241134752,
      "grad_norm": 1.0386710166931152,
      "learning_rate": 0.00011757869249394673,
      "loss": 1.5555,
      "step": 902
    },
    {
      "epoch": 1.2808510638297872,
      "grad_norm": 1.1206083297729492,
      "learning_rate": 0.0001174818401937046,
      "loss": 1.6718,
      "step": 903
    },
    {
      "epoch": 1.2822695035460994,
      "grad_norm": 1.0450093746185303,
      "learning_rate": 0.00011738498789346248,
      "loss": 1.6509,
      "step": 904
    },
    {
      "epoch": 1.2836879432624113,
      "grad_norm": 1.117169976234436,
      "learning_rate": 0.00011728813559322033,
      "loss": 1.5285,
      "step": 905
    },
    {
      "epoch": 1.2851063829787235,
      "grad_norm": 1.251837968826294,
      "learning_rate": 0.00011719128329297821,
      "loss": 1.7585,
      "step": 906
    },
    {
      "epoch": 1.2865248226950354,
      "grad_norm": 1.0525087118148804,
      "learning_rate": 0.00011709443099273609,
      "loss": 1.6006,
      "step": 907
    },
    {
      "epoch": 1.2879432624113476,
      "grad_norm": 1.113349199295044,
      "learning_rate": 0.00011699757869249395,
      "loss": 1.4166,
      "step": 908
    },
    {
      "epoch": 1.2893617021276595,
      "grad_norm": 1.0720531940460205,
      "learning_rate": 0.00011690072639225182,
      "loss": 1.5842,
      "step": 909
    },
    {
      "epoch": 1.2907801418439715,
      "grad_norm": 1.0865756273269653,
      "learning_rate": 0.0001168038740920097,
      "loss": 1.87,
      "step": 910
    },
    {
      "epoch": 1.2921985815602837,
      "grad_norm": 1.221030354499817,
      "learning_rate": 0.00011670702179176756,
      "loss": 1.6823,
      "step": 911
    },
    {
      "epoch": 1.2936170212765958,
      "grad_norm": 1.0974559783935547,
      "learning_rate": 0.00011661016949152544,
      "loss": 1.4962,
      "step": 912
    },
    {
      "epoch": 1.2950354609929078,
      "grad_norm": 1.1802328824996948,
      "learning_rate": 0.00011651331719128329,
      "loss": 1.6488,
      "step": 913
    },
    {
      "epoch": 1.2964539007092197,
      "grad_norm": 1.2646870613098145,
      "learning_rate": 0.00011641646489104117,
      "loss": 1.7997,
      "step": 914
    },
    {
      "epoch": 1.297872340425532,
      "grad_norm": 1.0725520849227905,
      "learning_rate": 0.00011631961259079905,
      "loss": 1.426,
      "step": 915
    },
    {
      "epoch": 1.299290780141844,
      "grad_norm": 1.141984462738037,
      "learning_rate": 0.0001162227602905569,
      "loss": 1.6595,
      "step": 916
    },
    {
      "epoch": 1.300709219858156,
      "grad_norm": 1.2002671957015991,
      "learning_rate": 0.00011612590799031478,
      "loss": 1.7648,
      "step": 917
    },
    {
      "epoch": 1.302127659574468,
      "grad_norm": 1.1848368644714355,
      "learning_rate": 0.00011602905569007265,
      "loss": 1.4617,
      "step": 918
    },
    {
      "epoch": 1.3035460992907801,
      "grad_norm": 1.2445858716964722,
      "learning_rate": 0.0001159322033898305,
      "loss": 1.6178,
      "step": 919
    },
    {
      "epoch": 1.3049645390070923,
      "grad_norm": 1.129919409751892,
      "learning_rate": 0.00011583535108958838,
      "loss": 1.609,
      "step": 920
    },
    {
      "epoch": 1.3063829787234043,
      "grad_norm": 1.2382508516311646,
      "learning_rate": 0.00011573849878934626,
      "loss": 1.6737,
      "step": 921
    },
    {
      "epoch": 1.3078014184397162,
      "grad_norm": 1.1222894191741943,
      "learning_rate": 0.00011564164648910411,
      "loss": 1.5186,
      "step": 922
    },
    {
      "epoch": 1.3092198581560284,
      "grad_norm": 1.1041353940963745,
      "learning_rate": 0.00011554479418886199,
      "loss": 1.6019,
      "step": 923
    },
    {
      "epoch": 1.3106382978723405,
      "grad_norm": 1.207726001739502,
      "learning_rate": 0.00011544794188861987,
      "loss": 1.7618,
      "step": 924
    },
    {
      "epoch": 1.3120567375886525,
      "grad_norm": 1.1176342964172363,
      "learning_rate": 0.00011535108958837772,
      "loss": 1.4716,
      "step": 925
    },
    {
      "epoch": 1.3134751773049644,
      "grad_norm": 1.090982437133789,
      "learning_rate": 0.0001152542372881356,
      "loss": 1.6561,
      "step": 926
    },
    {
      "epoch": 1.3148936170212766,
      "grad_norm": 1.2275443077087402,
      "learning_rate": 0.00011515738498789346,
      "loss": 1.8344,
      "step": 927
    },
    {
      "epoch": 1.3163120567375888,
      "grad_norm": 1.203176736831665,
      "learning_rate": 0.00011506053268765134,
      "loss": 1.7922,
      "step": 928
    },
    {
      "epoch": 1.3177304964539007,
      "grad_norm": 1.186428427696228,
      "learning_rate": 0.00011496368038740922,
      "loss": 1.5429,
      "step": 929
    },
    {
      "epoch": 1.3191489361702127,
      "grad_norm": 1.2277957201004028,
      "learning_rate": 0.00011486682808716707,
      "loss": 1.7031,
      "step": 930
    },
    {
      "epoch": 1.3205673758865248,
      "grad_norm": 1.174821376800537,
      "learning_rate": 0.00011476997578692495,
      "loss": 1.5746,
      "step": 931
    },
    {
      "epoch": 1.321985815602837,
      "grad_norm": 1.1172395944595337,
      "learning_rate": 0.00011467312348668283,
      "loss": 1.5025,
      "step": 932
    },
    {
      "epoch": 1.323404255319149,
      "grad_norm": 1.2259165048599243,
      "learning_rate": 0.00011457627118644068,
      "loss": 1.8143,
      "step": 933
    },
    {
      "epoch": 1.324822695035461,
      "grad_norm": 1.2798879146575928,
      "learning_rate": 0.00011447941888619855,
      "loss": 1.8829,
      "step": 934
    },
    {
      "epoch": 1.326241134751773,
      "grad_norm": 2.020556688308716,
      "learning_rate": 0.00011438256658595643,
      "loss": 1.698,
      "step": 935
    },
    {
      "epoch": 1.327659574468085,
      "grad_norm": 1.2863777875900269,
      "learning_rate": 0.00011428571428571428,
      "loss": 1.9202,
      "step": 936
    },
    {
      "epoch": 1.3290780141843972,
      "grad_norm": 1.1320306062698364,
      "learning_rate": 0.00011418886198547216,
      "loss": 1.5284,
      "step": 937
    },
    {
      "epoch": 1.3304964539007091,
      "grad_norm": 1.1341561079025269,
      "learning_rate": 0.00011409200968523004,
      "loss": 1.4473,
      "step": 938
    },
    {
      "epoch": 1.3319148936170213,
      "grad_norm": 1.25192391872406,
      "learning_rate": 0.00011399515738498789,
      "loss": 1.6659,
      "step": 939
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 1.1707910299301147,
      "learning_rate": 0.00011389830508474577,
      "loss": 1.7806,
      "step": 940
    },
    {
      "epoch": 1.3347517730496454,
      "grad_norm": 1.0474827289581299,
      "learning_rate": 0.00011380145278450365,
      "loss": 1.4623,
      "step": 941
    },
    {
      "epoch": 1.3361702127659574,
      "grad_norm": 1.2655093669891357,
      "learning_rate": 0.0001137046004842615,
      "loss": 1.8148,
      "step": 942
    },
    {
      "epoch": 1.3375886524822695,
      "grad_norm": 1.229114294052124,
      "learning_rate": 0.00011360774818401938,
      "loss": 1.6801,
      "step": 943
    },
    {
      "epoch": 1.3390070921985815,
      "grad_norm": 1.1582350730895996,
      "learning_rate": 0.00011351089588377724,
      "loss": 1.6184,
      "step": 944
    },
    {
      "epoch": 1.3404255319148937,
      "grad_norm": 1.1487945318222046,
      "learning_rate": 0.00011341404358353512,
      "loss": 1.6317,
      "step": 945
    },
    {
      "epoch": 1.3418439716312056,
      "grad_norm": 1.2071768045425415,
      "learning_rate": 0.00011331719128329298,
      "loss": 1.6888,
      "step": 946
    },
    {
      "epoch": 1.3432624113475178,
      "grad_norm": 1.239540696144104,
      "learning_rate": 0.00011322033898305085,
      "loss": 1.6647,
      "step": 947
    },
    {
      "epoch": 1.3446808510638297,
      "grad_norm": 1.2201781272888184,
      "learning_rate": 0.00011312348668280873,
      "loss": 1.661,
      "step": 948
    },
    {
      "epoch": 1.346099290780142,
      "grad_norm": 1.113709807395935,
      "learning_rate": 0.0001130266343825666,
      "loss": 1.576,
      "step": 949
    },
    {
      "epoch": 1.3475177304964538,
      "grad_norm": 1.2019202709197998,
      "learning_rate": 0.00011292978208232446,
      "loss": 1.6173,
      "step": 950
    },
    {
      "epoch": 1.348936170212766,
      "grad_norm": 1.1095081567764282,
      "learning_rate": 0.00011283292978208233,
      "loss": 1.7291,
      "step": 951
    },
    {
      "epoch": 1.350354609929078,
      "grad_norm": 1.1851284503936768,
      "learning_rate": 0.00011273607748184021,
      "loss": 1.7803,
      "step": 952
    },
    {
      "epoch": 1.3517730496453901,
      "grad_norm": 1.0814930200576782,
      "learning_rate": 0.00011263922518159806,
      "loss": 1.6586,
      "step": 953
    },
    {
      "epoch": 1.353191489361702,
      "grad_norm": 1.1287448406219482,
      "learning_rate": 0.00011254237288135594,
      "loss": 1.5438,
      "step": 954
    },
    {
      "epoch": 1.3546099290780143,
      "grad_norm": 1.1824431419372559,
      "learning_rate": 0.00011244552058111382,
      "loss": 1.5447,
      "step": 955
    },
    {
      "epoch": 1.3560283687943262,
      "grad_norm": 1.1342796087265015,
      "learning_rate": 0.00011234866828087167,
      "loss": 1.7099,
      "step": 956
    },
    {
      "epoch": 1.3574468085106384,
      "grad_norm": 1.1588134765625,
      "learning_rate": 0.00011225181598062955,
      "loss": 1.6343,
      "step": 957
    },
    {
      "epoch": 1.3588652482269503,
      "grad_norm": 1.146774411201477,
      "learning_rate": 0.0001121549636803874,
      "loss": 1.8391,
      "step": 958
    },
    {
      "epoch": 1.3602836879432625,
      "grad_norm": 1.1741163730621338,
      "learning_rate": 0.00011205811138014528,
      "loss": 1.4939,
      "step": 959
    },
    {
      "epoch": 1.3617021276595744,
      "grad_norm": 1.1536725759506226,
      "learning_rate": 0.00011196125907990315,
      "loss": 1.658,
      "step": 960
    },
    {
      "epoch": 1.3631205673758866,
      "grad_norm": 1.1312150955200195,
      "learning_rate": 0.00011186440677966102,
      "loss": 1.7353,
      "step": 961
    },
    {
      "epoch": 1.3645390070921986,
      "grad_norm": 1.1812450885772705,
      "learning_rate": 0.00011176755447941888,
      "loss": 1.6056,
      "step": 962
    },
    {
      "epoch": 1.3659574468085105,
      "grad_norm": 1.2007774114608765,
      "learning_rate": 0.00011167070217917676,
      "loss": 1.673,
      "step": 963
    },
    {
      "epoch": 1.3673758865248227,
      "grad_norm": 1.2752985954284668,
      "learning_rate": 0.00011157384987893463,
      "loss": 1.9037,
      "step": 964
    },
    {
      "epoch": 1.3687943262411348,
      "grad_norm": 1.2208536863327026,
      "learning_rate": 0.0001114769975786925,
      "loss": 1.6925,
      "step": 965
    },
    {
      "epoch": 1.3702127659574468,
      "grad_norm": 1.2808160781860352,
      "learning_rate": 0.00011138014527845038,
      "loss": 1.6853,
      "step": 966
    },
    {
      "epoch": 1.3716312056737587,
      "grad_norm": 1.089027762413025,
      "learning_rate": 0.00011128329297820823,
      "loss": 1.6555,
      "step": 967
    },
    {
      "epoch": 1.373049645390071,
      "grad_norm": 1.1047166585922241,
      "learning_rate": 0.00011118644067796611,
      "loss": 1.4978,
      "step": 968
    },
    {
      "epoch": 1.374468085106383,
      "grad_norm": 1.0739012956619263,
      "learning_rate": 0.00011108958837772399,
      "loss": 1.6007,
      "step": 969
    },
    {
      "epoch": 1.375886524822695,
      "grad_norm": 1.179026484489441,
      "learning_rate": 0.00011099273607748184,
      "loss": 1.6669,
      "step": 970
    },
    {
      "epoch": 1.377304964539007,
      "grad_norm": 1.1046103239059448,
      "learning_rate": 0.00011089588377723972,
      "loss": 1.5983,
      "step": 971
    },
    {
      "epoch": 1.3787234042553191,
      "grad_norm": 1.1961252689361572,
      "learning_rate": 0.0001107990314769976,
      "loss": 1.5383,
      "step": 972
    },
    {
      "epoch": 1.3801418439716313,
      "grad_norm": 1.1818937063217163,
      "learning_rate": 0.00011070217917675545,
      "loss": 1.6442,
      "step": 973
    },
    {
      "epoch": 1.3815602836879433,
      "grad_norm": 1.2215759754180908,
      "learning_rate": 0.00011060532687651333,
      "loss": 1.9487,
      "step": 974
    },
    {
      "epoch": 1.3829787234042552,
      "grad_norm": 1.4783482551574707,
      "learning_rate": 0.00011050847457627118,
      "loss": 1.4912,
      "step": 975
    },
    {
      "epoch": 1.3843971631205674,
      "grad_norm": 1.1202548742294312,
      "learning_rate": 0.00011041162227602906,
      "loss": 1.4729,
      "step": 976
    },
    {
      "epoch": 1.3858156028368795,
      "grad_norm": 1.1596672534942627,
      "learning_rate": 0.00011031476997578693,
      "loss": 1.6297,
      "step": 977
    },
    {
      "epoch": 1.3872340425531915,
      "grad_norm": 1.1592689752578735,
      "learning_rate": 0.00011021791767554478,
      "loss": 1.6549,
      "step": 978
    },
    {
      "epoch": 1.3886524822695034,
      "grad_norm": 1.1292668581008911,
      "learning_rate": 0.00011012106537530266,
      "loss": 1.6048,
      "step": 979
    },
    {
      "epoch": 1.3900709219858156,
      "grad_norm": 1.0825718641281128,
      "learning_rate": 0.00011002421307506054,
      "loss": 1.5129,
      "step": 980
    },
    {
      "epoch": 1.3914893617021278,
      "grad_norm": 1.2114427089691162,
      "learning_rate": 0.0001099273607748184,
      "loss": 1.823,
      "step": 981
    },
    {
      "epoch": 1.3929078014184397,
      "grad_norm": 1.1911460161209106,
      "learning_rate": 0.00010983050847457627,
      "loss": 1.7533,
      "step": 982
    },
    {
      "epoch": 1.3943262411347517,
      "grad_norm": 1.221181869506836,
      "learning_rate": 0.00010973365617433415,
      "loss": 1.7017,
      "step": 983
    },
    {
      "epoch": 1.3957446808510638,
      "grad_norm": 1.1635750532150269,
      "learning_rate": 0.00010963680387409201,
      "loss": 1.7917,
      "step": 984
    },
    {
      "epoch": 1.397163120567376,
      "grad_norm": 1.235533595085144,
      "learning_rate": 0.00010953995157384989,
      "loss": 1.8506,
      "step": 985
    },
    {
      "epoch": 1.398581560283688,
      "grad_norm": 1.1691476106643677,
      "learning_rate": 0.00010944309927360777,
      "loss": 1.8524,
      "step": 986
    },
    {
      "epoch": 1.4,
      "grad_norm": 1.0987571477890015,
      "learning_rate": 0.00010934624697336562,
      "loss": 1.5169,
      "step": 987
    },
    {
      "epoch": 1.401418439716312,
      "grad_norm": 1.0995463132858276,
      "learning_rate": 0.0001092493946731235,
      "loss": 1.6282,
      "step": 988
    },
    {
      "epoch": 1.4028368794326243,
      "grad_norm": 1.2586760520935059,
      "learning_rate": 0.00010915254237288135,
      "loss": 1.8451,
      "step": 989
    },
    {
      "epoch": 1.4042553191489362,
      "grad_norm": 1.1941345930099487,
      "learning_rate": 0.00010905569007263923,
      "loss": 1.9499,
      "step": 990
    },
    {
      "epoch": 1.4056737588652481,
      "grad_norm": 1.048651933670044,
      "learning_rate": 0.0001089588377723971,
      "loss": 1.5519,
      "step": 991
    },
    {
      "epoch": 1.4070921985815603,
      "grad_norm": 1.0933752059936523,
      "learning_rate": 0.00010886198547215496,
      "loss": 1.3748,
      "step": 992
    },
    {
      "epoch": 1.4085106382978723,
      "grad_norm": 1.1557165384292603,
      "learning_rate": 0.00010876513317191283,
      "loss": 1.6281,
      "step": 993
    },
    {
      "epoch": 1.4099290780141844,
      "grad_norm": 1.192756175994873,
      "learning_rate": 0.00010866828087167071,
      "loss": 1.4983,
      "step": 994
    },
    {
      "epoch": 1.4113475177304964,
      "grad_norm": 1.102967381477356,
      "learning_rate": 0.00010857142857142856,
      "loss": 1.8108,
      "step": 995
    },
    {
      "epoch": 1.4127659574468086,
      "grad_norm": 1.0901373624801636,
      "learning_rate": 0.00010847457627118644,
      "loss": 1.5046,
      "step": 996
    },
    {
      "epoch": 1.4141843971631205,
      "grad_norm": 1.1881366968154907,
      "learning_rate": 0.00010837772397094432,
      "loss": 1.6548,
      "step": 997
    },
    {
      "epoch": 1.4156028368794327,
      "grad_norm": 1.064683437347412,
      "learning_rate": 0.00010828087167070217,
      "loss": 1.3942,
      "step": 998
    },
    {
      "epoch": 1.4170212765957446,
      "grad_norm": 1.1951991319656372,
      "learning_rate": 0.00010818401937046005,
      "loss": 1.5399,
      "step": 999
    },
    {
      "epoch": 1.4184397163120568,
      "grad_norm": 1.143358826637268,
      "learning_rate": 0.00010808716707021793,
      "loss": 1.4712,
      "step": 1000
    },
    {
      "epoch": 1.4184397163120568,
      "eval_loss": 1.7848999500274658,
      "eval_runtime": 95.6905,
      "eval_samples_per_second": 14.735,
      "eval_steps_per_second": 7.368,
      "step": 1000
    },
    {
      "epoch": 1.4198581560283687,
      "grad_norm": 1.250627040863037,
      "learning_rate": 0.00010799031476997579,
      "loss": 1.6666,
      "step": 1001
    },
    {
      "epoch": 1.421276595744681,
      "grad_norm": 1.1276670694351196,
      "learning_rate": 0.00010789346246973367,
      "loss": 1.4467,
      "step": 1002
    },
    {
      "epoch": 1.4226950354609929,
      "grad_norm": 1.1663285493850708,
      "learning_rate": 0.00010779661016949153,
      "loss": 1.6256,
      "step": 1003
    },
    {
      "epoch": 1.424113475177305,
      "grad_norm": 1.3014975786209106,
      "learning_rate": 0.0001076997578692494,
      "loss": 1.6594,
      "step": 1004
    },
    {
      "epoch": 1.425531914893617,
      "grad_norm": 1.1404392719268799,
      "learning_rate": 0.00010760290556900728,
      "loss": 1.5517,
      "step": 1005
    },
    {
      "epoch": 1.4269503546099291,
      "grad_norm": 1.2436838150024414,
      "learning_rate": 0.00010750605326876513,
      "loss": 1.7173,
      "step": 1006
    },
    {
      "epoch": 1.428368794326241,
      "grad_norm": 1.206904411315918,
      "learning_rate": 0.000107409200968523,
      "loss": 1.5916,
      "step": 1007
    },
    {
      "epoch": 1.4297872340425533,
      "grad_norm": 1.0634384155273438,
      "learning_rate": 0.00010731234866828088,
      "loss": 1.5392,
      "step": 1008
    },
    {
      "epoch": 1.4312056737588652,
      "grad_norm": 1.2041406631469727,
      "learning_rate": 0.00010721549636803874,
      "loss": 1.7321,
      "step": 1009
    },
    {
      "epoch": 1.4326241134751774,
      "grad_norm": 1.1784354448318481,
      "learning_rate": 0.00010711864406779661,
      "loss": 1.4247,
      "step": 1010
    },
    {
      "epoch": 1.4340425531914893,
      "grad_norm": 1.1446878910064697,
      "learning_rate": 0.00010702179176755449,
      "loss": 1.5408,
      "step": 1011
    },
    {
      "epoch": 1.4354609929078015,
      "grad_norm": 1.1386545896530151,
      "learning_rate": 0.00010692493946731234,
      "loss": 1.7523,
      "step": 1012
    },
    {
      "epoch": 1.4368794326241134,
      "grad_norm": 1.1305359601974487,
      "learning_rate": 0.00010682808716707022,
      "loss": 1.5649,
      "step": 1013
    },
    {
      "epoch": 1.4382978723404256,
      "grad_norm": 1.203908920288086,
      "learning_rate": 0.0001067312348668281,
      "loss": 1.6884,
      "step": 1014
    },
    {
      "epoch": 1.4397163120567376,
      "grad_norm": 1.1672354936599731,
      "learning_rate": 0.00010663438256658595,
      "loss": 1.499,
      "step": 1015
    },
    {
      "epoch": 1.4411347517730497,
      "grad_norm": 1.0992895364761353,
      "learning_rate": 0.00010653753026634383,
      "loss": 1.6058,
      "step": 1016
    },
    {
      "epoch": 1.4425531914893617,
      "grad_norm": 1.3166310787200928,
      "learning_rate": 0.0001064406779661017,
      "loss": 1.8949,
      "step": 1017
    },
    {
      "epoch": 1.4439716312056738,
      "grad_norm": 1.1119927167892456,
      "learning_rate": 0.00010634382566585957,
      "loss": 1.4649,
      "step": 1018
    },
    {
      "epoch": 1.4453900709219858,
      "grad_norm": 1.5446300506591797,
      "learning_rate": 0.00010624697336561744,
      "loss": 1.5333,
      "step": 1019
    },
    {
      "epoch": 1.4468085106382977,
      "grad_norm": 1.1928774118423462,
      "learning_rate": 0.00010615012106537531,
      "loss": 1.6102,
      "step": 1020
    },
    {
      "epoch": 1.44822695035461,
      "grad_norm": 1.2823550701141357,
      "learning_rate": 0.00010605326876513318,
      "loss": 1.5935,
      "step": 1021
    },
    {
      "epoch": 1.449645390070922,
      "grad_norm": 1.0981944799423218,
      "learning_rate": 0.00010595641646489106,
      "loss": 1.6855,
      "step": 1022
    },
    {
      "epoch": 1.451063829787234,
      "grad_norm": 1.1628055572509766,
      "learning_rate": 0.00010585956416464891,
      "loss": 1.5947,
      "step": 1023
    },
    {
      "epoch": 1.452482269503546,
      "grad_norm": 1.2586944103240967,
      "learning_rate": 0.00010576271186440679,
      "loss": 1.8222,
      "step": 1024
    },
    {
      "epoch": 1.4539007092198581,
      "grad_norm": 1.2343426942825317,
      "learning_rate": 0.00010566585956416466,
      "loss": 1.5343,
      "step": 1025
    },
    {
      "epoch": 1.4553191489361703,
      "grad_norm": 1.162571907043457,
      "learning_rate": 0.00010556900726392251,
      "loss": 1.5977,
      "step": 1026
    },
    {
      "epoch": 1.4567375886524823,
      "grad_norm": 1.1869854927062988,
      "learning_rate": 0.00010547215496368039,
      "loss": 1.4277,
      "step": 1027
    },
    {
      "epoch": 1.4581560283687942,
      "grad_norm": 1.2226042747497559,
      "learning_rate": 0.00010537530266343827,
      "loss": 1.7448,
      "step": 1028
    },
    {
      "epoch": 1.4595744680851064,
      "grad_norm": 1.1162033081054688,
      "learning_rate": 0.00010527845036319612,
      "loss": 1.5056,
      "step": 1029
    },
    {
      "epoch": 1.4609929078014185,
      "grad_norm": 1.1861238479614258,
      "learning_rate": 0.000105181598062954,
      "loss": 1.6546,
      "step": 1030
    },
    {
      "epoch": 1.4624113475177305,
      "grad_norm": 1.2245166301727295,
      "learning_rate": 0.00010508474576271188,
      "loss": 1.7745,
      "step": 1031
    },
    {
      "epoch": 1.4638297872340424,
      "grad_norm": 1.2926734685897827,
      "learning_rate": 0.00010498789346246973,
      "loss": 1.7146,
      "step": 1032
    },
    {
      "epoch": 1.4652482269503546,
      "grad_norm": 1.170531153678894,
      "learning_rate": 0.00010489104116222761,
      "loss": 1.6402,
      "step": 1033
    },
    {
      "epoch": 1.4666666666666668,
      "grad_norm": 1.1785286664962769,
      "learning_rate": 0.00010479418886198549,
      "loss": 1.4685,
      "step": 1034
    },
    {
      "epoch": 1.4680851063829787,
      "grad_norm": 1.2093333005905151,
      "learning_rate": 0.00010469733656174334,
      "loss": 1.5179,
      "step": 1035
    },
    {
      "epoch": 1.4695035460992907,
      "grad_norm": 1.0691760778427124,
      "learning_rate": 0.00010460048426150121,
      "loss": 1.5352,
      "step": 1036
    },
    {
      "epoch": 1.4709219858156029,
      "grad_norm": 1.0903273820877075,
      "learning_rate": 0.00010450363196125908,
      "loss": 1.6217,
      "step": 1037
    },
    {
      "epoch": 1.472340425531915,
      "grad_norm": 1.129533052444458,
      "learning_rate": 0.00010440677966101696,
      "loss": 1.6034,
      "step": 1038
    },
    {
      "epoch": 1.473758865248227,
      "grad_norm": 1.1431431770324707,
      "learning_rate": 0.00010430992736077484,
      "loss": 1.6554,
      "step": 1039
    },
    {
      "epoch": 1.475177304964539,
      "grad_norm": 1.1836168766021729,
      "learning_rate": 0.00010421307506053269,
      "loss": 1.748,
      "step": 1040
    },
    {
      "epoch": 1.476595744680851,
      "grad_norm": 1.104200839996338,
      "learning_rate": 0.00010411622276029056,
      "loss": 1.4904,
      "step": 1041
    },
    {
      "epoch": 1.4780141843971633,
      "grad_norm": 1.159486174583435,
      "learning_rate": 0.00010401937046004844,
      "loss": 1.7875,
      "step": 1042
    },
    {
      "epoch": 1.4794326241134752,
      "grad_norm": 1.1066216230392456,
      "learning_rate": 0.0001039225181598063,
      "loss": 1.5682,
      "step": 1043
    },
    {
      "epoch": 1.4808510638297872,
      "grad_norm": 1.2182421684265137,
      "learning_rate": 0.00010382566585956417,
      "loss": 1.8323,
      "step": 1044
    },
    {
      "epoch": 1.4822695035460993,
      "grad_norm": 1.144346833229065,
      "learning_rate": 0.00010372881355932205,
      "loss": 1.4734,
      "step": 1045
    },
    {
      "epoch": 1.4836879432624113,
      "grad_norm": 1.2416585683822632,
      "learning_rate": 0.0001036319612590799,
      "loss": 1.4771,
      "step": 1046
    },
    {
      "epoch": 1.4851063829787234,
      "grad_norm": 1.2179794311523438,
      "learning_rate": 0.00010353510895883778,
      "loss": 1.644,
      "step": 1047
    },
    {
      "epoch": 1.4865248226950354,
      "grad_norm": 1.188798189163208,
      "learning_rate": 0.00010343825665859566,
      "loss": 1.7105,
      "step": 1048
    },
    {
      "epoch": 1.4879432624113476,
      "grad_norm": 1.1968789100646973,
      "learning_rate": 0.00010334140435835351,
      "loss": 1.6034,
      "step": 1049
    },
    {
      "epoch": 1.4893617021276595,
      "grad_norm": 1.229955792427063,
      "learning_rate": 0.00010324455205811139,
      "loss": 1.894,
      "step": 1050
    },
    {
      "epoch": 1.4907801418439717,
      "grad_norm": 1.16615891456604,
      "learning_rate": 0.00010314769975786926,
      "loss": 1.4654,
      "step": 1051
    },
    {
      "epoch": 1.4921985815602836,
      "grad_norm": 1.387286901473999,
      "learning_rate": 0.00010305084745762712,
      "loss": 1.7727,
      "step": 1052
    },
    {
      "epoch": 1.4936170212765958,
      "grad_norm": 1.1950308084487915,
      "learning_rate": 0.000102953995157385,
      "loss": 1.6251,
      "step": 1053
    },
    {
      "epoch": 1.4950354609929077,
      "grad_norm": 1.148232102394104,
      "learning_rate": 0.00010285714285714286,
      "loss": 1.6269,
      "step": 1054
    },
    {
      "epoch": 1.49645390070922,
      "grad_norm": 1.1423676013946533,
      "learning_rate": 0.00010276029055690072,
      "loss": 1.8724,
      "step": 1055
    },
    {
      "epoch": 1.4978723404255319,
      "grad_norm": 1.3556063175201416,
      "learning_rate": 0.0001026634382566586,
      "loss": 1.7566,
      "step": 1056
    },
    {
      "epoch": 1.499290780141844,
      "grad_norm": 1.1854469776153564,
      "learning_rate": 0.00010256658595641647,
      "loss": 1.5835,
      "step": 1057
    },
    {
      "epoch": 1.500709219858156,
      "grad_norm": 1.1135293245315552,
      "learning_rate": 0.00010246973365617434,
      "loss": 1.6287,
      "step": 1058
    },
    {
      "epoch": 1.5021276595744681,
      "grad_norm": 1.1407042741775513,
      "learning_rate": 0.00010237288135593222,
      "loss": 1.6024,
      "step": 1059
    },
    {
      "epoch": 1.50354609929078,
      "grad_norm": 1.2171127796173096,
      "learning_rate": 0.00010227602905569007,
      "loss": 1.8712,
      "step": 1060
    },
    {
      "epoch": 1.504964539007092,
      "grad_norm": 1.2459406852722168,
      "learning_rate": 0.00010217917675544795,
      "loss": 1.7654,
      "step": 1061
    },
    {
      "epoch": 1.5063829787234042,
      "grad_norm": 1.2182395458221436,
      "learning_rate": 0.00010208232445520583,
      "loss": 1.5626,
      "step": 1062
    },
    {
      "epoch": 1.5078014184397164,
      "grad_norm": 1.1418731212615967,
      "learning_rate": 0.00010198547215496368,
      "loss": 1.6579,
      "step": 1063
    },
    {
      "epoch": 1.5092198581560283,
      "grad_norm": 1.2582125663757324,
      "learning_rate": 0.00010188861985472156,
      "loss": 1.5397,
      "step": 1064
    },
    {
      "epoch": 1.5106382978723403,
      "grad_norm": 1.1945207118988037,
      "learning_rate": 0.00010179176755447944,
      "loss": 1.5254,
      "step": 1065
    },
    {
      "epoch": 1.5120567375886524,
      "grad_norm": 1.1911303997039795,
      "learning_rate": 0.00010169491525423729,
      "loss": 1.719,
      "step": 1066
    },
    {
      "epoch": 1.5134751773049646,
      "grad_norm": 1.2286810874938965,
      "learning_rate": 0.00010159806295399516,
      "loss": 1.663,
      "step": 1067
    },
    {
      "epoch": 1.5148936170212766,
      "grad_norm": 1.2317695617675781,
      "learning_rate": 0.00010150121065375302,
      "loss": 1.4186,
      "step": 1068
    },
    {
      "epoch": 1.5163120567375885,
      "grad_norm": 1.348183512687683,
      "learning_rate": 0.0001014043583535109,
      "loss": 1.8714,
      "step": 1069
    },
    {
      "epoch": 1.5177304964539007,
      "grad_norm": 1.2277743816375732,
      "learning_rate": 0.00010130750605326877,
      "loss": 1.5904,
      "step": 1070
    },
    {
      "epoch": 1.5191489361702128,
      "grad_norm": 1.2605550289154053,
      "learning_rate": 0.00010121065375302662,
      "loss": 1.5752,
      "step": 1071
    },
    {
      "epoch": 1.5205673758865248,
      "grad_norm": 1.1153925657272339,
      "learning_rate": 0.0001011138014527845,
      "loss": 1.5471,
      "step": 1072
    },
    {
      "epoch": 1.5219858156028367,
      "grad_norm": 1.2856723070144653,
      "learning_rate": 0.00010101694915254238,
      "loss": 1.9491,
      "step": 1073
    },
    {
      "epoch": 1.523404255319149,
      "grad_norm": 1.2184542417526245,
      "learning_rate": 0.00010092009685230024,
      "loss": 1.5911,
      "step": 1074
    },
    {
      "epoch": 1.524822695035461,
      "grad_norm": 1.1654748916625977,
      "learning_rate": 0.00010082324455205812,
      "loss": 1.5911,
      "step": 1075
    },
    {
      "epoch": 1.526241134751773,
      "grad_norm": 1.1990547180175781,
      "learning_rate": 0.00010072639225181599,
      "loss": 1.6421,
      "step": 1076
    },
    {
      "epoch": 1.527659574468085,
      "grad_norm": 1.1661653518676758,
      "learning_rate": 0.00010062953995157385,
      "loss": 1.6278,
      "step": 1077
    },
    {
      "epoch": 1.5290780141843971,
      "grad_norm": 1.2501269578933716,
      "learning_rate": 0.00010053268765133173,
      "loss": 1.6054,
      "step": 1078
    },
    {
      "epoch": 1.5304964539007093,
      "grad_norm": 1.2548218965530396,
      "learning_rate": 0.00010043583535108961,
      "loss": 1.557,
      "step": 1079
    },
    {
      "epoch": 1.5319148936170213,
      "grad_norm": 1.2531864643096924,
      "learning_rate": 0.00010033898305084746,
      "loss": 1.7777,
      "step": 1080
    },
    {
      "epoch": 1.5333333333333332,
      "grad_norm": 1.1340835094451904,
      "learning_rate": 0.00010024213075060534,
      "loss": 1.5104,
      "step": 1081
    },
    {
      "epoch": 1.5347517730496454,
      "grad_norm": 1.1483999490737915,
      "learning_rate": 0.00010014527845036321,
      "loss": 1.5319,
      "step": 1082
    },
    {
      "epoch": 1.5361702127659576,
      "grad_norm": 1.1708811521530151,
      "learning_rate": 0.00010004842615012107,
      "loss": 1.585,
      "step": 1083
    },
    {
      "epoch": 1.5375886524822695,
      "grad_norm": 1.2194960117340088,
      "learning_rate": 9.995157384987894e-05,
      "loss": 1.6367,
      "step": 1084
    },
    {
      "epoch": 1.5390070921985815,
      "grad_norm": 1.2664326429367065,
      "learning_rate": 9.985472154963681e-05,
      "loss": 1.423,
      "step": 1085
    },
    {
      "epoch": 1.5404255319148936,
      "grad_norm": 1.201811671257019,
      "learning_rate": 9.975786924939467e-05,
      "loss": 1.5361,
      "step": 1086
    },
    {
      "epoch": 1.5418439716312058,
      "grad_norm": 1.2037533521652222,
      "learning_rate": 9.966101694915255e-05,
      "loss": 1.6281,
      "step": 1087
    },
    {
      "epoch": 1.5432624113475177,
      "grad_norm": 1.3659192323684692,
      "learning_rate": 9.956416464891042e-05,
      "loss": 1.7561,
      "step": 1088
    },
    {
      "epoch": 1.5446808510638297,
      "grad_norm": 1.48048734664917,
      "learning_rate": 9.946731234866828e-05,
      "loss": 1.7909,
      "step": 1089
    },
    {
      "epoch": 1.5460992907801419,
      "grad_norm": 1.189576268196106,
      "learning_rate": 9.937046004842614e-05,
      "loss": 1.5238,
      "step": 1090
    },
    {
      "epoch": 1.547517730496454,
      "grad_norm": 1.1616965532302856,
      "learning_rate": 9.927360774818402e-05,
      "loss": 1.7333,
      "step": 1091
    },
    {
      "epoch": 1.548936170212766,
      "grad_norm": 1.292144775390625,
      "learning_rate": 9.917675544794189e-05,
      "loss": 1.8231,
      "step": 1092
    },
    {
      "epoch": 1.550354609929078,
      "grad_norm": 1.3057124614715576,
      "learning_rate": 9.907990314769977e-05,
      "loss": 1.5822,
      "step": 1093
    },
    {
      "epoch": 1.55177304964539,
      "grad_norm": 1.0835222005844116,
      "learning_rate": 9.898305084745763e-05,
      "loss": 1.5131,
      "step": 1094
    },
    {
      "epoch": 1.5531914893617023,
      "grad_norm": 1.2304952144622803,
      "learning_rate": 9.888619854721551e-05,
      "loss": 1.6151,
      "step": 1095
    },
    {
      "epoch": 1.5546099290780142,
      "grad_norm": 1.1598743200302124,
      "learning_rate": 9.878934624697337e-05,
      "loss": 1.2629,
      "step": 1096
    },
    {
      "epoch": 1.5560283687943262,
      "grad_norm": 1.2297303676605225,
      "learning_rate": 9.869249394673124e-05,
      "loss": 1.5419,
      "step": 1097
    },
    {
      "epoch": 1.5574468085106383,
      "grad_norm": 1.2189191579818726,
      "learning_rate": 9.859564164648912e-05,
      "loss": 1.7223,
      "step": 1098
    },
    {
      "epoch": 1.5588652482269505,
      "grad_norm": 1.139158010482788,
      "learning_rate": 9.849878934624698e-05,
      "loss": 1.6569,
      "step": 1099
    },
    {
      "epoch": 1.5602836879432624,
      "grad_norm": 1.1942012310028076,
      "learning_rate": 9.840193704600484e-05,
      "loss": 1.6851,
      "step": 1100
    },
    {
      "epoch": 1.5617021276595744,
      "grad_norm": 1.2258739471435547,
      "learning_rate": 9.830508474576272e-05,
      "loss": 1.8895,
      "step": 1101
    },
    {
      "epoch": 1.5631205673758866,
      "grad_norm": 1.190891146659851,
      "learning_rate": 9.820823244552059e-05,
      "loss": 1.4321,
      "step": 1102
    },
    {
      "epoch": 1.5645390070921987,
      "grad_norm": 1.283712387084961,
      "learning_rate": 9.811138014527845e-05,
      "loss": 1.6581,
      "step": 1103
    },
    {
      "epoch": 1.5659574468085107,
      "grad_norm": 1.1420197486877441,
      "learning_rate": 9.801452784503632e-05,
      "loss": 1.6242,
      "step": 1104
    },
    {
      "epoch": 1.5673758865248226,
      "grad_norm": 1.176924705505371,
      "learning_rate": 9.79176755447942e-05,
      "loss": 1.4062,
      "step": 1105
    },
    {
      "epoch": 1.5687943262411348,
      "grad_norm": 1.1795506477355957,
      "learning_rate": 9.782082324455206e-05,
      "loss": 1.6127,
      "step": 1106
    },
    {
      "epoch": 1.570212765957447,
      "grad_norm": 1.2599860429763794,
      "learning_rate": 9.772397094430992e-05,
      "loss": 1.7477,
      "step": 1107
    },
    {
      "epoch": 1.571631205673759,
      "grad_norm": 1.1330934762954712,
      "learning_rate": 9.76271186440678e-05,
      "loss": 1.6434,
      "step": 1108
    },
    {
      "epoch": 1.5730496453900709,
      "grad_norm": 1.14244544506073,
      "learning_rate": 9.753026634382567e-05,
      "loss": 1.3755,
      "step": 1109
    },
    {
      "epoch": 1.574468085106383,
      "grad_norm": 1.1504817008972168,
      "learning_rate": 9.743341404358353e-05,
      "loss": 1.5881,
      "step": 1110
    },
    {
      "epoch": 1.5758865248226952,
      "grad_norm": 1.2764220237731934,
      "learning_rate": 9.733656174334141e-05,
      "loss": 1.4897,
      "step": 1111
    },
    {
      "epoch": 1.5773049645390071,
      "grad_norm": 1.217751383781433,
      "learning_rate": 9.723970944309929e-05,
      "loss": 1.7436,
      "step": 1112
    },
    {
      "epoch": 1.578723404255319,
      "grad_norm": 1.2563790082931519,
      "learning_rate": 9.714285714285715e-05,
      "loss": 1.6222,
      "step": 1113
    },
    {
      "epoch": 1.580141843971631,
      "grad_norm": 1.3544992208480835,
      "learning_rate": 9.704600484261502e-05,
      "loss": 1.9061,
      "step": 1114
    },
    {
      "epoch": 1.5815602836879432,
      "grad_norm": 1.1149351596832275,
      "learning_rate": 9.69491525423729e-05,
      "loss": 1.4153,
      "step": 1115
    },
    {
      "epoch": 1.5829787234042554,
      "grad_norm": 1.2329972982406616,
      "learning_rate": 9.685230024213076e-05,
      "loss": 1.6081,
      "step": 1116
    },
    {
      "epoch": 1.5843971631205673,
      "grad_norm": 1.2274267673492432,
      "learning_rate": 9.675544794188862e-05,
      "loss": 1.5443,
      "step": 1117
    },
    {
      "epoch": 1.5858156028368793,
      "grad_norm": 1.1302452087402344,
      "learning_rate": 9.66585956416465e-05,
      "loss": 1.4577,
      "step": 1118
    },
    {
      "epoch": 1.5872340425531914,
      "grad_norm": 1.1702690124511719,
      "learning_rate": 9.656174334140437e-05,
      "loss": 1.5826,
      "step": 1119
    },
    {
      "epoch": 1.5886524822695036,
      "grad_norm": 1.3281491994857788,
      "learning_rate": 9.646489104116223e-05,
      "loss": 1.5827,
      "step": 1120
    },
    {
      "epoch": 1.5900709219858156,
      "grad_norm": 1.1842857599258423,
      "learning_rate": 9.63680387409201e-05,
      "loss": 1.5748,
      "step": 1121
    },
    {
      "epoch": 1.5914893617021275,
      "grad_norm": 1.2697441577911377,
      "learning_rate": 9.627118644067797e-05,
      "loss": 1.7233,
      "step": 1122
    },
    {
      "epoch": 1.5929078014184397,
      "grad_norm": 1.1010757684707642,
      "learning_rate": 9.617433414043584e-05,
      "loss": 1.4082,
      "step": 1123
    },
    {
      "epoch": 1.5943262411347519,
      "grad_norm": 1.2107036113739014,
      "learning_rate": 9.60774818401937e-05,
      "loss": 1.6385,
      "step": 1124
    },
    {
      "epoch": 1.5957446808510638,
      "grad_norm": 1.1693320274353027,
      "learning_rate": 9.598062953995158e-05,
      "loss": 1.4801,
      "step": 1125
    },
    {
      "epoch": 1.5971631205673757,
      "grad_norm": 1.1928670406341553,
      "learning_rate": 9.588377723970945e-05,
      "loss": 1.4418,
      "step": 1126
    },
    {
      "epoch": 1.598581560283688,
      "grad_norm": 1.2616398334503174,
      "learning_rate": 9.578692493946731e-05,
      "loss": 1.573,
      "step": 1127
    },
    {
      "epoch": 1.6,
      "grad_norm": 1.1307281255722046,
      "learning_rate": 9.569007263922517e-05,
      "loss": 1.3505,
      "step": 1128
    },
    {
      "epoch": 1.601418439716312,
      "grad_norm": 1.3365283012390137,
      "learning_rate": 9.559322033898305e-05,
      "loss": 1.7817,
      "step": 1129
    },
    {
      "epoch": 1.602836879432624,
      "grad_norm": 1.2639219760894775,
      "learning_rate": 9.549636803874093e-05,
      "loss": 1.6947,
      "step": 1130
    },
    {
      "epoch": 1.6042553191489362,
      "grad_norm": 1.1617590188980103,
      "learning_rate": 9.53995157384988e-05,
      "loss": 1.6466,
      "step": 1131
    },
    {
      "epoch": 1.6056737588652483,
      "grad_norm": 1.2534148693084717,
      "learning_rate": 9.530266343825667e-05,
      "loss": 1.6833,
      "step": 1132
    },
    {
      "epoch": 1.6070921985815603,
      "grad_norm": 1.2897037267684937,
      "learning_rate": 9.520581113801454e-05,
      "loss": 1.7585,
      "step": 1133
    },
    {
      "epoch": 1.6085106382978722,
      "grad_norm": 1.1867380142211914,
      "learning_rate": 9.51089588377724e-05,
      "loss": 1.6483,
      "step": 1134
    },
    {
      "epoch": 1.6099290780141844,
      "grad_norm": 1.2313565015792847,
      "learning_rate": 9.501210653753027e-05,
      "loss": 1.5283,
      "step": 1135
    },
    {
      "epoch": 1.6113475177304966,
      "grad_norm": 1.2211858034133911,
      "learning_rate": 9.491525423728815e-05,
      "loss": 1.5649,
      "step": 1136
    },
    {
      "epoch": 1.6127659574468085,
      "grad_norm": 1.1665276288986206,
      "learning_rate": 9.481840193704601e-05,
      "loss": 1.5342,
      "step": 1137
    },
    {
      "epoch": 1.6141843971631205,
      "grad_norm": 1.201115369796753,
      "learning_rate": 9.472154963680387e-05,
      "loss": 1.6683,
      "step": 1138
    },
    {
      "epoch": 1.6156028368794326,
      "grad_norm": 1.1044392585754395,
      "learning_rate": 9.462469733656175e-05,
      "loss": 1.6347,
      "step": 1139
    },
    {
      "epoch": 1.6170212765957448,
      "grad_norm": 1.3035749197006226,
      "learning_rate": 9.452784503631962e-05,
      "loss": 1.8378,
      "step": 1140
    },
    {
      "epoch": 1.6184397163120567,
      "grad_norm": 1.2326042652130127,
      "learning_rate": 9.443099273607748e-05,
      "loss": 1.5633,
      "step": 1141
    },
    {
      "epoch": 1.6198581560283687,
      "grad_norm": 1.197429895401001,
      "learning_rate": 9.433414043583536e-05,
      "loss": 1.6478,
      "step": 1142
    },
    {
      "epoch": 1.6212765957446809,
      "grad_norm": 1.1895322799682617,
      "learning_rate": 9.423728813559322e-05,
      "loss": 1.5572,
      "step": 1143
    },
    {
      "epoch": 1.622695035460993,
      "grad_norm": 1.3060075044631958,
      "learning_rate": 9.414043583535109e-05,
      "loss": 1.534,
      "step": 1144
    },
    {
      "epoch": 1.624113475177305,
      "grad_norm": 1.3099530935287476,
      "learning_rate": 9.404358353510895e-05,
      "loss": 1.3192,
      "step": 1145
    },
    {
      "epoch": 1.625531914893617,
      "grad_norm": 1.2094568014144897,
      "learning_rate": 9.394673123486683e-05,
      "loss": 1.5668,
      "step": 1146
    },
    {
      "epoch": 1.626950354609929,
      "grad_norm": 1.1802932024002075,
      "learning_rate": 9.38498789346247e-05,
      "loss": 1.7688,
      "step": 1147
    },
    {
      "epoch": 1.6283687943262413,
      "grad_norm": 1.3282928466796875,
      "learning_rate": 9.375302663438257e-05,
      "loss": 1.4079,
      "step": 1148
    },
    {
      "epoch": 1.6297872340425532,
      "grad_norm": 1.1205180883407593,
      "learning_rate": 9.365617433414044e-05,
      "loss": 1.548,
      "step": 1149
    },
    {
      "epoch": 1.6312056737588652,
      "grad_norm": 1.2785497903823853,
      "learning_rate": 9.355932203389832e-05,
      "loss": 1.798,
      "step": 1150
    },
    {
      "epoch": 1.6326241134751773,
      "grad_norm": 1.286961317062378,
      "learning_rate": 9.346246973365618e-05,
      "loss": 1.9104,
      "step": 1151
    },
    {
      "epoch": 1.6340425531914895,
      "grad_norm": 1.239988088607788,
      "learning_rate": 9.336561743341405e-05,
      "loss": 1.6305,
      "step": 1152
    },
    {
      "epoch": 1.6354609929078014,
      "grad_norm": 1.2006521224975586,
      "learning_rate": 9.326876513317192e-05,
      "loss": 1.5667,
      "step": 1153
    },
    {
      "epoch": 1.6368794326241134,
      "grad_norm": 1.232912302017212,
      "learning_rate": 9.317191283292979e-05,
      "loss": 1.8349,
      "step": 1154
    },
    {
      "epoch": 1.6382978723404256,
      "grad_norm": 1.1713968515396118,
      "learning_rate": 9.307506053268765e-05,
      "loss": 1.5621,
      "step": 1155
    },
    {
      "epoch": 1.6397163120567377,
      "grad_norm": 1.1977888345718384,
      "learning_rate": 9.297820823244553e-05,
      "loss": 1.4652,
      "step": 1156
    },
    {
      "epoch": 1.6411347517730497,
      "grad_norm": 1.192791223526001,
      "learning_rate": 9.28813559322034e-05,
      "loss": 1.4579,
      "step": 1157
    },
    {
      "epoch": 1.6425531914893616,
      "grad_norm": 1.183491826057434,
      "learning_rate": 9.278450363196126e-05,
      "loss": 1.4482,
      "step": 1158
    },
    {
      "epoch": 1.6439716312056738,
      "grad_norm": 1.3135274648666382,
      "learning_rate": 9.268765133171913e-05,
      "loss": 1.5185,
      "step": 1159
    },
    {
      "epoch": 1.645390070921986,
      "grad_norm": 1.3244645595550537,
      "learning_rate": 9.2590799031477e-05,
      "loss": 1.6535,
      "step": 1160
    },
    {
      "epoch": 1.646808510638298,
      "grad_norm": 1.1740316152572632,
      "learning_rate": 9.249394673123487e-05,
      "loss": 1.4743,
      "step": 1161
    },
    {
      "epoch": 1.6482269503546099,
      "grad_norm": 1.1661099195480347,
      "learning_rate": 9.239709443099273e-05,
      "loss": 1.5318,
      "step": 1162
    },
    {
      "epoch": 1.649645390070922,
      "grad_norm": 1.2274373769760132,
      "learning_rate": 9.230024213075061e-05,
      "loss": 1.5361,
      "step": 1163
    },
    {
      "epoch": 1.6510638297872342,
      "grad_norm": 1.2832502126693726,
      "learning_rate": 9.220338983050847e-05,
      "loss": 1.743,
      "step": 1164
    },
    {
      "epoch": 1.6524822695035462,
      "grad_norm": 1.225006341934204,
      "learning_rate": 9.210653753026634e-05,
      "loss": 1.6311,
      "step": 1165
    },
    {
      "epoch": 1.653900709219858,
      "grad_norm": 1.2135566473007202,
      "learning_rate": 9.200968523002422e-05,
      "loss": 1.7116,
      "step": 1166
    },
    {
      "epoch": 1.65531914893617,
      "grad_norm": 1.187087893486023,
      "learning_rate": 9.191283292978208e-05,
      "loss": 1.3664,
      "step": 1167
    },
    {
      "epoch": 1.6567375886524822,
      "grad_norm": 1.2258929014205933,
      "learning_rate": 9.181598062953996e-05,
      "loss": 1.4041,
      "step": 1168
    },
    {
      "epoch": 1.6581560283687944,
      "grad_norm": 1.1951972246170044,
      "learning_rate": 9.171912832929782e-05,
      "loss": 1.5384,
      "step": 1169
    },
    {
      "epoch": 1.6595744680851063,
      "grad_norm": 1.266250491142273,
      "learning_rate": 9.16222760290557e-05,
      "loss": 1.7322,
      "step": 1170
    },
    {
      "epoch": 1.6609929078014183,
      "grad_norm": 1.3175538778305054,
      "learning_rate": 9.152542372881357e-05,
      "loss": 1.5449,
      "step": 1171
    },
    {
      "epoch": 1.6624113475177305,
      "grad_norm": 1.2568349838256836,
      "learning_rate": 9.142857142857143e-05,
      "loss": 1.7921,
      "step": 1172
    },
    {
      "epoch": 1.6638297872340426,
      "grad_norm": 1.2950316667556763,
      "learning_rate": 9.133171912832931e-05,
      "loss": 1.5976,
      "step": 1173
    },
    {
      "epoch": 1.6652482269503546,
      "grad_norm": 1.1912752389907837,
      "learning_rate": 9.123486682808717e-05,
      "loss": 1.6401,
      "step": 1174
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 1.230519413948059,
      "learning_rate": 9.113801452784504e-05,
      "loss": 1.4876,
      "step": 1175
    },
    {
      "epoch": 1.6680851063829787,
      "grad_norm": 1.2477937936782837,
      "learning_rate": 9.10411622276029e-05,
      "loss": 1.6295,
      "step": 1176
    },
    {
      "epoch": 1.6695035460992909,
      "grad_norm": 1.0978775024414062,
      "learning_rate": 9.094430992736078e-05,
      "loss": 1.3149,
      "step": 1177
    },
    {
      "epoch": 1.6709219858156028,
      "grad_norm": 1.1919137239456177,
      "learning_rate": 9.084745762711865e-05,
      "loss": 1.4392,
      "step": 1178
    },
    {
      "epoch": 1.6723404255319148,
      "grad_norm": 1.2115204334259033,
      "learning_rate": 9.075060532687651e-05,
      "loss": 1.5806,
      "step": 1179
    },
    {
      "epoch": 1.673758865248227,
      "grad_norm": 1.281996250152588,
      "learning_rate": 9.065375302663439e-05,
      "loss": 1.7746,
      "step": 1180
    },
    {
      "epoch": 1.675177304964539,
      "grad_norm": 1.2350205183029175,
      "learning_rate": 9.055690072639225e-05,
      "loss": 1.7959,
      "step": 1181
    },
    {
      "epoch": 1.676595744680851,
      "grad_norm": 1.1735223531723022,
      "learning_rate": 9.046004842615012e-05,
      "loss": 1.5155,
      "step": 1182
    },
    {
      "epoch": 1.678014184397163,
      "grad_norm": 1.2192974090576172,
      "learning_rate": 9.036319612590798e-05,
      "loss": 1.621,
      "step": 1183
    },
    {
      "epoch": 1.6794326241134752,
      "grad_norm": 1.2157201766967773,
      "learning_rate": 9.026634382566586e-05,
      "loss": 1.6298,
      "step": 1184
    },
    {
      "epoch": 1.6808510638297873,
      "grad_norm": 1.1091216802597046,
      "learning_rate": 9.016949152542374e-05,
      "loss": 1.5534,
      "step": 1185
    },
    {
      "epoch": 1.6822695035460993,
      "grad_norm": 1.1780449151992798,
      "learning_rate": 9.00726392251816e-05,
      "loss": 1.5919,
      "step": 1186
    },
    {
      "epoch": 1.6836879432624112,
      "grad_norm": 1.2776356935501099,
      "learning_rate": 8.997578692493948e-05,
      "loss": 1.6397,
      "step": 1187
    },
    {
      "epoch": 1.6851063829787234,
      "grad_norm": 1.146835207939148,
      "learning_rate": 8.987893462469735e-05,
      "loss": 1.5355,
      "step": 1188
    },
    {
      "epoch": 1.6865248226950356,
      "grad_norm": 1.1458556652069092,
      "learning_rate": 8.978208232445521e-05,
      "loss": 1.5346,
      "step": 1189
    },
    {
      "epoch": 1.6879432624113475,
      "grad_norm": 1.1859493255615234,
      "learning_rate": 8.968523002421308e-05,
      "loss": 1.5823,
      "step": 1190
    },
    {
      "epoch": 1.6893617021276595,
      "grad_norm": 1.1900116205215454,
      "learning_rate": 8.958837772397095e-05,
      "loss": 1.773,
      "step": 1191
    },
    {
      "epoch": 1.6907801418439716,
      "grad_norm": 1.1376723051071167,
      "learning_rate": 8.949152542372882e-05,
      "loss": 1.413,
      "step": 1192
    },
    {
      "epoch": 1.6921985815602838,
      "grad_norm": 1.2381013631820679,
      "learning_rate": 8.939467312348668e-05,
      "loss": 1.7663,
      "step": 1193
    },
    {
      "epoch": 1.6936170212765957,
      "grad_norm": 1.1931205987930298,
      "learning_rate": 8.929782082324456e-05,
      "loss": 1.6963,
      "step": 1194
    },
    {
      "epoch": 1.6950354609929077,
      "grad_norm": 1.1824601888656616,
      "learning_rate": 8.920096852300243e-05,
      "loss": 1.5168,
      "step": 1195
    },
    {
      "epoch": 1.6964539007092199,
      "grad_norm": 1.1185039281845093,
      "learning_rate": 8.910411622276029e-05,
      "loss": 1.4956,
      "step": 1196
    },
    {
      "epoch": 1.697872340425532,
      "grad_norm": 1.1359814405441284,
      "learning_rate": 8.900726392251817e-05,
      "loss": 1.4329,
      "step": 1197
    },
    {
      "epoch": 1.699290780141844,
      "grad_norm": 1.203432321548462,
      "learning_rate": 8.891041162227603e-05,
      "loss": 1.7253,
      "step": 1198
    },
    {
      "epoch": 1.700709219858156,
      "grad_norm": 1.0977801084518433,
      "learning_rate": 8.88135593220339e-05,
      "loss": 1.333,
      "step": 1199
    },
    {
      "epoch": 1.702127659574468,
      "grad_norm": 1.321647047996521,
      "learning_rate": 8.871670702179176e-05,
      "loss": 1.5332,
      "step": 1200
    },
    {
      "epoch": 1.7035460992907803,
      "grad_norm": 1.1613287925720215,
      "learning_rate": 8.861985472154964e-05,
      "loss": 1.5949,
      "step": 1201
    },
    {
      "epoch": 1.7049645390070922,
      "grad_norm": 1.2112473249435425,
      "learning_rate": 8.85230024213075e-05,
      "loss": 1.6944,
      "step": 1202
    },
    {
      "epoch": 1.7063829787234042,
      "grad_norm": 1.1608988046646118,
      "learning_rate": 8.842615012106538e-05,
      "loss": 1.5788,
      "step": 1203
    },
    {
      "epoch": 1.7078014184397163,
      "grad_norm": 1.2491108179092407,
      "learning_rate": 8.832929782082325e-05,
      "loss": 1.7817,
      "step": 1204
    },
    {
      "epoch": 1.7092198581560285,
      "grad_norm": 1.2189260721206665,
      "learning_rate": 8.823244552058113e-05,
      "loss": 1.6144,
      "step": 1205
    },
    {
      "epoch": 1.7106382978723405,
      "grad_norm": 1.1778279542922974,
      "learning_rate": 8.813559322033899e-05,
      "loss": 1.5616,
      "step": 1206
    },
    {
      "epoch": 1.7120567375886524,
      "grad_norm": 1.215409755706787,
      "learning_rate": 8.803874092009685e-05,
      "loss": 1.8003,
      "step": 1207
    },
    {
      "epoch": 1.7134751773049646,
      "grad_norm": 1.2682640552520752,
      "learning_rate": 8.794188861985473e-05,
      "loss": 1.6386,
      "step": 1208
    },
    {
      "epoch": 1.7148936170212767,
      "grad_norm": 1.2108651399612427,
      "learning_rate": 8.78450363196126e-05,
      "loss": 1.6346,
      "step": 1209
    },
    {
      "epoch": 1.7163120567375887,
      "grad_norm": 1.2301558256149292,
      "learning_rate": 8.774818401937046e-05,
      "loss": 1.6658,
      "step": 1210
    },
    {
      "epoch": 1.7177304964539006,
      "grad_norm": 1.1428885459899902,
      "learning_rate": 8.765133171912834e-05,
      "loss": 1.4897,
      "step": 1211
    },
    {
      "epoch": 1.7191489361702128,
      "grad_norm": 1.334267497062683,
      "learning_rate": 8.75544794188862e-05,
      "loss": 1.7627,
      "step": 1212
    },
    {
      "epoch": 1.720567375886525,
      "grad_norm": 1.2525633573532104,
      "learning_rate": 8.745762711864407e-05,
      "loss": 1.6567,
      "step": 1213
    },
    {
      "epoch": 1.721985815602837,
      "grad_norm": 1.1409846544265747,
      "learning_rate": 8.736077481840193e-05,
      "loss": 1.5922,
      "step": 1214
    },
    {
      "epoch": 1.7234042553191489,
      "grad_norm": 1.207256555557251,
      "learning_rate": 8.726392251815981e-05,
      "loss": 1.7564,
      "step": 1215
    },
    {
      "epoch": 1.724822695035461,
      "grad_norm": 1.3131457567214966,
      "learning_rate": 8.716707021791768e-05,
      "loss": 1.5678,
      "step": 1216
    },
    {
      "epoch": 1.7262411347517732,
      "grad_norm": 1.2557023763656616,
      "learning_rate": 8.707021791767554e-05,
      "loss": 1.5466,
      "step": 1217
    },
    {
      "epoch": 1.7276595744680852,
      "grad_norm": 1.1587532758712769,
      "learning_rate": 8.697336561743342e-05,
      "loss": 1.7134,
      "step": 1218
    },
    {
      "epoch": 1.729078014184397,
      "grad_norm": 1.2149909734725952,
      "learning_rate": 8.687651331719128e-05,
      "loss": 1.6842,
      "step": 1219
    },
    {
      "epoch": 1.7304964539007093,
      "grad_norm": 1.1420096158981323,
      "learning_rate": 8.677966101694915e-05,
      "loss": 1.634,
      "step": 1220
    },
    {
      "epoch": 1.7319148936170212,
      "grad_norm": 1.2958171367645264,
      "learning_rate": 8.668280871670703e-05,
      "loss": 1.7922,
      "step": 1221
    },
    {
      "epoch": 1.7333333333333334,
      "grad_norm": 1.1991345882415771,
      "learning_rate": 8.658595641646489e-05,
      "loss": 1.5574,
      "step": 1222
    },
    {
      "epoch": 1.7347517730496453,
      "grad_norm": 1.1450904607772827,
      "learning_rate": 8.648910411622277e-05,
      "loss": 1.6932,
      "step": 1223
    },
    {
      "epoch": 1.7361702127659573,
      "grad_norm": 1.15325927734375,
      "learning_rate": 8.639225181598063e-05,
      "loss": 1.4413,
      "step": 1224
    },
    {
      "epoch": 1.7375886524822695,
      "grad_norm": 1.2434687614440918,
      "learning_rate": 8.629539951573851e-05,
      "loss": 1.8157,
      "step": 1225
    },
    {
      "epoch": 1.7390070921985816,
      "grad_norm": 1.2423616647720337,
      "learning_rate": 8.619854721549638e-05,
      "loss": 1.8207,
      "step": 1226
    },
    {
      "epoch": 1.7404255319148936,
      "grad_norm": 1.323386311531067,
      "learning_rate": 8.610169491525424e-05,
      "loss": 1.6637,
      "step": 1227
    },
    {
      "epoch": 1.7418439716312055,
      "grad_norm": 1.2557734251022339,
      "learning_rate": 8.600484261501212e-05,
      "loss": 1.6063,
      "step": 1228
    },
    {
      "epoch": 1.7432624113475177,
      "grad_norm": 1.2526748180389404,
      "learning_rate": 8.590799031476998e-05,
      "loss": 1.6698,
      "step": 1229
    },
    {
      "epoch": 1.7446808510638299,
      "grad_norm": 1.2981785535812378,
      "learning_rate": 8.581113801452785e-05,
      "loss": 1.6246,
      "step": 1230
    },
    {
      "epoch": 1.7460992907801418,
      "grad_norm": 1.2107841968536377,
      "learning_rate": 8.571428571428571e-05,
      "loss": 1.5524,
      "step": 1231
    },
    {
      "epoch": 1.7475177304964538,
      "grad_norm": 1.1543030738830566,
      "learning_rate": 8.561743341404359e-05,
      "loss": 1.5607,
      "step": 1232
    },
    {
      "epoch": 1.748936170212766,
      "grad_norm": 1.270320177078247,
      "learning_rate": 8.552058111380146e-05,
      "loss": 1.5876,
      "step": 1233
    },
    {
      "epoch": 1.750354609929078,
      "grad_norm": 1.1761534214019775,
      "learning_rate": 8.542372881355932e-05,
      "loss": 1.4099,
      "step": 1234
    },
    {
      "epoch": 1.75177304964539,
      "grad_norm": 1.2721564769744873,
      "learning_rate": 8.53268765133172e-05,
      "loss": 1.5806,
      "step": 1235
    },
    {
      "epoch": 1.753191489361702,
      "grad_norm": 1.23183012008667,
      "learning_rate": 8.523002421307506e-05,
      "loss": 1.4681,
      "step": 1236
    },
    {
      "epoch": 1.7546099290780142,
      "grad_norm": 1.1974949836730957,
      "learning_rate": 8.513317191283293e-05,
      "loss": 1.4283,
      "step": 1237
    },
    {
      "epoch": 1.7560283687943263,
      "grad_norm": 1.2160788774490356,
      "learning_rate": 8.503631961259079e-05,
      "loss": 1.5087,
      "step": 1238
    },
    {
      "epoch": 1.7574468085106383,
      "grad_norm": 1.2650861740112305,
      "learning_rate": 8.493946731234867e-05,
      "loss": 1.4186,
      "step": 1239
    },
    {
      "epoch": 1.7588652482269502,
      "grad_norm": 1.3660235404968262,
      "learning_rate": 8.484261501210653e-05,
      "loss": 1.6632,
      "step": 1240
    },
    {
      "epoch": 1.7602836879432624,
      "grad_norm": 1.133344054222107,
      "learning_rate": 8.474576271186441e-05,
      "loss": 1.498,
      "step": 1241
    },
    {
      "epoch": 1.7617021276595746,
      "grad_norm": 1.1711515188217163,
      "learning_rate": 8.464891041162229e-05,
      "loss": 1.4885,
      "step": 1242
    },
    {
      "epoch": 1.7631205673758865,
      "grad_norm": 1.2109581232070923,
      "learning_rate": 8.455205811138016e-05,
      "loss": 1.6616,
      "step": 1243
    },
    {
      "epoch": 1.7645390070921985,
      "grad_norm": 1.2399318218231201,
      "learning_rate": 8.445520581113802e-05,
      "loss": 1.7038,
      "step": 1244
    },
    {
      "epoch": 1.7659574468085106,
      "grad_norm": 1.1890742778778076,
      "learning_rate": 8.43583535108959e-05,
      "loss": 1.6436,
      "step": 1245
    },
    {
      "epoch": 1.7673758865248228,
      "grad_norm": 1.2202457189559937,
      "learning_rate": 8.426150121065376e-05,
      "loss": 1.4743,
      "step": 1246
    },
    {
      "epoch": 1.7687943262411348,
      "grad_norm": 1.3332960605621338,
      "learning_rate": 8.416464891041163e-05,
      "loss": 1.6438,
      "step": 1247
    },
    {
      "epoch": 1.7702127659574467,
      "grad_norm": 1.2063568830490112,
      "learning_rate": 8.406779661016949e-05,
      "loss": 1.6183,
      "step": 1248
    },
    {
      "epoch": 1.7716312056737589,
      "grad_norm": 1.2814836502075195,
      "learning_rate": 8.397094430992737e-05,
      "loss": 1.4927,
      "step": 1249
    },
    {
      "epoch": 1.773049645390071,
      "grad_norm": 1.164703607559204,
      "learning_rate": 8.387409200968523e-05,
      "loss": 1.3537,
      "step": 1250
    },
    {
      "epoch": 1.774468085106383,
      "grad_norm": 1.2187211513519287,
      "learning_rate": 8.37772397094431e-05,
      "loss": 1.4638,
      "step": 1251
    },
    {
      "epoch": 1.775886524822695,
      "grad_norm": 1.1555248498916626,
      "learning_rate": 8.368038740920098e-05,
      "loss": 1.566,
      "step": 1252
    },
    {
      "epoch": 1.777304964539007,
      "grad_norm": 1.2372775077819824,
      "learning_rate": 8.358353510895884e-05,
      "loss": 1.6263,
      "step": 1253
    },
    {
      "epoch": 1.7787234042553193,
      "grad_norm": 1.2899086475372314,
      "learning_rate": 8.34866828087167e-05,
      "loss": 1.6523,
      "step": 1254
    },
    {
      "epoch": 1.7801418439716312,
      "grad_norm": 1.215492606163025,
      "learning_rate": 8.338983050847457e-05,
      "loss": 1.9679,
      "step": 1255
    },
    {
      "epoch": 1.7815602836879432,
      "grad_norm": 1.2721043825149536,
      "learning_rate": 8.329297820823245e-05,
      "loss": 1.7326,
      "step": 1256
    },
    {
      "epoch": 1.7829787234042553,
      "grad_norm": 1.2488867044448853,
      "learning_rate": 8.319612590799031e-05,
      "loss": 1.5653,
      "step": 1257
    },
    {
      "epoch": 1.7843971631205675,
      "grad_norm": 1.3319871425628662,
      "learning_rate": 8.309927360774819e-05,
      "loss": 1.5829,
      "step": 1258
    },
    {
      "epoch": 1.7858156028368795,
      "grad_norm": 1.2554069757461548,
      "learning_rate": 8.300242130750606e-05,
      "loss": 1.7275,
      "step": 1259
    },
    {
      "epoch": 1.7872340425531914,
      "grad_norm": 1.2222684621810913,
      "learning_rate": 8.290556900726393e-05,
      "loss": 1.5165,
      "step": 1260
    },
    {
      "epoch": 1.7886524822695036,
      "grad_norm": 1.199997067451477,
      "learning_rate": 8.28087167070218e-05,
      "loss": 1.5362,
      "step": 1261
    },
    {
      "epoch": 1.7900709219858157,
      "grad_norm": 1.258263111114502,
      "learning_rate": 8.271186440677966e-05,
      "loss": 1.7044,
      "step": 1262
    },
    {
      "epoch": 1.7914893617021277,
      "grad_norm": 1.1825488805770874,
      "learning_rate": 8.261501210653754e-05,
      "loss": 1.5957,
      "step": 1263
    },
    {
      "epoch": 1.7929078014184396,
      "grad_norm": 1.1580922603607178,
      "learning_rate": 8.25181598062954e-05,
      "loss": 1.5715,
      "step": 1264
    },
    {
      "epoch": 1.7943262411347518,
      "grad_norm": 1.4064009189605713,
      "learning_rate": 8.242130750605327e-05,
      "loss": 1.7803,
      "step": 1265
    },
    {
      "epoch": 1.795744680851064,
      "grad_norm": 1.1824532747268677,
      "learning_rate": 8.232445520581115e-05,
      "loss": 1.5509,
      "step": 1266
    },
    {
      "epoch": 1.797163120567376,
      "grad_norm": 1.288155198097229,
      "learning_rate": 8.222760290556901e-05,
      "loss": 1.47,
      "step": 1267
    },
    {
      "epoch": 1.7985815602836879,
      "grad_norm": 1.360640048980713,
      "learning_rate": 8.213075060532688e-05,
      "loss": 1.9228,
      "step": 1268
    },
    {
      "epoch": 1.8,
      "grad_norm": 1.1402020454406738,
      "learning_rate": 8.203389830508474e-05,
      "loss": 1.5839,
      "step": 1269
    },
    {
      "epoch": 1.8014184397163122,
      "grad_norm": 1.3218138217926025,
      "learning_rate": 8.193704600484262e-05,
      "loss": 1.6615,
      "step": 1270
    },
    {
      "epoch": 1.8028368794326242,
      "grad_norm": 1.2762415409088135,
      "learning_rate": 8.184019370460048e-05,
      "loss": 1.7781,
      "step": 1271
    },
    {
      "epoch": 1.804255319148936,
      "grad_norm": 1.204567790031433,
      "learning_rate": 8.174334140435835e-05,
      "loss": 1.6313,
      "step": 1272
    },
    {
      "epoch": 1.8056737588652483,
      "grad_norm": 1.2543363571166992,
      "learning_rate": 8.164648910411623e-05,
      "loss": 1.5454,
      "step": 1273
    },
    {
      "epoch": 1.8070921985815604,
      "grad_norm": 1.2237075567245483,
      "learning_rate": 8.154963680387409e-05,
      "loss": 1.532,
      "step": 1274
    },
    {
      "epoch": 1.8085106382978724,
      "grad_norm": 1.2514640092849731,
      "learning_rate": 8.145278450363196e-05,
      "loss": 1.5346,
      "step": 1275
    },
    {
      "epoch": 1.8099290780141843,
      "grad_norm": 1.2860122919082642,
      "learning_rate": 8.135593220338983e-05,
      "loss": 1.6694,
      "step": 1276
    },
    {
      "epoch": 1.8113475177304963,
      "grad_norm": 1.1732842922210693,
      "learning_rate": 8.12590799031477e-05,
      "loss": 1.5221,
      "step": 1277
    },
    {
      "epoch": 1.8127659574468085,
      "grad_norm": 1.292851209640503,
      "learning_rate": 8.116222760290558e-05,
      "loss": 1.6847,
      "step": 1278
    },
    {
      "epoch": 1.8141843971631206,
      "grad_norm": 1.2947359085083008,
      "learning_rate": 8.106537530266344e-05,
      "loss": 1.7441,
      "step": 1279
    },
    {
      "epoch": 1.8156028368794326,
      "grad_norm": 1.1717039346694946,
      "learning_rate": 8.096852300242132e-05,
      "loss": 1.6176,
      "step": 1280
    },
    {
      "epoch": 1.8170212765957445,
      "grad_norm": 1.2617876529693604,
      "learning_rate": 8.087167070217918e-05,
      "loss": 1.5985,
      "step": 1281
    },
    {
      "epoch": 1.8184397163120567,
      "grad_norm": 1.1685508489608765,
      "learning_rate": 8.077481840193705e-05,
      "loss": 1.4514,
      "step": 1282
    },
    {
      "epoch": 1.8198581560283689,
      "grad_norm": 1.2071070671081543,
      "learning_rate": 8.067796610169493e-05,
      "loss": 1.5437,
      "step": 1283
    },
    {
      "epoch": 1.8212765957446808,
      "grad_norm": 1.2090308666229248,
      "learning_rate": 8.058111380145279e-05,
      "loss": 1.6169,
      "step": 1284
    },
    {
      "epoch": 1.8226950354609928,
      "grad_norm": 1.2786093950271606,
      "learning_rate": 8.048426150121066e-05,
      "loss": 1.6827,
      "step": 1285
    },
    {
      "epoch": 1.824113475177305,
      "grad_norm": 1.2549551725387573,
      "learning_rate": 8.038740920096852e-05,
      "loss": 1.5771,
      "step": 1286
    },
    {
      "epoch": 1.825531914893617,
      "grad_norm": 1.1693141460418701,
      "learning_rate": 8.02905569007264e-05,
      "loss": 1.5316,
      "step": 1287
    },
    {
      "epoch": 1.826950354609929,
      "grad_norm": 1.262085199356079,
      "learning_rate": 8.019370460048426e-05,
      "loss": 1.64,
      "step": 1288
    },
    {
      "epoch": 1.828368794326241,
      "grad_norm": 1.253464937210083,
      "learning_rate": 8.009685230024213e-05,
      "loss": 1.611,
      "step": 1289
    },
    {
      "epoch": 1.8297872340425532,
      "grad_norm": 1.2331581115722656,
      "learning_rate": 8e-05,
      "loss": 1.524,
      "step": 1290
    },
    {
      "epoch": 1.8312056737588653,
      "grad_norm": 1.2263919115066528,
      "learning_rate": 7.990314769975787e-05,
      "loss": 1.5344,
      "step": 1291
    },
    {
      "epoch": 1.8326241134751773,
      "grad_norm": 1.2493174076080322,
      "learning_rate": 7.980629539951574e-05,
      "loss": 1.5552,
      "step": 1292
    },
    {
      "epoch": 1.8340425531914892,
      "grad_norm": 1.3451696634292603,
      "learning_rate": 7.97094430992736e-05,
      "loss": 1.5394,
      "step": 1293
    },
    {
      "epoch": 1.8354609929078014,
      "grad_norm": 1.2157318592071533,
      "learning_rate": 7.961259079903148e-05,
      "loss": 1.6616,
      "step": 1294
    },
    {
      "epoch": 1.8368794326241136,
      "grad_norm": 1.2132909297943115,
      "learning_rate": 7.951573849878934e-05,
      "loss": 1.5762,
      "step": 1295
    },
    {
      "epoch": 1.8382978723404255,
      "grad_norm": 1.2333414554595947,
      "learning_rate": 7.941888619854722e-05,
      "loss": 1.6694,
      "step": 1296
    },
    {
      "epoch": 1.8397163120567375,
      "grad_norm": 1.2984707355499268,
      "learning_rate": 7.93220338983051e-05,
      "loss": 1.5381,
      "step": 1297
    },
    {
      "epoch": 1.8411347517730496,
      "grad_norm": 1.2250295877456665,
      "learning_rate": 7.922518159806296e-05,
      "loss": 1.4639,
      "step": 1298
    },
    {
      "epoch": 1.8425531914893618,
      "grad_norm": 1.2242701053619385,
      "learning_rate": 7.912832929782083e-05,
      "loss": 1.4747,
      "step": 1299
    },
    {
      "epoch": 1.8439716312056738,
      "grad_norm": 1.3211188316345215,
      "learning_rate": 7.90314769975787e-05,
      "loss": 1.6465,
      "step": 1300
    },
    {
      "epoch": 1.8453900709219857,
      "grad_norm": 1.2320055961608887,
      "learning_rate": 7.893462469733657e-05,
      "loss": 1.6276,
      "step": 1301
    },
    {
      "epoch": 1.8468085106382979,
      "grad_norm": 1.119112491607666,
      "learning_rate": 7.883777239709444e-05,
      "loss": 1.4598,
      "step": 1302
    },
    {
      "epoch": 1.84822695035461,
      "grad_norm": 1.2619479894638062,
      "learning_rate": 7.87409200968523e-05,
      "loss": 1.6935,
      "step": 1303
    },
    {
      "epoch": 1.849645390070922,
      "grad_norm": 1.2406182289123535,
      "learning_rate": 7.864406779661018e-05,
      "loss": 1.5736,
      "step": 1304
    },
    {
      "epoch": 1.851063829787234,
      "grad_norm": 1.238468050956726,
      "learning_rate": 7.854721549636804e-05,
      "loss": 1.6569,
      "step": 1305
    },
    {
      "epoch": 1.852482269503546,
      "grad_norm": 1.2643851041793823,
      "learning_rate": 7.845036319612591e-05,
      "loss": 1.7913,
      "step": 1306
    },
    {
      "epoch": 1.8539007092198583,
      "grad_norm": 1.241404414176941,
      "learning_rate": 7.835351089588379e-05,
      "loss": 1.5837,
      "step": 1307
    },
    {
      "epoch": 1.8553191489361702,
      "grad_norm": 1.2201935052871704,
      "learning_rate": 7.825665859564165e-05,
      "loss": 1.7265,
      "step": 1308
    },
    {
      "epoch": 1.8567375886524822,
      "grad_norm": 1.2842940092086792,
      "learning_rate": 7.815980629539951e-05,
      "loss": 1.6138,
      "step": 1309
    },
    {
      "epoch": 1.8581560283687943,
      "grad_norm": 1.2985299825668335,
      "learning_rate": 7.806295399515738e-05,
      "loss": 1.8438,
      "step": 1310
    },
    {
      "epoch": 1.8595744680851065,
      "grad_norm": 1.3142226934432983,
      "learning_rate": 7.796610169491526e-05,
      "loss": 2.0371,
      "step": 1311
    },
    {
      "epoch": 1.8609929078014185,
      "grad_norm": 1.3236888647079468,
      "learning_rate": 7.786924939467312e-05,
      "loss": 1.6101,
      "step": 1312
    },
    {
      "epoch": 1.8624113475177304,
      "grad_norm": 1.1507089138031006,
      "learning_rate": 7.7772397094431e-05,
      "loss": 1.5661,
      "step": 1313
    },
    {
      "epoch": 1.8638297872340426,
      "grad_norm": 1.2894216775894165,
      "learning_rate": 7.767554479418886e-05,
      "loss": 1.8013,
      "step": 1314
    },
    {
      "epoch": 1.8652482269503547,
      "grad_norm": 1.1670016050338745,
      "learning_rate": 7.757869249394674e-05,
      "loss": 1.4594,
      "step": 1315
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 1.2988665103912354,
      "learning_rate": 7.748184019370461e-05,
      "loss": 1.5389,
      "step": 1316
    },
    {
      "epoch": 1.8680851063829786,
      "grad_norm": 1.2557634115219116,
      "learning_rate": 7.738498789346247e-05,
      "loss": 1.6916,
      "step": 1317
    },
    {
      "epoch": 1.8695035460992908,
      "grad_norm": 1.1916543245315552,
      "learning_rate": 7.728813559322035e-05,
      "loss": 1.4336,
      "step": 1318
    },
    {
      "epoch": 1.870921985815603,
      "grad_norm": 1.1936824321746826,
      "learning_rate": 7.719128329297821e-05,
      "loss": 1.881,
      "step": 1319
    },
    {
      "epoch": 1.872340425531915,
      "grad_norm": 1.1796000003814697,
      "learning_rate": 7.709443099273608e-05,
      "loss": 1.4908,
      "step": 1320
    },
    {
      "epoch": 1.8737588652482269,
      "grad_norm": 1.213279366493225,
      "learning_rate": 7.699757869249396e-05,
      "loss": 1.6762,
      "step": 1321
    },
    {
      "epoch": 1.875177304964539,
      "grad_norm": 1.1668477058410645,
      "learning_rate": 7.690072639225182e-05,
      "loss": 1.5443,
      "step": 1322
    },
    {
      "epoch": 1.8765957446808512,
      "grad_norm": 1.2475833892822266,
      "learning_rate": 7.680387409200969e-05,
      "loss": 1.4871,
      "step": 1323
    },
    {
      "epoch": 1.8780141843971632,
      "grad_norm": 1.1453133821487427,
      "learning_rate": 7.670702179176755e-05,
      "loss": 1.362,
      "step": 1324
    },
    {
      "epoch": 1.8794326241134751,
      "grad_norm": 1.129152536392212,
      "learning_rate": 7.661016949152543e-05,
      "loss": 1.3684,
      "step": 1325
    },
    {
      "epoch": 1.8808510638297873,
      "grad_norm": 1.1625628471374512,
      "learning_rate": 7.65133171912833e-05,
      "loss": 1.5104,
      "step": 1326
    },
    {
      "epoch": 1.8822695035460995,
      "grad_norm": 1.216731309890747,
      "learning_rate": 7.641646489104116e-05,
      "loss": 1.5418,
      "step": 1327
    },
    {
      "epoch": 1.8836879432624114,
      "grad_norm": 1.346356749534607,
      "learning_rate": 7.631961259079904e-05,
      "loss": 1.6267,
      "step": 1328
    },
    {
      "epoch": 1.8851063829787233,
      "grad_norm": 1.2288217544555664,
      "learning_rate": 7.62227602905569e-05,
      "loss": 1.4011,
      "step": 1329
    },
    {
      "epoch": 1.8865248226950353,
      "grad_norm": 1.2082568407058716,
      "learning_rate": 7.612590799031477e-05,
      "loss": 1.668,
      "step": 1330
    },
    {
      "epoch": 1.8879432624113475,
      "grad_norm": 1.3458770513534546,
      "learning_rate": 7.602905569007264e-05,
      "loss": 1.8011,
      "step": 1331
    },
    {
      "epoch": 1.8893617021276596,
      "grad_norm": 1.266349196434021,
      "learning_rate": 7.593220338983051e-05,
      "loss": 1.6949,
      "step": 1332
    },
    {
      "epoch": 1.8907801418439716,
      "grad_norm": 1.2198905944824219,
      "learning_rate": 7.583535108958839e-05,
      "loss": 1.4629,
      "step": 1333
    },
    {
      "epoch": 1.8921985815602835,
      "grad_norm": 1.2172703742980957,
      "learning_rate": 7.573849878934625e-05,
      "loss": 1.552,
      "step": 1334
    },
    {
      "epoch": 1.8936170212765957,
      "grad_norm": 1.2311712503433228,
      "learning_rate": 7.564164648910413e-05,
      "loss": 1.6053,
      "step": 1335
    },
    {
      "epoch": 1.8950354609929079,
      "grad_norm": 1.2357101440429688,
      "learning_rate": 7.5544794188862e-05,
      "loss": 1.494,
      "step": 1336
    },
    {
      "epoch": 1.8964539007092198,
      "grad_norm": 1.191764235496521,
      "learning_rate": 7.544794188861986e-05,
      "loss": 1.5214,
      "step": 1337
    },
    {
      "epoch": 1.8978723404255318,
      "grad_norm": 1.2426259517669678,
      "learning_rate": 7.535108958837774e-05,
      "loss": 1.5609,
      "step": 1338
    },
    {
      "epoch": 1.899290780141844,
      "grad_norm": 1.3303778171539307,
      "learning_rate": 7.52542372881356e-05,
      "loss": 1.7935,
      "step": 1339
    },
    {
      "epoch": 1.900709219858156,
      "grad_norm": 1.3144311904907227,
      "learning_rate": 7.515738498789347e-05,
      "loss": 1.6135,
      "step": 1340
    },
    {
      "epoch": 1.902127659574468,
      "grad_norm": 1.3513447046279907,
      "learning_rate": 7.506053268765133e-05,
      "loss": 1.8211,
      "step": 1341
    },
    {
      "epoch": 1.90354609929078,
      "grad_norm": 1.1804039478302002,
      "learning_rate": 7.496368038740921e-05,
      "loss": 1.4765,
      "step": 1342
    },
    {
      "epoch": 1.9049645390070922,
      "grad_norm": 1.3103199005126953,
      "learning_rate": 7.486682808716707e-05,
      "loss": 1.5439,
      "step": 1343
    },
    {
      "epoch": 1.9063829787234043,
      "grad_norm": 1.2102347612380981,
      "learning_rate": 7.476997578692494e-05,
      "loss": 1.5423,
      "step": 1344
    },
    {
      "epoch": 1.9078014184397163,
      "grad_norm": 1.2309465408325195,
      "learning_rate": 7.467312348668282e-05,
      "loss": 1.5484,
      "step": 1345
    },
    {
      "epoch": 1.9092198581560282,
      "grad_norm": 1.3033835887908936,
      "learning_rate": 7.457627118644068e-05,
      "loss": 1.7094,
      "step": 1346
    },
    {
      "epoch": 1.9106382978723404,
      "grad_norm": 1.3636541366577148,
      "learning_rate": 7.447941888619854e-05,
      "loss": 1.7271,
      "step": 1347
    },
    {
      "epoch": 1.9120567375886526,
      "grad_norm": 1.1765104532241821,
      "learning_rate": 7.438256658595641e-05,
      "loss": 1.4997,
      "step": 1348
    },
    {
      "epoch": 1.9134751773049645,
      "grad_norm": 1.2661726474761963,
      "learning_rate": 7.428571428571429e-05,
      "loss": 1.7299,
      "step": 1349
    },
    {
      "epoch": 1.9148936170212765,
      "grad_norm": 1.207046627998352,
      "learning_rate": 7.418886198547215e-05,
      "loss": 1.584,
      "step": 1350
    },
    {
      "epoch": 1.9163120567375886,
      "grad_norm": 1.294163703918457,
      "learning_rate": 7.409200968523003e-05,
      "loss": 1.8733,
      "step": 1351
    },
    {
      "epoch": 1.9177304964539008,
      "grad_norm": 1.1760830879211426,
      "learning_rate": 7.39951573849879e-05,
      "loss": 1.5796,
      "step": 1352
    },
    {
      "epoch": 1.9191489361702128,
      "grad_norm": 1.280287742614746,
      "learning_rate": 7.389830508474577e-05,
      "loss": 1.6627,
      "step": 1353
    },
    {
      "epoch": 1.9205673758865247,
      "grad_norm": 1.2621490955352783,
      "learning_rate": 7.380145278450364e-05,
      "loss": 1.4893,
      "step": 1354
    },
    {
      "epoch": 1.9219858156028369,
      "grad_norm": 1.3137407302856445,
      "learning_rate": 7.370460048426152e-05,
      "loss": 1.675,
      "step": 1355
    },
    {
      "epoch": 1.923404255319149,
      "grad_norm": 1.1772040128707886,
      "learning_rate": 7.360774818401938e-05,
      "loss": 1.5731,
      "step": 1356
    },
    {
      "epoch": 1.924822695035461,
      "grad_norm": 1.194597840309143,
      "learning_rate": 7.351089588377724e-05,
      "loss": 1.4866,
      "step": 1357
    },
    {
      "epoch": 1.926241134751773,
      "grad_norm": 1.206408143043518,
      "learning_rate": 7.341404358353511e-05,
      "loss": 1.5731,
      "step": 1358
    },
    {
      "epoch": 1.9276595744680851,
      "grad_norm": 1.1753695011138916,
      "learning_rate": 7.331719128329299e-05,
      "loss": 1.4957,
      "step": 1359
    },
    {
      "epoch": 1.9290780141843973,
      "grad_norm": 1.2009775638580322,
      "learning_rate": 7.322033898305085e-05,
      "loss": 1.5467,
      "step": 1360
    },
    {
      "epoch": 1.9304964539007092,
      "grad_norm": 1.2095006704330444,
      "learning_rate": 7.312348668280872e-05,
      "loss": 1.5186,
      "step": 1361
    },
    {
      "epoch": 1.9319148936170212,
      "grad_norm": 1.2269947528839111,
      "learning_rate": 7.30266343825666e-05,
      "loss": 1.4933,
      "step": 1362
    },
    {
      "epoch": 1.9333333333333333,
      "grad_norm": 1.298048973083496,
      "learning_rate": 7.292978208232446e-05,
      "loss": 1.7354,
      "step": 1363
    },
    {
      "epoch": 1.9347517730496455,
      "grad_norm": 1.2701084613800049,
      "learning_rate": 7.283292978208232e-05,
      "loss": 1.6798,
      "step": 1364
    },
    {
      "epoch": 1.9361702127659575,
      "grad_norm": 1.1934467554092407,
      "learning_rate": 7.273607748184019e-05,
      "loss": 1.5085,
      "step": 1365
    },
    {
      "epoch": 1.9375886524822694,
      "grad_norm": 1.2437546253204346,
      "learning_rate": 7.263922518159807e-05,
      "loss": 1.6957,
      "step": 1366
    },
    {
      "epoch": 1.9390070921985816,
      "grad_norm": 1.2427289485931396,
      "learning_rate": 7.254237288135593e-05,
      "loss": 1.6039,
      "step": 1367
    },
    {
      "epoch": 1.9404255319148938,
      "grad_norm": 1.1987226009368896,
      "learning_rate": 7.24455205811138e-05,
      "loss": 1.5358,
      "step": 1368
    },
    {
      "epoch": 1.9418439716312057,
      "grad_norm": 1.2729698419570923,
      "learning_rate": 7.234866828087167e-05,
      "loss": 1.4927,
      "step": 1369
    },
    {
      "epoch": 1.9432624113475176,
      "grad_norm": 1.1945502758026123,
      "learning_rate": 7.225181598062955e-05,
      "loss": 1.5929,
      "step": 1370
    },
    {
      "epoch": 1.9446808510638298,
      "grad_norm": 1.2309025526046753,
      "learning_rate": 7.215496368038742e-05,
      "loss": 1.6878,
      "step": 1371
    },
    {
      "epoch": 1.946099290780142,
      "grad_norm": 1.3083933591842651,
      "learning_rate": 7.205811138014528e-05,
      "loss": 1.6174,
      "step": 1372
    },
    {
      "epoch": 1.947517730496454,
      "grad_norm": 1.210256814956665,
      "learning_rate": 7.196125907990316e-05,
      "loss": 1.4798,
      "step": 1373
    },
    {
      "epoch": 1.9489361702127659,
      "grad_norm": 1.146670937538147,
      "learning_rate": 7.186440677966102e-05,
      "loss": 1.5709,
      "step": 1374
    },
    {
      "epoch": 1.950354609929078,
      "grad_norm": 1.2197978496551514,
      "learning_rate": 7.176755447941889e-05,
      "loss": 1.5567,
      "step": 1375
    },
    {
      "epoch": 1.9517730496453902,
      "grad_norm": 1.2731398344039917,
      "learning_rate": 7.167070217917677e-05,
      "loss": 1.6028,
      "step": 1376
    },
    {
      "epoch": 1.9531914893617022,
      "grad_norm": 1.2282391786575317,
      "learning_rate": 7.157384987893463e-05,
      "loss": 1.6231,
      "step": 1377
    },
    {
      "epoch": 1.9546099290780141,
      "grad_norm": 1.2408250570297241,
      "learning_rate": 7.14769975786925e-05,
      "loss": 1.5026,
      "step": 1378
    },
    {
      "epoch": 1.9560283687943263,
      "grad_norm": 1.1595221757888794,
      "learning_rate": 7.138014527845037e-05,
      "loss": 1.6704,
      "step": 1379
    },
    {
      "epoch": 1.9574468085106385,
      "grad_norm": 1.2995257377624512,
      "learning_rate": 7.128329297820824e-05,
      "loss": 1.769,
      "step": 1380
    },
    {
      "epoch": 1.9588652482269504,
      "grad_norm": 1.3435873985290527,
      "learning_rate": 7.11864406779661e-05,
      "loss": 1.5395,
      "step": 1381
    },
    {
      "epoch": 1.9602836879432624,
      "grad_norm": 1.2043524980545044,
      "learning_rate": 7.108958837772397e-05,
      "loss": 1.7333,
      "step": 1382
    },
    {
      "epoch": 1.9617021276595743,
      "grad_norm": 1.2391901016235352,
      "learning_rate": 7.099273607748184e-05,
      "loss": 1.8218,
      "step": 1383
    },
    {
      "epoch": 1.9631205673758865,
      "grad_norm": 1.1702243089675903,
      "learning_rate": 7.089588377723971e-05,
      "loss": 1.4867,
      "step": 1384
    },
    {
      "epoch": 1.9645390070921986,
      "grad_norm": 1.2263753414154053,
      "learning_rate": 7.079903147699757e-05,
      "loss": 1.6124,
      "step": 1385
    },
    {
      "epoch": 1.9659574468085106,
      "grad_norm": 1.2018554210662842,
      "learning_rate": 7.070217917675545e-05,
      "loss": 1.5873,
      "step": 1386
    },
    {
      "epoch": 1.9673758865248225,
      "grad_norm": 1.2432951927185059,
      "learning_rate": 7.060532687651332e-05,
      "loss": 1.5537,
      "step": 1387
    },
    {
      "epoch": 1.9687943262411347,
      "grad_norm": 1.1637085676193237,
      "learning_rate": 7.05084745762712e-05,
      "loss": 1.5598,
      "step": 1388
    },
    {
      "epoch": 1.9702127659574469,
      "grad_norm": 1.2289663553237915,
      "learning_rate": 7.041162227602906e-05,
      "loss": 1.572,
      "step": 1389
    },
    {
      "epoch": 1.9716312056737588,
      "grad_norm": 1.1686475276947021,
      "learning_rate": 7.031476997578694e-05,
      "loss": 1.4634,
      "step": 1390
    },
    {
      "epoch": 1.9730496453900708,
      "grad_norm": 1.2483896017074585,
      "learning_rate": 7.02179176755448e-05,
      "loss": 1.633,
      "step": 1391
    },
    {
      "epoch": 1.974468085106383,
      "grad_norm": 1.1586350202560425,
      "learning_rate": 7.012106537530267e-05,
      "loss": 1.659,
      "step": 1392
    },
    {
      "epoch": 1.9758865248226951,
      "grad_norm": 1.2889074087142944,
      "learning_rate": 7.002421307506054e-05,
      "loss": 1.575,
      "step": 1393
    },
    {
      "epoch": 1.977304964539007,
      "grad_norm": 1.2776657342910767,
      "learning_rate": 6.992736077481841e-05,
      "loss": 1.6583,
      "step": 1394
    },
    {
      "epoch": 1.978723404255319,
      "grad_norm": 1.197878122329712,
      "learning_rate": 6.983050847457627e-05,
      "loss": 1.5154,
      "step": 1395
    },
    {
      "epoch": 1.9801418439716312,
      "grad_norm": 1.2469369173049927,
      "learning_rate": 6.973365617433414e-05,
      "loss": 1.5438,
      "step": 1396
    },
    {
      "epoch": 1.9815602836879433,
      "grad_norm": 1.25271737575531,
      "learning_rate": 6.963680387409202e-05,
      "loss": 1.5245,
      "step": 1397
    },
    {
      "epoch": 1.9829787234042553,
      "grad_norm": 1.2716610431671143,
      "learning_rate": 6.953995157384988e-05,
      "loss": 1.6391,
      "step": 1398
    },
    {
      "epoch": 1.9843971631205672,
      "grad_norm": 1.2030324935913086,
      "learning_rate": 6.944309927360775e-05,
      "loss": 1.5376,
      "step": 1399
    },
    {
      "epoch": 1.9858156028368794,
      "grad_norm": 1.3654292821884155,
      "learning_rate": 6.934624697336562e-05,
      "loss": 1.7681,
      "step": 1400
    },
    {
      "epoch": 1.9872340425531916,
      "grad_norm": 1.269286036491394,
      "learning_rate": 6.924939467312349e-05,
      "loss": 1.5787,
      "step": 1401
    },
    {
      "epoch": 1.9886524822695035,
      "grad_norm": 1.2320507764816284,
      "learning_rate": 6.915254237288135e-05,
      "loss": 1.5153,
      "step": 1402
    },
    {
      "epoch": 1.9900709219858155,
      "grad_norm": 1.3980575799942017,
      "learning_rate": 6.905569007263922e-05,
      "loss": 1.6295,
      "step": 1403
    },
    {
      "epoch": 1.9914893617021276,
      "grad_norm": 1.298835039138794,
      "learning_rate": 6.89588377723971e-05,
      "loss": 1.7556,
      "step": 1404
    },
    {
      "epoch": 1.9929078014184398,
      "grad_norm": 1.3351497650146484,
      "learning_rate": 6.886198547215496e-05,
      "loss": 1.8929,
      "step": 1405
    },
    {
      "epoch": 1.9943262411347518,
      "grad_norm": 1.293969750404358,
      "learning_rate": 6.876513317191284e-05,
      "loss": 1.5618,
      "step": 1406
    },
    {
      "epoch": 1.9957446808510637,
      "grad_norm": 1.2980881929397583,
      "learning_rate": 6.86682808716707e-05,
      "loss": 1.5668,
      "step": 1407
    },
    {
      "epoch": 1.9971631205673759,
      "grad_norm": 1.2483323812484741,
      "learning_rate": 6.857142857142858e-05,
      "loss": 1.619,
      "step": 1408
    },
    {
      "epoch": 1.998581560283688,
      "grad_norm": 1.212083339691162,
      "learning_rate": 6.847457627118645e-05,
      "loss": 1.5741,
      "step": 1409
    },
    {
      "epoch": 2.0,
      "grad_norm": 1.265015959739685,
      "learning_rate": 6.837772397094432e-05,
      "loss": 1.2055,
      "step": 1410
    },
    {
      "epoch": 2.001418439716312,
      "grad_norm": 1.1439275741577148,
      "learning_rate": 6.828087167070219e-05,
      "loss": 1.4802,
      "step": 1411
    },
    {
      "epoch": 2.002836879432624,
      "grad_norm": 1.1396907567977905,
      "learning_rate": 6.818401937046005e-05,
      "loss": 1.4525,
      "step": 1412
    },
    {
      "epoch": 2.0042553191489363,
      "grad_norm": 1.1547789573669434,
      "learning_rate": 6.808716707021792e-05,
      "loss": 1.3494,
      "step": 1413
    },
    {
      "epoch": 2.0056737588652482,
      "grad_norm": 1.1742703914642334,
      "learning_rate": 6.79903147699758e-05,
      "loss": 1.4265,
      "step": 1414
    },
    {
      "epoch": 2.00709219858156,
      "grad_norm": 1.190516471862793,
      "learning_rate": 6.789346246973366e-05,
      "loss": 1.3917,
      "step": 1415
    },
    {
      "epoch": 2.008510638297872,
      "grad_norm": 1.1448167562484741,
      "learning_rate": 6.779661016949152e-05,
      "loss": 1.3247,
      "step": 1416
    },
    {
      "epoch": 2.0099290780141845,
      "grad_norm": 1.167853593826294,
      "learning_rate": 6.76997578692494e-05,
      "loss": 1.4765,
      "step": 1417
    },
    {
      "epoch": 2.0113475177304965,
      "grad_norm": 1.173433542251587,
      "learning_rate": 6.760290556900727e-05,
      "loss": 1.3309,
      "step": 1418
    },
    {
      "epoch": 2.0127659574468084,
      "grad_norm": 1.1640278100967407,
      "learning_rate": 6.750605326876513e-05,
      "loss": 1.5153,
      "step": 1419
    },
    {
      "epoch": 2.0141843971631204,
      "grad_norm": 1.221225619316101,
      "learning_rate": 6.7409200968523e-05,
      "loss": 1.5815,
      "step": 1420
    },
    {
      "epoch": 2.0156028368794328,
      "grad_norm": 1.2106242179870605,
      "learning_rate": 6.731234866828087e-05,
      "loss": 1.2771,
      "step": 1421
    },
    {
      "epoch": 2.0170212765957447,
      "grad_norm": 1.2301911115646362,
      "learning_rate": 6.721549636803874e-05,
      "loss": 1.5494,
      "step": 1422
    },
    {
      "epoch": 2.0184397163120567,
      "grad_norm": 1.2126729488372803,
      "learning_rate": 6.71186440677966e-05,
      "loss": 1.2966,
      "step": 1423
    },
    {
      "epoch": 2.0198581560283686,
      "grad_norm": 1.184798002243042,
      "learning_rate": 6.702179176755448e-05,
      "loss": 1.3132,
      "step": 1424
    },
    {
      "epoch": 2.021276595744681,
      "grad_norm": 1.2192165851593018,
      "learning_rate": 6.692493946731236e-05,
      "loss": 1.2321,
      "step": 1425
    },
    {
      "epoch": 2.022695035460993,
      "grad_norm": 1.1889797449111938,
      "learning_rate": 6.682808716707022e-05,
      "loss": 1.4338,
      "step": 1426
    },
    {
      "epoch": 2.024113475177305,
      "grad_norm": 1.342188835144043,
      "learning_rate": 6.673123486682809e-05,
      "loss": 1.5538,
      "step": 1427
    },
    {
      "epoch": 2.025531914893617,
      "grad_norm": 1.2531723976135254,
      "learning_rate": 6.663438256658597e-05,
      "loss": 1.4797,
      "step": 1428
    },
    {
      "epoch": 2.0269503546099292,
      "grad_norm": 1.4885846376419067,
      "learning_rate": 6.653753026634383e-05,
      "loss": 1.5876,
      "step": 1429
    },
    {
      "epoch": 2.028368794326241,
      "grad_norm": 1.3212724924087524,
      "learning_rate": 6.64406779661017e-05,
      "loss": 1.3891,
      "step": 1430
    },
    {
      "epoch": 2.029787234042553,
      "grad_norm": 1.3022278547286987,
      "learning_rate": 6.634382566585957e-05,
      "loss": 1.3512,
      "step": 1431
    },
    {
      "epoch": 2.031205673758865,
      "grad_norm": 1.3507497310638428,
      "learning_rate": 6.624697336561744e-05,
      "loss": 1.6501,
      "step": 1432
    },
    {
      "epoch": 2.0326241134751775,
      "grad_norm": 1.2418757677078247,
      "learning_rate": 6.61501210653753e-05,
      "loss": 1.3276,
      "step": 1433
    },
    {
      "epoch": 2.0340425531914894,
      "grad_norm": 1.344681739807129,
      "learning_rate": 6.605326876513318e-05,
      "loss": 1.4727,
      "step": 1434
    },
    {
      "epoch": 2.0354609929078014,
      "grad_norm": 1.1787645816802979,
      "learning_rate": 6.595641646489105e-05,
      "loss": 1.2421,
      "step": 1435
    },
    {
      "epoch": 2.0368794326241133,
      "grad_norm": 1.266188621520996,
      "learning_rate": 6.585956416464891e-05,
      "loss": 1.47,
      "step": 1436
    },
    {
      "epoch": 2.0382978723404257,
      "grad_norm": 1.346077799797058,
      "learning_rate": 6.576271186440678e-05,
      "loss": 1.4823,
      "step": 1437
    },
    {
      "epoch": 2.0397163120567376,
      "grad_norm": 1.3603013753890991,
      "learning_rate": 6.566585956416465e-05,
      "loss": 1.451,
      "step": 1438
    },
    {
      "epoch": 2.0411347517730496,
      "grad_norm": 1.308114767074585,
      "learning_rate": 6.556900726392252e-05,
      "loss": 1.5193,
      "step": 1439
    },
    {
      "epoch": 2.0425531914893615,
      "grad_norm": 1.2586885690689087,
      "learning_rate": 6.547215496368038e-05,
      "loss": 1.4684,
      "step": 1440
    },
    {
      "epoch": 2.043971631205674,
      "grad_norm": 1.1956117153167725,
      "learning_rate": 6.537530266343826e-05,
      "loss": 1.331,
      "step": 1441
    },
    {
      "epoch": 2.045390070921986,
      "grad_norm": 1.4297353029251099,
      "learning_rate": 6.527845036319613e-05,
      "loss": 1.5005,
      "step": 1442
    },
    {
      "epoch": 2.046808510638298,
      "grad_norm": 1.1872315406799316,
      "learning_rate": 6.5181598062954e-05,
      "loss": 1.5107,
      "step": 1443
    },
    {
      "epoch": 2.0482269503546098,
      "grad_norm": 1.444273829460144,
      "learning_rate": 6.508474576271187e-05,
      "loss": 1.4878,
      "step": 1444
    },
    {
      "epoch": 2.049645390070922,
      "grad_norm": 1.308821439743042,
      "learning_rate": 6.498789346246975e-05,
      "loss": 1.5197,
      "step": 1445
    },
    {
      "epoch": 2.051063829787234,
      "grad_norm": 1.3901115655899048,
      "learning_rate": 6.489104116222761e-05,
      "loss": 1.5767,
      "step": 1446
    },
    {
      "epoch": 2.052482269503546,
      "grad_norm": 1.3597790002822876,
      "learning_rate": 6.479418886198548e-05,
      "loss": 1.2807,
      "step": 1447
    },
    {
      "epoch": 2.053900709219858,
      "grad_norm": 1.295803427696228,
      "learning_rate": 6.469733656174335e-05,
      "loss": 1.4639,
      "step": 1448
    },
    {
      "epoch": 2.0553191489361704,
      "grad_norm": 1.2705892324447632,
      "learning_rate": 6.460048426150122e-05,
      "loss": 1.255,
      "step": 1449
    },
    {
      "epoch": 2.0567375886524824,
      "grad_norm": 1.575714111328125,
      "learning_rate": 6.450363196125908e-05,
      "loss": 1.3334,
      "step": 1450
    },
    {
      "epoch": 2.0581560283687943,
      "grad_norm": 1.4018473625183105,
      "learning_rate": 6.440677966101695e-05,
      "loss": 1.4676,
      "step": 1451
    },
    {
      "epoch": 2.0595744680851062,
      "grad_norm": 1.3074480295181274,
      "learning_rate": 6.430992736077483e-05,
      "loss": 1.4701,
      "step": 1452
    },
    {
      "epoch": 2.0609929078014186,
      "grad_norm": 1.316563367843628,
      "learning_rate": 6.421307506053269e-05,
      "loss": 1.4346,
      "step": 1453
    },
    {
      "epoch": 2.0624113475177306,
      "grad_norm": 1.214671015739441,
      "learning_rate": 6.411622276029055e-05,
      "loss": 1.317,
      "step": 1454
    },
    {
      "epoch": 2.0638297872340425,
      "grad_norm": 1.4145416021347046,
      "learning_rate": 6.401937046004843e-05,
      "loss": 1.5771,
      "step": 1455
    },
    {
      "epoch": 2.0652482269503545,
      "grad_norm": 1.3816382884979248,
      "learning_rate": 6.39225181598063e-05,
      "loss": 1.4611,
      "step": 1456
    },
    {
      "epoch": 2.066666666666667,
      "grad_norm": 1.351570963859558,
      "learning_rate": 6.382566585956416e-05,
      "loss": 1.3844,
      "step": 1457
    },
    {
      "epoch": 2.068085106382979,
      "grad_norm": 1.408668041229248,
      "learning_rate": 6.372881355932203e-05,
      "loss": 1.6017,
      "step": 1458
    },
    {
      "epoch": 2.0695035460992908,
      "grad_norm": 1.3895480632781982,
      "learning_rate": 6.36319612590799e-05,
      "loss": 1.5615,
      "step": 1459
    },
    {
      "epoch": 2.0709219858156027,
      "grad_norm": 1.4574886560440063,
      "learning_rate": 6.353510895883777e-05,
      "loss": 1.4858,
      "step": 1460
    },
    {
      "epoch": 2.072340425531915,
      "grad_norm": 1.3680154085159302,
      "learning_rate": 6.343825665859565e-05,
      "loss": 1.3852,
      "step": 1461
    },
    {
      "epoch": 2.073758865248227,
      "grad_norm": 1.4915521144866943,
      "learning_rate": 6.334140435835351e-05,
      "loss": 1.3603,
      "step": 1462
    },
    {
      "epoch": 2.075177304964539,
      "grad_norm": 1.4056795835494995,
      "learning_rate": 6.324455205811139e-05,
      "loss": 1.3754,
      "step": 1463
    },
    {
      "epoch": 2.076595744680851,
      "grad_norm": 1.3545506000518799,
      "learning_rate": 6.314769975786925e-05,
      "loss": 1.3925,
      "step": 1464
    },
    {
      "epoch": 2.078014184397163,
      "grad_norm": 1.3451842069625854,
      "learning_rate": 6.305084745762713e-05,
      "loss": 1.4745,
      "step": 1465
    },
    {
      "epoch": 2.0794326241134753,
      "grad_norm": 1.383893609046936,
      "learning_rate": 6.2953995157385e-05,
      "loss": 1.4629,
      "step": 1466
    },
    {
      "epoch": 2.0808510638297872,
      "grad_norm": 1.3028916120529175,
      "learning_rate": 6.285714285714286e-05,
      "loss": 1.3288,
      "step": 1467
    },
    {
      "epoch": 2.082269503546099,
      "grad_norm": 1.2646969556808472,
      "learning_rate": 6.276029055690073e-05,
      "loss": 1.38,
      "step": 1468
    },
    {
      "epoch": 2.083687943262411,
      "grad_norm": 1.3031203746795654,
      "learning_rate": 6.26634382566586e-05,
      "loss": 1.2728,
      "step": 1469
    },
    {
      "epoch": 2.0851063829787235,
      "grad_norm": 1.3582552671432495,
      "learning_rate": 6.256658595641647e-05,
      "loss": 1.3998,
      "step": 1470
    },
    {
      "epoch": 2.0865248226950355,
      "grad_norm": 1.288375735282898,
      "learning_rate": 6.246973365617433e-05,
      "loss": 1.4384,
      "step": 1471
    },
    {
      "epoch": 2.0879432624113474,
      "grad_norm": 1.4968421459197998,
      "learning_rate": 6.237288135593221e-05,
      "loss": 1.5398,
      "step": 1472
    },
    {
      "epoch": 2.0893617021276594,
      "grad_norm": 1.2889823913574219,
      "learning_rate": 6.227602905569008e-05,
      "loss": 1.4594,
      "step": 1473
    },
    {
      "epoch": 2.0907801418439718,
      "grad_norm": 1.3350849151611328,
      "learning_rate": 6.217917675544794e-05,
      "loss": 1.4376,
      "step": 1474
    },
    {
      "epoch": 2.0921985815602837,
      "grad_norm": 1.461351752281189,
      "learning_rate": 6.20823244552058e-05,
      "loss": 1.4215,
      "step": 1475
    },
    {
      "epoch": 2.0936170212765957,
      "grad_norm": 1.4056448936462402,
      "learning_rate": 6.198547215496368e-05,
      "loss": 1.5385,
      "step": 1476
    },
    {
      "epoch": 2.0950354609929076,
      "grad_norm": 1.4462265968322754,
      "learning_rate": 6.188861985472155e-05,
      "loss": 1.4599,
      "step": 1477
    },
    {
      "epoch": 2.09645390070922,
      "grad_norm": 1.367525577545166,
      "learning_rate": 6.179176755447941e-05,
      "loss": 1.2551,
      "step": 1478
    },
    {
      "epoch": 2.097872340425532,
      "grad_norm": 1.319663405418396,
      "learning_rate": 6.169491525423729e-05,
      "loss": 1.3623,
      "step": 1479
    },
    {
      "epoch": 2.099290780141844,
      "grad_norm": 1.3851237297058105,
      "learning_rate": 6.159806295399515e-05,
      "loss": 1.3506,
      "step": 1480
    },
    {
      "epoch": 2.100709219858156,
      "grad_norm": 1.4274437427520752,
      "learning_rate": 6.150121065375303e-05,
      "loss": 1.4817,
      "step": 1481
    },
    {
      "epoch": 2.1021276595744682,
      "grad_norm": 1.419600009918213,
      "learning_rate": 6.14043583535109e-05,
      "loss": 1.437,
      "step": 1482
    },
    {
      "epoch": 2.10354609929078,
      "grad_norm": 1.3293741941452026,
      "learning_rate": 6.130750605326878e-05,
      "loss": 1.4634,
      "step": 1483
    },
    {
      "epoch": 2.104964539007092,
      "grad_norm": 1.3659441471099854,
      "learning_rate": 6.121065375302664e-05,
      "loss": 1.2452,
      "step": 1484
    },
    {
      "epoch": 2.106382978723404,
      "grad_norm": 1.43529212474823,
      "learning_rate": 6.11138014527845e-05,
      "loss": 1.4916,
      "step": 1485
    },
    {
      "epoch": 2.1078014184397165,
      "grad_norm": 1.3810783624649048,
      "learning_rate": 6.101694915254238e-05,
      "loss": 1.3534,
      "step": 1486
    },
    {
      "epoch": 2.1092198581560284,
      "grad_norm": 1.4102414846420288,
      "learning_rate": 6.092009685230025e-05,
      "loss": 1.3403,
      "step": 1487
    },
    {
      "epoch": 2.1106382978723404,
      "grad_norm": 1.3714323043823242,
      "learning_rate": 6.082324455205811e-05,
      "loss": 1.3799,
      "step": 1488
    },
    {
      "epoch": 2.1120567375886523,
      "grad_norm": 1.466052770614624,
      "learning_rate": 6.072639225181599e-05,
      "loss": 1.4243,
      "step": 1489
    },
    {
      "epoch": 2.1134751773049647,
      "grad_norm": 1.354107141494751,
      "learning_rate": 6.0629539951573855e-05,
      "loss": 1.4104,
      "step": 1490
    },
    {
      "epoch": 2.1148936170212767,
      "grad_norm": 1.4758011102676392,
      "learning_rate": 6.053268765133172e-05,
      "loss": 1.3249,
      "step": 1491
    },
    {
      "epoch": 2.1163120567375886,
      "grad_norm": 1.4178739786148071,
      "learning_rate": 6.0435835351089584e-05,
      "loss": 1.5334,
      "step": 1492
    },
    {
      "epoch": 2.1177304964539005,
      "grad_norm": 1.3571573495864868,
      "learning_rate": 6.033898305084746e-05,
      "loss": 1.3791,
      "step": 1493
    },
    {
      "epoch": 2.119148936170213,
      "grad_norm": 1.301010012626648,
      "learning_rate": 6.024213075060533e-05,
      "loss": 1.4013,
      "step": 1494
    },
    {
      "epoch": 2.120567375886525,
      "grad_norm": 1.3668224811553955,
      "learning_rate": 6.01452784503632e-05,
      "loss": 1.3038,
      "step": 1495
    },
    {
      "epoch": 2.121985815602837,
      "grad_norm": 1.3615373373031616,
      "learning_rate": 6.0048426150121076e-05,
      "loss": 1.5828,
      "step": 1496
    },
    {
      "epoch": 2.123404255319149,
      "grad_norm": 1.4721412658691406,
      "learning_rate": 5.995157384987894e-05,
      "loss": 1.5653,
      "step": 1497
    },
    {
      "epoch": 2.124822695035461,
      "grad_norm": 1.3650556802749634,
      "learning_rate": 5.9854721549636805e-05,
      "loss": 1.3907,
      "step": 1498
    },
    {
      "epoch": 2.126241134751773,
      "grad_norm": 1.468342661857605,
      "learning_rate": 5.975786924939467e-05,
      "loss": 1.4172,
      "step": 1499
    },
    {
      "epoch": 2.127659574468085,
      "grad_norm": 1.3972855806350708,
      "learning_rate": 5.966101694915255e-05,
      "loss": 1.4,
      "step": 1500
    },
    {
      "epoch": 2.127659574468085,
      "eval_loss": 1.8102105855941772,
      "eval_runtime": 95.4784,
      "eval_samples_per_second": 14.768,
      "eval_steps_per_second": 7.384,
      "step": 1500
    },
    {
      "epoch": 2.129078014184397,
      "grad_norm": 1.3588851690292358,
      "learning_rate": 5.956416464891041e-05,
      "loss": 1.4707,
      "step": 1501
    },
    {
      "epoch": 2.1304964539007094,
      "grad_norm": 1.4343152046203613,
      "learning_rate": 5.9467312348668284e-05,
      "loss": 1.5815,
      "step": 1502
    },
    {
      "epoch": 2.1319148936170214,
      "grad_norm": 1.4252465963363647,
      "learning_rate": 5.9370460048426155e-05,
      "loss": 1.3227,
      "step": 1503
    },
    {
      "epoch": 2.1333333333333333,
      "grad_norm": 1.339411735534668,
      "learning_rate": 5.9273607748184026e-05,
      "loss": 1.3664,
      "step": 1504
    },
    {
      "epoch": 2.1347517730496453,
      "grad_norm": 1.5309778451919556,
      "learning_rate": 5.917675544794189e-05,
      "loss": 1.4412,
      "step": 1505
    },
    {
      "epoch": 2.1361702127659576,
      "grad_norm": 1.403559684753418,
      "learning_rate": 5.9079903147699756e-05,
      "loss": 1.4041,
      "step": 1506
    },
    {
      "epoch": 2.1375886524822696,
      "grad_norm": 1.3656716346740723,
      "learning_rate": 5.8983050847457634e-05,
      "loss": 1.4732,
      "step": 1507
    },
    {
      "epoch": 2.1390070921985815,
      "grad_norm": 1.3245346546173096,
      "learning_rate": 5.88861985472155e-05,
      "loss": 1.4017,
      "step": 1508
    },
    {
      "epoch": 2.1404255319148935,
      "grad_norm": 1.4083797931671143,
      "learning_rate": 5.878934624697336e-05,
      "loss": 1.31,
      "step": 1509
    },
    {
      "epoch": 2.141843971631206,
      "grad_norm": 1.460699200630188,
      "learning_rate": 5.869249394673124e-05,
      "loss": 1.3995,
      "step": 1510
    },
    {
      "epoch": 2.143262411347518,
      "grad_norm": 1.4471380710601807,
      "learning_rate": 5.8595641646489105e-05,
      "loss": 1.2957,
      "step": 1511
    },
    {
      "epoch": 2.1446808510638298,
      "grad_norm": 1.5048774480819702,
      "learning_rate": 5.849878934624698e-05,
      "loss": 1.4101,
      "step": 1512
    },
    {
      "epoch": 2.1460992907801417,
      "grad_norm": 1.4230422973632812,
      "learning_rate": 5.840193704600485e-05,
      "loss": 1.4422,
      "step": 1513
    },
    {
      "epoch": 2.147517730496454,
      "grad_norm": 1.4218825101852417,
      "learning_rate": 5.830508474576272e-05,
      "loss": 1.441,
      "step": 1514
    },
    {
      "epoch": 2.148936170212766,
      "grad_norm": 1.4351035356521606,
      "learning_rate": 5.8208232445520584e-05,
      "loss": 1.4542,
      "step": 1515
    },
    {
      "epoch": 2.150354609929078,
      "grad_norm": 1.3532663583755493,
      "learning_rate": 5.811138014527845e-05,
      "loss": 1.3301,
      "step": 1516
    },
    {
      "epoch": 2.15177304964539,
      "grad_norm": 1.5154633522033691,
      "learning_rate": 5.801452784503633e-05,
      "loss": 1.2798,
      "step": 1517
    },
    {
      "epoch": 2.153191489361702,
      "grad_norm": 1.2473000288009644,
      "learning_rate": 5.791767554479419e-05,
      "loss": 1.1353,
      "step": 1518
    },
    {
      "epoch": 2.1546099290780143,
      "grad_norm": 1.4737454652786255,
      "learning_rate": 5.7820823244552056e-05,
      "loss": 1.3556,
      "step": 1519
    },
    {
      "epoch": 2.1560283687943262,
      "grad_norm": 1.3257734775543213,
      "learning_rate": 5.7723970944309934e-05,
      "loss": 1.2503,
      "step": 1520
    },
    {
      "epoch": 2.157446808510638,
      "grad_norm": 1.471232533454895,
      "learning_rate": 5.76271186440678e-05,
      "loss": 1.6792,
      "step": 1521
    },
    {
      "epoch": 2.1588652482269506,
      "grad_norm": 1.341779351234436,
      "learning_rate": 5.753026634382567e-05,
      "loss": 1.3723,
      "step": 1522
    },
    {
      "epoch": 2.1602836879432625,
      "grad_norm": 1.4646774530410767,
      "learning_rate": 5.7433414043583534e-05,
      "loss": 1.4394,
      "step": 1523
    },
    {
      "epoch": 2.1617021276595745,
      "grad_norm": 1.488379716873169,
      "learning_rate": 5.733656174334141e-05,
      "loss": 1.3846,
      "step": 1524
    },
    {
      "epoch": 2.1631205673758864,
      "grad_norm": 1.4904536008834839,
      "learning_rate": 5.723970944309928e-05,
      "loss": 1.6833,
      "step": 1525
    },
    {
      "epoch": 2.1645390070921984,
      "grad_norm": 1.3733750581741333,
      "learning_rate": 5.714285714285714e-05,
      "loss": 1.4478,
      "step": 1526
    },
    {
      "epoch": 2.1659574468085108,
      "grad_norm": 1.4215924739837646,
      "learning_rate": 5.704600484261502e-05,
      "loss": 1.4702,
      "step": 1527
    },
    {
      "epoch": 2.1673758865248227,
      "grad_norm": 1.4669262170791626,
      "learning_rate": 5.6949152542372884e-05,
      "loss": 1.7007,
      "step": 1528
    },
    {
      "epoch": 2.1687943262411347,
      "grad_norm": 1.5076426267623901,
      "learning_rate": 5.685230024213075e-05,
      "loss": 1.5532,
      "step": 1529
    },
    {
      "epoch": 2.1702127659574466,
      "grad_norm": 1.4333219528198242,
      "learning_rate": 5.675544794188862e-05,
      "loss": 1.478,
      "step": 1530
    },
    {
      "epoch": 2.171631205673759,
      "grad_norm": 1.2726181745529175,
      "learning_rate": 5.665859564164649e-05,
      "loss": 1.2011,
      "step": 1531
    },
    {
      "epoch": 2.173049645390071,
      "grad_norm": 1.337852120399475,
      "learning_rate": 5.656174334140436e-05,
      "loss": 1.323,
      "step": 1532
    },
    {
      "epoch": 2.174468085106383,
      "grad_norm": 1.4280567169189453,
      "learning_rate": 5.646489104116223e-05,
      "loss": 1.3539,
      "step": 1533
    },
    {
      "epoch": 2.175886524822695,
      "grad_norm": 1.4032779932022095,
      "learning_rate": 5.6368038740920106e-05,
      "loss": 1.4104,
      "step": 1534
    },
    {
      "epoch": 2.1773049645390072,
      "grad_norm": 1.4599080085754395,
      "learning_rate": 5.627118644067797e-05,
      "loss": 1.3677,
      "step": 1535
    },
    {
      "epoch": 2.178723404255319,
      "grad_norm": 1.4429600238800049,
      "learning_rate": 5.6174334140435835e-05,
      "loss": 1.5444,
      "step": 1536
    },
    {
      "epoch": 2.180141843971631,
      "grad_norm": 1.4588769674301147,
      "learning_rate": 5.60774818401937e-05,
      "loss": 1.6845,
      "step": 1537
    },
    {
      "epoch": 2.181560283687943,
      "grad_norm": 1.4499218463897705,
      "learning_rate": 5.598062953995158e-05,
      "loss": 1.6056,
      "step": 1538
    },
    {
      "epoch": 2.1829787234042555,
      "grad_norm": 1.4217100143432617,
      "learning_rate": 5.588377723970944e-05,
      "loss": 1.5164,
      "step": 1539
    },
    {
      "epoch": 2.1843971631205674,
      "grad_norm": 1.7048784494400024,
      "learning_rate": 5.5786924939467313e-05,
      "loss": 1.6158,
      "step": 1540
    },
    {
      "epoch": 2.1858156028368794,
      "grad_norm": 1.4721574783325195,
      "learning_rate": 5.569007263922519e-05,
      "loss": 1.3184,
      "step": 1541
    },
    {
      "epoch": 2.1872340425531913,
      "grad_norm": 1.4719549417495728,
      "learning_rate": 5.5593220338983056e-05,
      "loss": 1.628,
      "step": 1542
    },
    {
      "epoch": 2.1886524822695037,
      "grad_norm": 1.3929582834243774,
      "learning_rate": 5.549636803874092e-05,
      "loss": 1.2032,
      "step": 1543
    },
    {
      "epoch": 2.1900709219858157,
      "grad_norm": 1.2670844793319702,
      "learning_rate": 5.53995157384988e-05,
      "loss": 1.2768,
      "step": 1544
    },
    {
      "epoch": 2.1914893617021276,
      "grad_norm": 1.4104726314544678,
      "learning_rate": 5.530266343825666e-05,
      "loss": 1.6576,
      "step": 1545
    },
    {
      "epoch": 2.1929078014184396,
      "grad_norm": 1.4296951293945312,
      "learning_rate": 5.520581113801453e-05,
      "loss": 1.3529,
      "step": 1546
    },
    {
      "epoch": 2.194326241134752,
      "grad_norm": 1.447723150253296,
      "learning_rate": 5.510895883777239e-05,
      "loss": 1.3478,
      "step": 1547
    },
    {
      "epoch": 2.195744680851064,
      "grad_norm": 1.3587093353271484,
      "learning_rate": 5.501210653753027e-05,
      "loss": 1.4533,
      "step": 1548
    },
    {
      "epoch": 2.197163120567376,
      "grad_norm": 1.3998538255691528,
      "learning_rate": 5.4915254237288135e-05,
      "loss": 1.1881,
      "step": 1549
    },
    {
      "epoch": 2.198581560283688,
      "grad_norm": 1.4579882621765137,
      "learning_rate": 5.4818401937046006e-05,
      "loss": 1.3974,
      "step": 1550
    },
    {
      "epoch": 2.2,
      "grad_norm": 1.5941901206970215,
      "learning_rate": 5.4721549636803885e-05,
      "loss": 1.2855,
      "step": 1551
    },
    {
      "epoch": 2.201418439716312,
      "grad_norm": 1.3714962005615234,
      "learning_rate": 5.462469733656175e-05,
      "loss": 1.4963,
      "step": 1552
    },
    {
      "epoch": 2.202836879432624,
      "grad_norm": 1.646890640258789,
      "learning_rate": 5.4527845036319614e-05,
      "loss": 1.5062,
      "step": 1553
    },
    {
      "epoch": 2.204255319148936,
      "grad_norm": 1.4244167804718018,
      "learning_rate": 5.443099273607748e-05,
      "loss": 1.4897,
      "step": 1554
    },
    {
      "epoch": 2.2056737588652484,
      "grad_norm": 1.409417986869812,
      "learning_rate": 5.4334140435835356e-05,
      "loss": 1.4431,
      "step": 1555
    },
    {
      "epoch": 2.2070921985815604,
      "grad_norm": 1.5329416990280151,
      "learning_rate": 5.423728813559322e-05,
      "loss": 1.3111,
      "step": 1556
    },
    {
      "epoch": 2.2085106382978723,
      "grad_norm": 1.3950151205062866,
      "learning_rate": 5.4140435835351086e-05,
      "loss": 1.3404,
      "step": 1557
    },
    {
      "epoch": 2.2099290780141843,
      "grad_norm": 1.4433573484420776,
      "learning_rate": 5.4043583535108964e-05,
      "loss": 1.341,
      "step": 1558
    },
    {
      "epoch": 2.2113475177304966,
      "grad_norm": 1.371188759803772,
      "learning_rate": 5.3946731234866835e-05,
      "loss": 1.3806,
      "step": 1559
    },
    {
      "epoch": 2.2127659574468086,
      "grad_norm": 1.2483582496643066,
      "learning_rate": 5.38498789346247e-05,
      "loss": 1.2748,
      "step": 1560
    },
    {
      "epoch": 2.2141843971631205,
      "grad_norm": 1.5010273456573486,
      "learning_rate": 5.3753026634382564e-05,
      "loss": 1.3488,
      "step": 1561
    },
    {
      "epoch": 2.2156028368794325,
      "grad_norm": 1.4726163148880005,
      "learning_rate": 5.365617433414044e-05,
      "loss": 1.4701,
      "step": 1562
    },
    {
      "epoch": 2.217021276595745,
      "grad_norm": 1.4843426942825317,
      "learning_rate": 5.355932203389831e-05,
      "loss": 1.5365,
      "step": 1563
    },
    {
      "epoch": 2.218439716312057,
      "grad_norm": 1.5079954862594604,
      "learning_rate": 5.346246973365617e-05,
      "loss": 1.3958,
      "step": 1564
    },
    {
      "epoch": 2.219858156028369,
      "grad_norm": 1.4931302070617676,
      "learning_rate": 5.336561743341405e-05,
      "loss": 1.4594,
      "step": 1565
    },
    {
      "epoch": 2.2212765957446807,
      "grad_norm": 1.4800339937210083,
      "learning_rate": 5.3268765133171914e-05,
      "loss": 1.4424,
      "step": 1566
    },
    {
      "epoch": 2.222695035460993,
      "grad_norm": 1.4149153232574463,
      "learning_rate": 5.3171912832929785e-05,
      "loss": 1.5518,
      "step": 1567
    },
    {
      "epoch": 2.224113475177305,
      "grad_norm": 1.4211757183074951,
      "learning_rate": 5.307506053268766e-05,
      "loss": 1.5342,
      "step": 1568
    },
    {
      "epoch": 2.225531914893617,
      "grad_norm": 1.4615157842636108,
      "learning_rate": 5.297820823244553e-05,
      "loss": 1.3839,
      "step": 1569
    },
    {
      "epoch": 2.226950354609929,
      "grad_norm": 1.4641366004943848,
      "learning_rate": 5.288135593220339e-05,
      "loss": 1.4572,
      "step": 1570
    },
    {
      "epoch": 2.228368794326241,
      "grad_norm": 1.4823384284973145,
      "learning_rate": 5.278450363196126e-05,
      "loss": 1.3552,
      "step": 1571
    },
    {
      "epoch": 2.2297872340425533,
      "grad_norm": 1.4737218618392944,
      "learning_rate": 5.2687651331719135e-05,
      "loss": 1.5722,
      "step": 1572
    },
    {
      "epoch": 2.2312056737588652,
      "grad_norm": 1.4619742631912231,
      "learning_rate": 5.2590799031477e-05,
      "loss": 1.4652,
      "step": 1573
    },
    {
      "epoch": 2.232624113475177,
      "grad_norm": 1.4805889129638672,
      "learning_rate": 5.2493946731234864e-05,
      "loss": 1.5752,
      "step": 1574
    },
    {
      "epoch": 2.2340425531914896,
      "grad_norm": 1.3996835947036743,
      "learning_rate": 5.239709443099274e-05,
      "loss": 1.4519,
      "step": 1575
    },
    {
      "epoch": 2.2354609929078015,
      "grad_norm": 1.65507972240448,
      "learning_rate": 5.230024213075061e-05,
      "loss": 1.4934,
      "step": 1576
    },
    {
      "epoch": 2.2368794326241135,
      "grad_norm": 1.3664945363998413,
      "learning_rate": 5.220338983050848e-05,
      "loss": 1.3807,
      "step": 1577
    },
    {
      "epoch": 2.2382978723404254,
      "grad_norm": 1.5511596202850342,
      "learning_rate": 5.210653753026634e-05,
      "loss": 1.4826,
      "step": 1578
    },
    {
      "epoch": 2.2397163120567374,
      "grad_norm": 1.4802807569503784,
      "learning_rate": 5.200968523002422e-05,
      "loss": 1.3172,
      "step": 1579
    },
    {
      "epoch": 2.2411347517730498,
      "grad_norm": 1.359431266784668,
      "learning_rate": 5.1912832929782086e-05,
      "loss": 1.3489,
      "step": 1580
    },
    {
      "epoch": 2.2425531914893617,
      "grad_norm": 1.4834880828857422,
      "learning_rate": 5.181598062953995e-05,
      "loss": 1.4461,
      "step": 1581
    },
    {
      "epoch": 2.2439716312056737,
      "grad_norm": 1.5643024444580078,
      "learning_rate": 5.171912832929783e-05,
      "loss": 1.4155,
      "step": 1582
    },
    {
      "epoch": 2.2453900709219856,
      "grad_norm": 1.3991096019744873,
      "learning_rate": 5.162227602905569e-05,
      "loss": 1.5339,
      "step": 1583
    },
    {
      "epoch": 2.246808510638298,
      "grad_norm": 1.5002858638763428,
      "learning_rate": 5.152542372881356e-05,
      "loss": 1.4401,
      "step": 1584
    },
    {
      "epoch": 2.24822695035461,
      "grad_norm": 1.462113618850708,
      "learning_rate": 5.142857142857143e-05,
      "loss": 1.4176,
      "step": 1585
    },
    {
      "epoch": 2.249645390070922,
      "grad_norm": 1.4111770391464233,
      "learning_rate": 5.13317191283293e-05,
      "loss": 1.2856,
      "step": 1586
    },
    {
      "epoch": 2.251063829787234,
      "grad_norm": 1.470426082611084,
      "learning_rate": 5.123486682808717e-05,
      "loss": 1.4311,
      "step": 1587
    },
    {
      "epoch": 2.2524822695035462,
      "grad_norm": 1.4308130741119385,
      "learning_rate": 5.1138014527845036e-05,
      "loss": 1.5944,
      "step": 1588
    },
    {
      "epoch": 2.253900709219858,
      "grad_norm": 1.2967040538787842,
      "learning_rate": 5.1041162227602914e-05,
      "loss": 1.1535,
      "step": 1589
    },
    {
      "epoch": 2.25531914893617,
      "grad_norm": 1.4070385694503784,
      "learning_rate": 5.094430992736078e-05,
      "loss": 1.4127,
      "step": 1590
    },
    {
      "epoch": 2.256737588652482,
      "grad_norm": 1.5038777589797974,
      "learning_rate": 5.0847457627118643e-05,
      "loss": 1.5129,
      "step": 1591
    },
    {
      "epoch": 2.2581560283687945,
      "grad_norm": 1.4017616510391235,
      "learning_rate": 5.075060532687651e-05,
      "loss": 1.6704,
      "step": 1592
    },
    {
      "epoch": 2.2595744680851064,
      "grad_norm": 1.3327068090438843,
      "learning_rate": 5.0653753026634386e-05,
      "loss": 1.3597,
      "step": 1593
    },
    {
      "epoch": 2.2609929078014184,
      "grad_norm": 1.4838975667953491,
      "learning_rate": 5.055690072639225e-05,
      "loss": 1.5105,
      "step": 1594
    },
    {
      "epoch": 2.2624113475177303,
      "grad_norm": 1.425686240196228,
      "learning_rate": 5.046004842615012e-05,
      "loss": 1.4244,
      "step": 1595
    },
    {
      "epoch": 2.2638297872340427,
      "grad_norm": 1.5136964321136475,
      "learning_rate": 5.036319612590799e-05,
      "loss": 1.6527,
      "step": 1596
    },
    {
      "epoch": 2.2652482269503547,
      "grad_norm": 1.5183194875717163,
      "learning_rate": 5.0266343825665865e-05,
      "loss": 1.44,
      "step": 1597
    },
    {
      "epoch": 2.2666666666666666,
      "grad_norm": 1.4581177234649658,
      "learning_rate": 5.016949152542373e-05,
      "loss": 1.4021,
      "step": 1598
    },
    {
      "epoch": 2.2680851063829786,
      "grad_norm": 1.4429172277450562,
      "learning_rate": 5.007263922518161e-05,
      "loss": 1.4918,
      "step": 1599
    },
    {
      "epoch": 2.269503546099291,
      "grad_norm": 1.4356329441070557,
      "learning_rate": 4.997578692493947e-05,
      "loss": 1.4653,
      "step": 1600
    },
    {
      "epoch": 2.270921985815603,
      "grad_norm": 1.3865786790847778,
      "learning_rate": 4.9878934624697336e-05,
      "loss": 1.4659,
      "step": 1601
    },
    {
      "epoch": 2.272340425531915,
      "grad_norm": 1.3404757976531982,
      "learning_rate": 4.978208232445521e-05,
      "loss": 1.1372,
      "step": 1602
    },
    {
      "epoch": 2.273758865248227,
      "grad_norm": 1.3724678754806519,
      "learning_rate": 4.968523002421307e-05,
      "loss": 1.0846,
      "step": 1603
    },
    {
      "epoch": 2.275177304964539,
      "grad_norm": 1.5572434663772583,
      "learning_rate": 4.9588377723970944e-05,
      "loss": 1.4087,
      "step": 1604
    },
    {
      "epoch": 2.276595744680851,
      "grad_norm": 1.4234168529510498,
      "learning_rate": 4.9491525423728815e-05,
      "loss": 1.3773,
      "step": 1605
    },
    {
      "epoch": 2.278014184397163,
      "grad_norm": 1.5790162086486816,
      "learning_rate": 4.9394673123486686e-05,
      "loss": 1.4893,
      "step": 1606
    },
    {
      "epoch": 2.279432624113475,
      "grad_norm": 1.5074700117111206,
      "learning_rate": 4.929782082324456e-05,
      "loss": 1.6053,
      "step": 1607
    },
    {
      "epoch": 2.2808510638297874,
      "grad_norm": 1.4618253707885742,
      "learning_rate": 4.920096852300242e-05,
      "loss": 1.4,
      "step": 1608
    },
    {
      "epoch": 2.2822695035460994,
      "grad_norm": 1.3910984992980957,
      "learning_rate": 4.9104116222760294e-05,
      "loss": 1.2679,
      "step": 1609
    },
    {
      "epoch": 2.2836879432624113,
      "grad_norm": 1.4241085052490234,
      "learning_rate": 4.900726392251816e-05,
      "loss": 1.315,
      "step": 1610
    },
    {
      "epoch": 2.2851063829787233,
      "grad_norm": 1.3961201906204224,
      "learning_rate": 4.891041162227603e-05,
      "loss": 1.3965,
      "step": 1611
    },
    {
      "epoch": 2.2865248226950357,
      "grad_norm": 1.4882863759994507,
      "learning_rate": 4.88135593220339e-05,
      "loss": 1.5277,
      "step": 1612
    },
    {
      "epoch": 2.2879432624113476,
      "grad_norm": 1.460922122001648,
      "learning_rate": 4.8716707021791765e-05,
      "loss": 1.5254,
      "step": 1613
    },
    {
      "epoch": 2.2893617021276595,
      "grad_norm": 1.4727625846862793,
      "learning_rate": 4.8619854721549644e-05,
      "loss": 1.3514,
      "step": 1614
    },
    {
      "epoch": 2.2907801418439715,
      "grad_norm": 1.4568837881088257,
      "learning_rate": 4.852300242130751e-05,
      "loss": 1.3645,
      "step": 1615
    },
    {
      "epoch": 2.2921985815602834,
      "grad_norm": 1.4977506399154663,
      "learning_rate": 4.842615012106538e-05,
      "loss": 1.371,
      "step": 1616
    },
    {
      "epoch": 2.293617021276596,
      "grad_norm": 1.4990290403366089,
      "learning_rate": 4.832929782082325e-05,
      "loss": 1.5116,
      "step": 1617
    },
    {
      "epoch": 2.295035460992908,
      "grad_norm": 1.493447184562683,
      "learning_rate": 4.8232445520581115e-05,
      "loss": 1.4141,
      "step": 1618
    },
    {
      "epoch": 2.2964539007092197,
      "grad_norm": 1.5359101295471191,
      "learning_rate": 4.813559322033899e-05,
      "loss": 1.4678,
      "step": 1619
    },
    {
      "epoch": 2.297872340425532,
      "grad_norm": 1.4075713157653809,
      "learning_rate": 4.803874092009685e-05,
      "loss": 1.3097,
      "step": 1620
    },
    {
      "epoch": 2.299290780141844,
      "grad_norm": 1.4158655405044556,
      "learning_rate": 4.794188861985472e-05,
      "loss": 1.4086,
      "step": 1621
    },
    {
      "epoch": 2.300709219858156,
      "grad_norm": 1.4398515224456787,
      "learning_rate": 4.784503631961259e-05,
      "loss": 1.3829,
      "step": 1622
    },
    {
      "epoch": 2.302127659574468,
      "grad_norm": 1.4655762910842896,
      "learning_rate": 4.7748184019370465e-05,
      "loss": 1.5089,
      "step": 1623
    },
    {
      "epoch": 2.30354609929078,
      "grad_norm": 1.4292892217636108,
      "learning_rate": 4.765133171912834e-05,
      "loss": 1.3833,
      "step": 1624
    },
    {
      "epoch": 2.3049645390070923,
      "grad_norm": 1.5714830160140991,
      "learning_rate": 4.75544794188862e-05,
      "loss": 1.3262,
      "step": 1625
    },
    {
      "epoch": 2.3063829787234043,
      "grad_norm": 1.6157231330871582,
      "learning_rate": 4.745762711864407e-05,
      "loss": 1.4903,
      "step": 1626
    },
    {
      "epoch": 2.307801418439716,
      "grad_norm": 1.3719149827957153,
      "learning_rate": 4.736077481840194e-05,
      "loss": 1.2405,
      "step": 1627
    },
    {
      "epoch": 2.3092198581560286,
      "grad_norm": 1.4175376892089844,
      "learning_rate": 4.726392251815981e-05,
      "loss": 1.4046,
      "step": 1628
    },
    {
      "epoch": 2.3106382978723405,
      "grad_norm": 1.4843593835830688,
      "learning_rate": 4.716707021791768e-05,
      "loss": 1.3819,
      "step": 1629
    },
    {
      "epoch": 2.3120567375886525,
      "grad_norm": 1.3763524293899536,
      "learning_rate": 4.7070217917675544e-05,
      "loss": 1.4685,
      "step": 1630
    },
    {
      "epoch": 2.3134751773049644,
      "grad_norm": 1.4763920307159424,
      "learning_rate": 4.6973365617433416e-05,
      "loss": 1.5422,
      "step": 1631
    },
    {
      "epoch": 2.3148936170212764,
      "grad_norm": 1.50879967212677,
      "learning_rate": 4.687651331719129e-05,
      "loss": 1.327,
      "step": 1632
    },
    {
      "epoch": 2.3163120567375888,
      "grad_norm": 1.5180238485336304,
      "learning_rate": 4.677966101694916e-05,
      "loss": 1.4556,
      "step": 1633
    },
    {
      "epoch": 2.3177304964539007,
      "grad_norm": 1.4092381000518799,
      "learning_rate": 4.668280871670702e-05,
      "loss": 1.3387,
      "step": 1634
    },
    {
      "epoch": 2.3191489361702127,
      "grad_norm": 1.4636515378952026,
      "learning_rate": 4.6585956416464894e-05,
      "loss": 1.4378,
      "step": 1635
    },
    {
      "epoch": 2.320567375886525,
      "grad_norm": 1.4951854944229126,
      "learning_rate": 4.6489104116222766e-05,
      "loss": 1.4716,
      "step": 1636
    },
    {
      "epoch": 2.321985815602837,
      "grad_norm": 1.4241470098495483,
      "learning_rate": 4.639225181598063e-05,
      "loss": 1.4948,
      "step": 1637
    },
    {
      "epoch": 2.323404255319149,
      "grad_norm": 1.4179487228393555,
      "learning_rate": 4.62953995157385e-05,
      "loss": 1.4388,
      "step": 1638
    },
    {
      "epoch": 2.324822695035461,
      "grad_norm": 1.4834036827087402,
      "learning_rate": 4.6198547215496366e-05,
      "loss": 1.6535,
      "step": 1639
    },
    {
      "epoch": 2.326241134751773,
      "grad_norm": 1.4283143281936646,
      "learning_rate": 4.610169491525424e-05,
      "loss": 1.2851,
      "step": 1640
    },
    {
      "epoch": 2.3276595744680852,
      "grad_norm": 1.5076119899749756,
      "learning_rate": 4.600484261501211e-05,
      "loss": 1.3207,
      "step": 1641
    },
    {
      "epoch": 2.329078014184397,
      "grad_norm": 1.527795672416687,
      "learning_rate": 4.590799031476998e-05,
      "loss": 1.4751,
      "step": 1642
    },
    {
      "epoch": 2.330496453900709,
      "grad_norm": 1.525974988937378,
      "learning_rate": 4.581113801452785e-05,
      "loss": 1.4291,
      "step": 1643
    },
    {
      "epoch": 2.331914893617021,
      "grad_norm": 1.522895097732544,
      "learning_rate": 4.5714285714285716e-05,
      "loss": 1.2891,
      "step": 1644
    },
    {
      "epoch": 2.3333333333333335,
      "grad_norm": 1.4680112600326538,
      "learning_rate": 4.561743341404359e-05,
      "loss": 1.5172,
      "step": 1645
    },
    {
      "epoch": 2.3347517730496454,
      "grad_norm": 1.3341217041015625,
      "learning_rate": 4.552058111380145e-05,
      "loss": 1.1967,
      "step": 1646
    },
    {
      "epoch": 2.3361702127659574,
      "grad_norm": 1.3934437036514282,
      "learning_rate": 4.542372881355932e-05,
      "loss": 1.3104,
      "step": 1647
    },
    {
      "epoch": 2.3375886524822693,
      "grad_norm": 1.467963457107544,
      "learning_rate": 4.5326876513317195e-05,
      "loss": 1.3151,
      "step": 1648
    },
    {
      "epoch": 2.3390070921985817,
      "grad_norm": 1.453902006149292,
      "learning_rate": 4.523002421307506e-05,
      "loss": 1.3597,
      "step": 1649
    },
    {
      "epoch": 2.3404255319148937,
      "grad_norm": 1.4679293632507324,
      "learning_rate": 4.513317191283293e-05,
      "loss": 1.5191,
      "step": 1650
    },
    {
      "epoch": 2.3418439716312056,
      "grad_norm": 1.3143577575683594,
      "learning_rate": 4.50363196125908e-05,
      "loss": 1.1844,
      "step": 1651
    },
    {
      "epoch": 2.3432624113475176,
      "grad_norm": 1.4757227897644043,
      "learning_rate": 4.493946731234867e-05,
      "loss": 1.3672,
      "step": 1652
    },
    {
      "epoch": 2.34468085106383,
      "grad_norm": 1.6714411973953247,
      "learning_rate": 4.484261501210654e-05,
      "loss": 1.4758,
      "step": 1653
    },
    {
      "epoch": 2.346099290780142,
      "grad_norm": 1.5576685667037964,
      "learning_rate": 4.474576271186441e-05,
      "loss": 1.6834,
      "step": 1654
    },
    {
      "epoch": 2.347517730496454,
      "grad_norm": 1.5092997550964355,
      "learning_rate": 4.464891041162228e-05,
      "loss": 1.5111,
      "step": 1655
    },
    {
      "epoch": 2.348936170212766,
      "grad_norm": 1.5623347759246826,
      "learning_rate": 4.4552058111380145e-05,
      "loss": 1.3278,
      "step": 1656
    },
    {
      "epoch": 2.350354609929078,
      "grad_norm": 1.4162752628326416,
      "learning_rate": 4.4455205811138016e-05,
      "loss": 1.3403,
      "step": 1657
    },
    {
      "epoch": 2.35177304964539,
      "grad_norm": 1.3750649690628052,
      "learning_rate": 4.435835351089588e-05,
      "loss": 1.2016,
      "step": 1658
    },
    {
      "epoch": 2.353191489361702,
      "grad_norm": 1.6367887258529663,
      "learning_rate": 4.426150121065375e-05,
      "loss": 1.5247,
      "step": 1659
    },
    {
      "epoch": 2.354609929078014,
      "grad_norm": 1.447830319404602,
      "learning_rate": 4.4164648910411624e-05,
      "loss": 1.4707,
      "step": 1660
    },
    {
      "epoch": 2.3560283687943264,
      "grad_norm": 1.4791500568389893,
      "learning_rate": 4.4067796610169495e-05,
      "loss": 1.4369,
      "step": 1661
    },
    {
      "epoch": 2.3574468085106384,
      "grad_norm": 1.3885992765426636,
      "learning_rate": 4.3970944309927366e-05,
      "loss": 1.3207,
      "step": 1662
    },
    {
      "epoch": 2.3588652482269503,
      "grad_norm": 1.4994771480560303,
      "learning_rate": 4.387409200968523e-05,
      "loss": 1.5841,
      "step": 1663
    },
    {
      "epoch": 2.3602836879432623,
      "grad_norm": 1.4345190525054932,
      "learning_rate": 4.37772397094431e-05,
      "loss": 1.3243,
      "step": 1664
    },
    {
      "epoch": 2.3617021276595747,
      "grad_norm": 1.3976573944091797,
      "learning_rate": 4.368038740920097e-05,
      "loss": 1.2762,
      "step": 1665
    },
    {
      "epoch": 2.3631205673758866,
      "grad_norm": 1.4812448024749756,
      "learning_rate": 4.358353510895884e-05,
      "loss": 1.4485,
      "step": 1666
    },
    {
      "epoch": 2.3645390070921986,
      "grad_norm": 1.4187253713607788,
      "learning_rate": 4.348668280871671e-05,
      "loss": 1.182,
      "step": 1667
    },
    {
      "epoch": 2.3659574468085105,
      "grad_norm": 1.5761120319366455,
      "learning_rate": 4.3389830508474574e-05,
      "loss": 1.4407,
      "step": 1668
    },
    {
      "epoch": 2.3673758865248224,
      "grad_norm": 1.489948034286499,
      "learning_rate": 4.3292978208232445e-05,
      "loss": 1.545,
      "step": 1669
    },
    {
      "epoch": 2.368794326241135,
      "grad_norm": 1.4674896001815796,
      "learning_rate": 4.319612590799032e-05,
      "loss": 1.3036,
      "step": 1670
    },
    {
      "epoch": 2.370212765957447,
      "grad_norm": 1.4974784851074219,
      "learning_rate": 4.309927360774819e-05,
      "loss": 1.4418,
      "step": 1671
    },
    {
      "epoch": 2.3716312056737587,
      "grad_norm": 1.413669466972351,
      "learning_rate": 4.300242130750606e-05,
      "loss": 1.3307,
      "step": 1672
    },
    {
      "epoch": 2.373049645390071,
      "grad_norm": 1.4751999378204346,
      "learning_rate": 4.2905569007263924e-05,
      "loss": 1.5308,
      "step": 1673
    },
    {
      "epoch": 2.374468085106383,
      "grad_norm": 1.4727727174758911,
      "learning_rate": 4.2808716707021795e-05,
      "loss": 1.603,
      "step": 1674
    },
    {
      "epoch": 2.375886524822695,
      "grad_norm": 1.5350171327590942,
      "learning_rate": 4.271186440677966e-05,
      "loss": 1.6643,
      "step": 1675
    },
    {
      "epoch": 2.377304964539007,
      "grad_norm": 1.5437899827957153,
      "learning_rate": 4.261501210653753e-05,
      "loss": 1.4718,
      "step": 1676
    },
    {
      "epoch": 2.378723404255319,
      "grad_norm": 1.5097969770431519,
      "learning_rate": 4.2518159806295396e-05,
      "loss": 1.3987,
      "step": 1677
    },
    {
      "epoch": 2.3801418439716313,
      "grad_norm": 1.4987001419067383,
      "learning_rate": 4.242130750605327e-05,
      "loss": 1.297,
      "step": 1678
    },
    {
      "epoch": 2.3815602836879433,
      "grad_norm": 1.637221097946167,
      "learning_rate": 4.2324455205811145e-05,
      "loss": 1.3446,
      "step": 1679
    },
    {
      "epoch": 2.382978723404255,
      "grad_norm": 1.4632866382598877,
      "learning_rate": 4.222760290556901e-05,
      "loss": 1.3008,
      "step": 1680
    },
    {
      "epoch": 2.3843971631205676,
      "grad_norm": 1.4777460098266602,
      "learning_rate": 4.213075060532688e-05,
      "loss": 1.4939,
      "step": 1681
    },
    {
      "epoch": 2.3858156028368795,
      "grad_norm": 1.4071625471115112,
      "learning_rate": 4.2033898305084746e-05,
      "loss": 1.428,
      "step": 1682
    },
    {
      "epoch": 2.3872340425531915,
      "grad_norm": 1.3877143859863281,
      "learning_rate": 4.193704600484262e-05,
      "loss": 1.3601,
      "step": 1683
    },
    {
      "epoch": 2.3886524822695034,
      "grad_norm": 1.5357396602630615,
      "learning_rate": 4.184019370460049e-05,
      "loss": 1.5575,
      "step": 1684
    },
    {
      "epoch": 2.3900709219858154,
      "grad_norm": 1.555010199546814,
      "learning_rate": 4.174334140435835e-05,
      "loss": 1.3912,
      "step": 1685
    },
    {
      "epoch": 2.391489361702128,
      "grad_norm": 1.4161949157714844,
      "learning_rate": 4.1646489104116224e-05,
      "loss": 1.2176,
      "step": 1686
    },
    {
      "epoch": 2.3929078014184397,
      "grad_norm": 1.5732923746109009,
      "learning_rate": 4.1549636803874096e-05,
      "loss": 1.3306,
      "step": 1687
    },
    {
      "epoch": 2.3943262411347517,
      "grad_norm": 1.5214390754699707,
      "learning_rate": 4.145278450363197e-05,
      "loss": 1.4557,
      "step": 1688
    },
    {
      "epoch": 2.395744680851064,
      "grad_norm": 1.5177104473114014,
      "learning_rate": 4.135593220338983e-05,
      "loss": 1.4041,
      "step": 1689
    },
    {
      "epoch": 2.397163120567376,
      "grad_norm": 1.5548256635665894,
      "learning_rate": 4.12590799031477e-05,
      "loss": 1.4336,
      "step": 1690
    },
    {
      "epoch": 2.398581560283688,
      "grad_norm": 1.4175368547439575,
      "learning_rate": 4.1162227602905574e-05,
      "loss": 1.4351,
      "step": 1691
    },
    {
      "epoch": 2.4,
      "grad_norm": 1.3038324117660522,
      "learning_rate": 4.106537530266344e-05,
      "loss": 1.3082,
      "step": 1692
    },
    {
      "epoch": 2.401418439716312,
      "grad_norm": 1.3668428659439087,
      "learning_rate": 4.096852300242131e-05,
      "loss": 1.4236,
      "step": 1693
    },
    {
      "epoch": 2.4028368794326243,
      "grad_norm": 1.381731390953064,
      "learning_rate": 4.0871670702179175e-05,
      "loss": 1.4327,
      "step": 1694
    },
    {
      "epoch": 2.404255319148936,
      "grad_norm": 1.3871852159500122,
      "learning_rate": 4.0774818401937046e-05,
      "loss": 1.178,
      "step": 1695
    },
    {
      "epoch": 2.405673758865248,
      "grad_norm": 1.6305668354034424,
      "learning_rate": 4.067796610169492e-05,
      "loss": 1.3577,
      "step": 1696
    },
    {
      "epoch": 2.40709219858156,
      "grad_norm": 1.3921003341674805,
      "learning_rate": 4.058111380145279e-05,
      "loss": 1.4793,
      "step": 1697
    },
    {
      "epoch": 2.4085106382978725,
      "grad_norm": 1.526225209236145,
      "learning_rate": 4.048426150121066e-05,
      "loss": 1.2671,
      "step": 1698
    },
    {
      "epoch": 2.4099290780141844,
      "grad_norm": 1.4391940832138062,
      "learning_rate": 4.0387409200968525e-05,
      "loss": 1.5386,
      "step": 1699
    },
    {
      "epoch": 2.4113475177304964,
      "grad_norm": 1.5194003582000732,
      "learning_rate": 4.0290556900726396e-05,
      "loss": 1.4396,
      "step": 1700
    },
    {
      "epoch": 2.4127659574468083,
      "grad_norm": 1.5773262977600098,
      "learning_rate": 4.019370460048426e-05,
      "loss": 1.5069,
      "step": 1701
    },
    {
      "epoch": 2.4141843971631207,
      "grad_norm": 1.5934138298034668,
      "learning_rate": 4.009685230024213e-05,
      "loss": 1.493,
      "step": 1702
    },
    {
      "epoch": 2.4156028368794327,
      "grad_norm": 1.5036745071411133,
      "learning_rate": 4e-05,
      "loss": 1.4285,
      "step": 1703
    },
    {
      "epoch": 2.4170212765957446,
      "grad_norm": 1.511169672012329,
      "learning_rate": 3.990314769975787e-05,
      "loss": 1.6135,
      "step": 1704
    },
    {
      "epoch": 2.4184397163120566,
      "grad_norm": 1.3829617500305176,
      "learning_rate": 3.980629539951574e-05,
      "loss": 1.4263,
      "step": 1705
    },
    {
      "epoch": 2.419858156028369,
      "grad_norm": 1.4460562467575073,
      "learning_rate": 3.970944309927361e-05,
      "loss": 1.3912,
      "step": 1706
    },
    {
      "epoch": 2.421276595744681,
      "grad_norm": 1.4056593179702759,
      "learning_rate": 3.961259079903148e-05,
      "loss": 1.518,
      "step": 1707
    },
    {
      "epoch": 2.422695035460993,
      "grad_norm": 1.385688304901123,
      "learning_rate": 3.951573849878935e-05,
      "loss": 1.532,
      "step": 1708
    },
    {
      "epoch": 2.424113475177305,
      "grad_norm": 1.2794289588928223,
      "learning_rate": 3.941888619854722e-05,
      "loss": 1.2551,
      "step": 1709
    },
    {
      "epoch": 2.425531914893617,
      "grad_norm": 1.6581021547317505,
      "learning_rate": 3.932203389830509e-05,
      "loss": 1.5692,
      "step": 1710
    },
    {
      "epoch": 2.426950354609929,
      "grad_norm": 1.4635530710220337,
      "learning_rate": 3.9225181598062954e-05,
      "loss": 1.2432,
      "step": 1711
    },
    {
      "epoch": 2.428368794326241,
      "grad_norm": 1.396939992904663,
      "learning_rate": 3.9128329297820825e-05,
      "loss": 1.3465,
      "step": 1712
    },
    {
      "epoch": 2.429787234042553,
      "grad_norm": 1.3597882986068726,
      "learning_rate": 3.903147699757869e-05,
      "loss": 1.2023,
      "step": 1713
    },
    {
      "epoch": 2.4312056737588654,
      "grad_norm": 1.4564788341522217,
      "learning_rate": 3.893462469733656e-05,
      "loss": 1.3172,
      "step": 1714
    },
    {
      "epoch": 2.4326241134751774,
      "grad_norm": 1.646226406097412,
      "learning_rate": 3.883777239709443e-05,
      "loss": 1.2507,
      "step": 1715
    },
    {
      "epoch": 2.4340425531914893,
      "grad_norm": 1.5150153636932373,
      "learning_rate": 3.8740920096852304e-05,
      "loss": 1.4828,
      "step": 1716
    },
    {
      "epoch": 2.4354609929078013,
      "grad_norm": 1.5115445852279663,
      "learning_rate": 3.8644067796610175e-05,
      "loss": 1.3686,
      "step": 1717
    },
    {
      "epoch": 2.4368794326241137,
      "grad_norm": 1.4320054054260254,
      "learning_rate": 3.854721549636804e-05,
      "loss": 1.4757,
      "step": 1718
    },
    {
      "epoch": 2.4382978723404256,
      "grad_norm": 1.5348459482192993,
      "learning_rate": 3.845036319612591e-05,
      "loss": 1.4951,
      "step": 1719
    },
    {
      "epoch": 2.4397163120567376,
      "grad_norm": 1.545628547668457,
      "learning_rate": 3.8353510895883775e-05,
      "loss": 1.5831,
      "step": 1720
    },
    {
      "epoch": 2.4411347517730495,
      "grad_norm": 1.5204341411590576,
      "learning_rate": 3.825665859564165e-05,
      "loss": 1.5023,
      "step": 1721
    },
    {
      "epoch": 2.4425531914893615,
      "grad_norm": 1.4361727237701416,
      "learning_rate": 3.815980629539952e-05,
      "loss": 1.5328,
      "step": 1722
    },
    {
      "epoch": 2.443971631205674,
      "grad_norm": 1.6043263673782349,
      "learning_rate": 3.806295399515738e-05,
      "loss": 1.4487,
      "step": 1723
    },
    {
      "epoch": 2.445390070921986,
      "grad_norm": 1.5678938627243042,
      "learning_rate": 3.7966101694915254e-05,
      "loss": 1.4792,
      "step": 1724
    },
    {
      "epoch": 2.4468085106382977,
      "grad_norm": 1.7011780738830566,
      "learning_rate": 3.7869249394673125e-05,
      "loss": 1.5701,
      "step": 1725
    },
    {
      "epoch": 2.44822695035461,
      "grad_norm": 1.4764121770858765,
      "learning_rate": 3.7772397094431e-05,
      "loss": 1.3311,
      "step": 1726
    },
    {
      "epoch": 2.449645390070922,
      "grad_norm": 1.5300358533859253,
      "learning_rate": 3.767554479418887e-05,
      "loss": 1.5318,
      "step": 1727
    },
    {
      "epoch": 2.451063829787234,
      "grad_norm": 1.5231943130493164,
      "learning_rate": 3.757869249394673e-05,
      "loss": 1.3567,
      "step": 1728
    },
    {
      "epoch": 2.452482269503546,
      "grad_norm": 1.4670379161834717,
      "learning_rate": 3.7481840193704604e-05,
      "loss": 1.4761,
      "step": 1729
    },
    {
      "epoch": 2.453900709219858,
      "grad_norm": 1.4697707891464233,
      "learning_rate": 3.738498789346247e-05,
      "loss": 1.4914,
      "step": 1730
    },
    {
      "epoch": 2.4553191489361703,
      "grad_norm": 1.4469412565231323,
      "learning_rate": 3.728813559322034e-05,
      "loss": 1.272,
      "step": 1731
    },
    {
      "epoch": 2.4567375886524823,
      "grad_norm": 1.480375051498413,
      "learning_rate": 3.7191283292978204e-05,
      "loss": 1.3979,
      "step": 1732
    },
    {
      "epoch": 2.458156028368794,
      "grad_norm": 1.4235470294952393,
      "learning_rate": 3.7094430992736076e-05,
      "loss": 1.367,
      "step": 1733
    },
    {
      "epoch": 2.4595744680851066,
      "grad_norm": 1.518324851989746,
      "learning_rate": 3.699757869249395e-05,
      "loss": 1.3994,
      "step": 1734
    },
    {
      "epoch": 2.4609929078014185,
      "grad_norm": 1.53078031539917,
      "learning_rate": 3.690072639225182e-05,
      "loss": 1.3384,
      "step": 1735
    },
    {
      "epoch": 2.4624113475177305,
      "grad_norm": 1.4972463846206665,
      "learning_rate": 3.680387409200969e-05,
      "loss": 1.4248,
      "step": 1736
    },
    {
      "epoch": 2.4638297872340424,
      "grad_norm": 1.3282992839813232,
      "learning_rate": 3.6707021791767554e-05,
      "loss": 1.3366,
      "step": 1737
    },
    {
      "epoch": 2.4652482269503544,
      "grad_norm": 1.5300089120864868,
      "learning_rate": 3.6610169491525426e-05,
      "loss": 1.4471,
      "step": 1738
    },
    {
      "epoch": 2.466666666666667,
      "grad_norm": 1.5617634057998657,
      "learning_rate": 3.65133171912833e-05,
      "loss": 1.3548,
      "step": 1739
    },
    {
      "epoch": 2.4680851063829787,
      "grad_norm": 1.4897772073745728,
      "learning_rate": 3.641646489104116e-05,
      "loss": 1.3681,
      "step": 1740
    },
    {
      "epoch": 2.4695035460992907,
      "grad_norm": 1.4970457553863525,
      "learning_rate": 3.631961259079903e-05,
      "loss": 1.301,
      "step": 1741
    },
    {
      "epoch": 2.470921985815603,
      "grad_norm": 1.4962764978408813,
      "learning_rate": 3.62227602905569e-05,
      "loss": 1.4231,
      "step": 1742
    },
    {
      "epoch": 2.472340425531915,
      "grad_norm": 1.3780161142349243,
      "learning_rate": 3.6125907990314776e-05,
      "loss": 1.4406,
      "step": 1743
    },
    {
      "epoch": 2.473758865248227,
      "grad_norm": 1.4962304830551147,
      "learning_rate": 3.602905569007264e-05,
      "loss": 1.3824,
      "step": 1744
    },
    {
      "epoch": 2.475177304964539,
      "grad_norm": 1.4746378660202026,
      "learning_rate": 3.593220338983051e-05,
      "loss": 1.3942,
      "step": 1745
    },
    {
      "epoch": 2.476595744680851,
      "grad_norm": 1.4059430360794067,
      "learning_rate": 3.583535108958838e-05,
      "loss": 1.3196,
      "step": 1746
    },
    {
      "epoch": 2.4780141843971633,
      "grad_norm": 1.4986426830291748,
      "learning_rate": 3.573849878934625e-05,
      "loss": 1.5702,
      "step": 1747
    },
    {
      "epoch": 2.479432624113475,
      "grad_norm": 1.480544090270996,
      "learning_rate": 3.564164648910412e-05,
      "loss": 1.3922,
      "step": 1748
    },
    {
      "epoch": 2.480851063829787,
      "grad_norm": 1.3771319389343262,
      "learning_rate": 3.554479418886198e-05,
      "loss": 1.3132,
      "step": 1749
    },
    {
      "epoch": 2.482269503546099,
      "grad_norm": 1.544189214706421,
      "learning_rate": 3.5447941888619855e-05,
      "loss": 1.4295,
      "step": 1750
    },
    {
      "epoch": 2.4836879432624115,
      "grad_norm": 1.5157018899917603,
      "learning_rate": 3.5351089588377726e-05,
      "loss": 1.6128,
      "step": 1751
    },
    {
      "epoch": 2.4851063829787234,
      "grad_norm": 1.5755587816238403,
      "learning_rate": 3.52542372881356e-05,
      "loss": 1.6844,
      "step": 1752
    },
    {
      "epoch": 2.4865248226950354,
      "grad_norm": 1.509829044342041,
      "learning_rate": 3.515738498789347e-05,
      "loss": 1.3888,
      "step": 1753
    },
    {
      "epoch": 2.4879432624113473,
      "grad_norm": 1.4956159591674805,
      "learning_rate": 3.506053268765133e-05,
      "loss": 1.7035,
      "step": 1754
    },
    {
      "epoch": 2.4893617021276597,
      "grad_norm": 1.4240912199020386,
      "learning_rate": 3.4963680387409205e-05,
      "loss": 1.4584,
      "step": 1755
    },
    {
      "epoch": 2.4907801418439717,
      "grad_norm": 1.5536539554595947,
      "learning_rate": 3.486682808716707e-05,
      "loss": 1.6377,
      "step": 1756
    },
    {
      "epoch": 2.4921985815602836,
      "grad_norm": 1.4846360683441162,
      "learning_rate": 3.476997578692494e-05,
      "loss": 1.2375,
      "step": 1757
    },
    {
      "epoch": 2.4936170212765956,
      "grad_norm": 1.5405184030532837,
      "learning_rate": 3.467312348668281e-05,
      "loss": 1.55,
      "step": 1758
    },
    {
      "epoch": 2.495035460992908,
      "grad_norm": 1.5734686851501465,
      "learning_rate": 3.4576271186440676e-05,
      "loss": 1.437,
      "step": 1759
    },
    {
      "epoch": 2.49645390070922,
      "grad_norm": 1.451116919517517,
      "learning_rate": 3.447941888619855e-05,
      "loss": 1.3953,
      "step": 1760
    },
    {
      "epoch": 2.497872340425532,
      "grad_norm": 1.472915530204773,
      "learning_rate": 3.438256658595642e-05,
      "loss": 1.2369,
      "step": 1761
    },
    {
      "epoch": 2.499290780141844,
      "grad_norm": 1.3835439682006836,
      "learning_rate": 3.428571428571429e-05,
      "loss": 1.3355,
      "step": 1762
    },
    {
      "epoch": 2.500709219858156,
      "grad_norm": 1.4822062253952026,
      "learning_rate": 3.418886198547216e-05,
      "loss": 1.3831,
      "step": 1763
    },
    {
      "epoch": 2.502127659574468,
      "grad_norm": 1.3304184675216675,
      "learning_rate": 3.4092009685230026e-05,
      "loss": 1.3295,
      "step": 1764
    },
    {
      "epoch": 2.50354609929078,
      "grad_norm": 1.4019577503204346,
      "learning_rate": 3.39951573849879e-05,
      "loss": 1.2786,
      "step": 1765
    },
    {
      "epoch": 2.504964539007092,
      "grad_norm": 1.4507983922958374,
      "learning_rate": 3.389830508474576e-05,
      "loss": 1.5561,
      "step": 1766
    },
    {
      "epoch": 2.506382978723404,
      "grad_norm": 1.4412188529968262,
      "learning_rate": 3.3801452784503634e-05,
      "loss": 1.3655,
      "step": 1767
    },
    {
      "epoch": 2.5078014184397164,
      "grad_norm": 1.5071163177490234,
      "learning_rate": 3.37046004842615e-05,
      "loss": 1.4446,
      "step": 1768
    },
    {
      "epoch": 2.5092198581560283,
      "grad_norm": 1.6865031719207764,
      "learning_rate": 3.360774818401937e-05,
      "loss": 1.2979,
      "step": 1769
    },
    {
      "epoch": 2.5106382978723403,
      "grad_norm": 1.559191107749939,
      "learning_rate": 3.351089588377724e-05,
      "loss": 1.5988,
      "step": 1770
    },
    {
      "epoch": 2.5120567375886527,
      "grad_norm": 1.492997407913208,
      "learning_rate": 3.341404358353511e-05,
      "loss": 1.4814,
      "step": 1771
    },
    {
      "epoch": 2.5134751773049646,
      "grad_norm": 1.5719977617263794,
      "learning_rate": 3.3317191283292984e-05,
      "loss": 1.4633,
      "step": 1772
    },
    {
      "epoch": 2.5148936170212766,
      "grad_norm": 1.5693031549453735,
      "learning_rate": 3.322033898305085e-05,
      "loss": 1.5211,
      "step": 1773
    },
    {
      "epoch": 2.5163120567375885,
      "grad_norm": 1.5848262310028076,
      "learning_rate": 3.312348668280872e-05,
      "loss": 1.5712,
      "step": 1774
    },
    {
      "epoch": 2.5177304964539005,
      "grad_norm": 1.493111491203308,
      "learning_rate": 3.302663438256659e-05,
      "loss": 1.3301,
      "step": 1775
    },
    {
      "epoch": 2.519148936170213,
      "grad_norm": 1.4322290420532227,
      "learning_rate": 3.2929782082324455e-05,
      "loss": 1.4498,
      "step": 1776
    },
    {
      "epoch": 2.520567375886525,
      "grad_norm": 1.4633938074111938,
      "learning_rate": 3.283292978208233e-05,
      "loss": 1.38,
      "step": 1777
    },
    {
      "epoch": 2.5219858156028367,
      "grad_norm": 1.4544676542282104,
      "learning_rate": 3.273607748184019e-05,
      "loss": 1.1998,
      "step": 1778
    },
    {
      "epoch": 2.523404255319149,
      "grad_norm": 1.6829121112823486,
      "learning_rate": 3.263922518159806e-05,
      "loss": 1.5109,
      "step": 1779
    },
    {
      "epoch": 2.524822695035461,
      "grad_norm": 1.4390735626220703,
      "learning_rate": 3.2542372881355934e-05,
      "loss": 1.3288,
      "step": 1780
    },
    {
      "epoch": 2.526241134751773,
      "grad_norm": 1.4440383911132812,
      "learning_rate": 3.2445520581113805e-05,
      "loss": 1.2433,
      "step": 1781
    },
    {
      "epoch": 2.527659574468085,
      "grad_norm": 1.4327366352081299,
      "learning_rate": 3.234866828087168e-05,
      "loss": 1.3518,
      "step": 1782
    },
    {
      "epoch": 2.529078014184397,
      "grad_norm": 1.4444465637207031,
      "learning_rate": 3.225181598062954e-05,
      "loss": 1.2973,
      "step": 1783
    },
    {
      "epoch": 2.5304964539007093,
      "grad_norm": 1.4565271139144897,
      "learning_rate": 3.215496368038741e-05,
      "loss": 1.3063,
      "step": 1784
    },
    {
      "epoch": 2.5319148936170213,
      "grad_norm": 1.3753236532211304,
      "learning_rate": 3.205811138014528e-05,
      "loss": 1.2841,
      "step": 1785
    },
    {
      "epoch": 2.533333333333333,
      "grad_norm": 1.596667766571045,
      "learning_rate": 3.196125907990315e-05,
      "loss": 1.4425,
      "step": 1786
    },
    {
      "epoch": 2.5347517730496456,
      "grad_norm": 1.5174481868743896,
      "learning_rate": 3.186440677966101e-05,
      "loss": 1.5588,
      "step": 1787
    },
    {
      "epoch": 2.5361702127659576,
      "grad_norm": 1.3981261253356934,
      "learning_rate": 3.1767554479418884e-05,
      "loss": 1.2404,
      "step": 1788
    },
    {
      "epoch": 2.5375886524822695,
      "grad_norm": 1.5806615352630615,
      "learning_rate": 3.1670702179176756e-05,
      "loss": 1.5612,
      "step": 1789
    },
    {
      "epoch": 2.5390070921985815,
      "grad_norm": 1.4103394746780396,
      "learning_rate": 3.157384987893463e-05,
      "loss": 1.335,
      "step": 1790
    },
    {
      "epoch": 2.5404255319148934,
      "grad_norm": 1.4725536108016968,
      "learning_rate": 3.14769975786925e-05,
      "loss": 1.4809,
      "step": 1791
    },
    {
      "epoch": 2.541843971631206,
      "grad_norm": 1.3994730710983276,
      "learning_rate": 3.138014527845036e-05,
      "loss": 1.3856,
      "step": 1792
    },
    {
      "epoch": 2.5432624113475177,
      "grad_norm": 1.4024913311004639,
      "learning_rate": 3.1283292978208234e-05,
      "loss": 1.2501,
      "step": 1793
    },
    {
      "epoch": 2.5446808510638297,
      "grad_norm": 1.6545089483261108,
      "learning_rate": 3.1186440677966106e-05,
      "loss": 1.5529,
      "step": 1794
    },
    {
      "epoch": 2.546099290780142,
      "grad_norm": 1.304354190826416,
      "learning_rate": 3.108958837772397e-05,
      "loss": 1.2466,
      "step": 1795
    },
    {
      "epoch": 2.547517730496454,
      "grad_norm": 1.420708179473877,
      "learning_rate": 3.099273607748184e-05,
      "loss": 1.4008,
      "step": 1796
    },
    {
      "epoch": 2.548936170212766,
      "grad_norm": 1.4709900617599487,
      "learning_rate": 3.0895883777239706e-05,
      "loss": 1.5772,
      "step": 1797
    },
    {
      "epoch": 2.550354609929078,
      "grad_norm": 1.4633599519729614,
      "learning_rate": 3.079903147699758e-05,
      "loss": 1.6077,
      "step": 1798
    },
    {
      "epoch": 2.55177304964539,
      "grad_norm": 1.4799492359161377,
      "learning_rate": 3.070217917675545e-05,
      "loss": 1.3081,
      "step": 1799
    },
    {
      "epoch": 2.5531914893617023,
      "grad_norm": 1.4908608198165894,
      "learning_rate": 3.060532687651332e-05,
      "loss": 1.4713,
      "step": 1800
    },
    {
      "epoch": 2.554609929078014,
      "grad_norm": 1.4276707172393799,
      "learning_rate": 3.050847457627119e-05,
      "loss": 1.4523,
      "step": 1801
    },
    {
      "epoch": 2.556028368794326,
      "grad_norm": 1.4208298921585083,
      "learning_rate": 3.0411622276029056e-05,
      "loss": 1.308,
      "step": 1802
    },
    {
      "epoch": 2.5574468085106385,
      "grad_norm": 1.445909857749939,
      "learning_rate": 3.0314769975786927e-05,
      "loss": 1.4019,
      "step": 1803
    },
    {
      "epoch": 2.5588652482269505,
      "grad_norm": 1.4242745637893677,
      "learning_rate": 3.0217917675544792e-05,
      "loss": 1.3489,
      "step": 1804
    },
    {
      "epoch": 2.5602836879432624,
      "grad_norm": 1.5859531164169312,
      "learning_rate": 3.0121065375302667e-05,
      "loss": 1.4054,
      "step": 1805
    },
    {
      "epoch": 2.5617021276595744,
      "grad_norm": 1.5177541971206665,
      "learning_rate": 3.0024213075060538e-05,
      "loss": 1.2958,
      "step": 1806
    },
    {
      "epoch": 2.5631205673758863,
      "grad_norm": 1.437294840812683,
      "learning_rate": 2.9927360774818403e-05,
      "loss": 1.3432,
      "step": 1807
    },
    {
      "epoch": 2.5645390070921987,
      "grad_norm": 1.405775547027588,
      "learning_rate": 2.9830508474576274e-05,
      "loss": 1.3641,
      "step": 1808
    },
    {
      "epoch": 2.5659574468085107,
      "grad_norm": 1.4151053428649902,
      "learning_rate": 2.9733656174334142e-05,
      "loss": 1.3378,
      "step": 1809
    },
    {
      "epoch": 2.5673758865248226,
      "grad_norm": 1.5464966297149658,
      "learning_rate": 2.9636803874092013e-05,
      "loss": 1.4737,
      "step": 1810
    },
    {
      "epoch": 2.568794326241135,
      "grad_norm": 1.572190761566162,
      "learning_rate": 2.9539951573849878e-05,
      "loss": 1.3776,
      "step": 1811
    },
    {
      "epoch": 2.570212765957447,
      "grad_norm": 1.46523118019104,
      "learning_rate": 2.944309927360775e-05,
      "loss": 1.5162,
      "step": 1812
    },
    {
      "epoch": 2.571631205673759,
      "grad_norm": 1.5492583513259888,
      "learning_rate": 2.934624697336562e-05,
      "loss": 1.3731,
      "step": 1813
    },
    {
      "epoch": 2.573049645390071,
      "grad_norm": 1.5572068691253662,
      "learning_rate": 2.924939467312349e-05,
      "loss": 1.46,
      "step": 1814
    },
    {
      "epoch": 2.574468085106383,
      "grad_norm": 1.5141173601150513,
      "learning_rate": 2.915254237288136e-05,
      "loss": 1.514,
      "step": 1815
    },
    {
      "epoch": 2.575886524822695,
      "grad_norm": 1.5330488681793213,
      "learning_rate": 2.9055690072639224e-05,
      "loss": 1.5627,
      "step": 1816
    },
    {
      "epoch": 2.577304964539007,
      "grad_norm": 1.6318014860153198,
      "learning_rate": 2.8958837772397096e-05,
      "loss": 1.6638,
      "step": 1817
    },
    {
      "epoch": 2.578723404255319,
      "grad_norm": 1.5417107343673706,
      "learning_rate": 2.8861985472154967e-05,
      "loss": 1.4224,
      "step": 1818
    },
    {
      "epoch": 2.580141843971631,
      "grad_norm": 1.6323078870773315,
      "learning_rate": 2.8765133171912835e-05,
      "loss": 1.402,
      "step": 1819
    },
    {
      "epoch": 2.581560283687943,
      "grad_norm": 1.5261383056640625,
      "learning_rate": 2.8668280871670706e-05,
      "loss": 1.445,
      "step": 1820
    },
    {
      "epoch": 2.5829787234042554,
      "grad_norm": 1.5914201736450195,
      "learning_rate": 2.857142857142857e-05,
      "loss": 1.355,
      "step": 1821
    },
    {
      "epoch": 2.5843971631205673,
      "grad_norm": 1.6295143365859985,
      "learning_rate": 2.8474576271186442e-05,
      "loss": 1.4719,
      "step": 1822
    },
    {
      "epoch": 2.5858156028368793,
      "grad_norm": 1.5643616914749146,
      "learning_rate": 2.837772397094431e-05,
      "loss": 1.396,
      "step": 1823
    },
    {
      "epoch": 2.5872340425531917,
      "grad_norm": 1.4602981805801392,
      "learning_rate": 2.828087167070218e-05,
      "loss": 1.3722,
      "step": 1824
    },
    {
      "epoch": 2.5886524822695036,
      "grad_norm": 1.4739199876785278,
      "learning_rate": 2.8184019370460053e-05,
      "loss": 1.2613,
      "step": 1825
    },
    {
      "epoch": 2.5900709219858156,
      "grad_norm": 1.3947217464447021,
      "learning_rate": 2.8087167070217917e-05,
      "loss": 1.3459,
      "step": 1826
    },
    {
      "epoch": 2.5914893617021275,
      "grad_norm": 1.4430004358291626,
      "learning_rate": 2.799031476997579e-05,
      "loss": 1.4211,
      "step": 1827
    },
    {
      "epoch": 2.5929078014184395,
      "grad_norm": 1.5242136716842651,
      "learning_rate": 2.7893462469733657e-05,
      "loss": 1.4669,
      "step": 1828
    },
    {
      "epoch": 2.594326241134752,
      "grad_norm": 1.522513747215271,
      "learning_rate": 2.7796610169491528e-05,
      "loss": 1.4736,
      "step": 1829
    },
    {
      "epoch": 2.595744680851064,
      "grad_norm": 1.5449966192245483,
      "learning_rate": 2.76997578692494e-05,
      "loss": 1.5167,
      "step": 1830
    },
    {
      "epoch": 2.5971631205673757,
      "grad_norm": 1.4744541645050049,
      "learning_rate": 2.7602905569007264e-05,
      "loss": 1.5045,
      "step": 1831
    },
    {
      "epoch": 2.598581560283688,
      "grad_norm": 1.4822121858596802,
      "learning_rate": 2.7506053268765135e-05,
      "loss": 1.3526,
      "step": 1832
    },
    {
      "epoch": 2.6,
      "grad_norm": 1.4581516981124878,
      "learning_rate": 2.7409200968523003e-05,
      "loss": 1.4026,
      "step": 1833
    },
    {
      "epoch": 2.601418439716312,
      "grad_norm": 1.574625849723816,
      "learning_rate": 2.7312348668280875e-05,
      "loss": 1.5278,
      "step": 1834
    },
    {
      "epoch": 2.602836879432624,
      "grad_norm": 1.5355517864227295,
      "learning_rate": 2.721549636803874e-05,
      "loss": 1.2653,
      "step": 1835
    },
    {
      "epoch": 2.604255319148936,
      "grad_norm": 1.5767017602920532,
      "learning_rate": 2.711864406779661e-05,
      "loss": 1.2638,
      "step": 1836
    },
    {
      "epoch": 2.6056737588652483,
      "grad_norm": 1.3827645778656006,
      "learning_rate": 2.7021791767554482e-05,
      "loss": 1.2972,
      "step": 1837
    },
    {
      "epoch": 2.6070921985815603,
      "grad_norm": 1.688406229019165,
      "learning_rate": 2.692493946731235e-05,
      "loss": 1.4449,
      "step": 1838
    },
    {
      "epoch": 2.608510638297872,
      "grad_norm": 1.4078423976898193,
      "learning_rate": 2.682808716707022e-05,
      "loss": 1.3609,
      "step": 1839
    },
    {
      "epoch": 2.6099290780141846,
      "grad_norm": 1.5533372163772583,
      "learning_rate": 2.6731234866828086e-05,
      "loss": 1.3439,
      "step": 1840
    },
    {
      "epoch": 2.6113475177304966,
      "grad_norm": 1.3983476161956787,
      "learning_rate": 2.6634382566585957e-05,
      "loss": 1.3765,
      "step": 1841
    },
    {
      "epoch": 2.6127659574468085,
      "grad_norm": 1.5791975259780884,
      "learning_rate": 2.653753026634383e-05,
      "loss": 1.5127,
      "step": 1842
    },
    {
      "epoch": 2.6141843971631205,
      "grad_norm": 1.463752031326294,
      "learning_rate": 2.6440677966101696e-05,
      "loss": 1.4424,
      "step": 1843
    },
    {
      "epoch": 2.6156028368794324,
      "grad_norm": 1.7516952753067017,
      "learning_rate": 2.6343825665859568e-05,
      "loss": 1.2583,
      "step": 1844
    },
    {
      "epoch": 2.617021276595745,
      "grad_norm": 1.500226616859436,
      "learning_rate": 2.6246973365617432e-05,
      "loss": 1.5709,
      "step": 1845
    },
    {
      "epoch": 2.6184397163120567,
      "grad_norm": 1.4723719358444214,
      "learning_rate": 2.6150121065375304e-05,
      "loss": 1.2834,
      "step": 1846
    },
    {
      "epoch": 2.6198581560283687,
      "grad_norm": 1.4127614498138428,
      "learning_rate": 2.605326876513317e-05,
      "loss": 1.3101,
      "step": 1847
    },
    {
      "epoch": 2.621276595744681,
      "grad_norm": 1.5505695343017578,
      "learning_rate": 2.5956416464891043e-05,
      "loss": 1.352,
      "step": 1848
    },
    {
      "epoch": 2.622695035460993,
      "grad_norm": 1.520652413368225,
      "learning_rate": 2.5859564164648914e-05,
      "loss": 1.5597,
      "step": 1849
    },
    {
      "epoch": 2.624113475177305,
      "grad_norm": 1.4895187616348267,
      "learning_rate": 2.576271186440678e-05,
      "loss": 1.4978,
      "step": 1850
    },
    {
      "epoch": 2.625531914893617,
      "grad_norm": 1.4361584186553955,
      "learning_rate": 2.566585956416465e-05,
      "loss": 1.3978,
      "step": 1851
    },
    {
      "epoch": 2.626950354609929,
      "grad_norm": 1.58907949924469,
      "learning_rate": 2.5569007263922518e-05,
      "loss": 1.3955,
      "step": 1852
    },
    {
      "epoch": 2.6283687943262413,
      "grad_norm": 1.5910638570785522,
      "learning_rate": 2.547215496368039e-05,
      "loss": 1.4887,
      "step": 1853
    },
    {
      "epoch": 2.629787234042553,
      "grad_norm": 1.5410783290863037,
      "learning_rate": 2.5375302663438254e-05,
      "loss": 1.3268,
      "step": 1854
    },
    {
      "epoch": 2.631205673758865,
      "grad_norm": 1.4382141828536987,
      "learning_rate": 2.5278450363196125e-05,
      "loss": 1.4638,
      "step": 1855
    },
    {
      "epoch": 2.6326241134751776,
      "grad_norm": 1.4745970964431763,
      "learning_rate": 2.5181598062953997e-05,
      "loss": 1.3992,
      "step": 1856
    },
    {
      "epoch": 2.6340425531914895,
      "grad_norm": 1.467373251914978,
      "learning_rate": 2.5084745762711865e-05,
      "loss": 1.4095,
      "step": 1857
    },
    {
      "epoch": 2.6354609929078014,
      "grad_norm": 1.5597448348999023,
      "learning_rate": 2.4987893462469736e-05,
      "loss": 1.5452,
      "step": 1858
    },
    {
      "epoch": 2.6368794326241134,
      "grad_norm": 1.5420891046524048,
      "learning_rate": 2.4891041162227604e-05,
      "loss": 1.414,
      "step": 1859
    },
    {
      "epoch": 2.6382978723404253,
      "grad_norm": 1.5878870487213135,
      "learning_rate": 2.4794188861985472e-05,
      "loss": 1.4748,
      "step": 1860
    },
    {
      "epoch": 2.6397163120567377,
      "grad_norm": 1.5225287675857544,
      "learning_rate": 2.4697336561743343e-05,
      "loss": 1.3975,
      "step": 1861
    },
    {
      "epoch": 2.6411347517730497,
      "grad_norm": 1.5357136726379395,
      "learning_rate": 2.460048426150121e-05,
      "loss": 1.2506,
      "step": 1862
    },
    {
      "epoch": 2.6425531914893616,
      "grad_norm": 1.5668476819992065,
      "learning_rate": 2.450363196125908e-05,
      "loss": 1.3327,
      "step": 1863
    },
    {
      "epoch": 2.643971631205674,
      "grad_norm": 1.3813921213150024,
      "learning_rate": 2.440677966101695e-05,
      "loss": 1.3236,
      "step": 1864
    },
    {
      "epoch": 2.645390070921986,
      "grad_norm": 1.4183279275894165,
      "learning_rate": 2.4309927360774822e-05,
      "loss": 1.2148,
      "step": 1865
    },
    {
      "epoch": 2.646808510638298,
      "grad_norm": 1.4919912815093994,
      "learning_rate": 2.421307506053269e-05,
      "loss": 1.5735,
      "step": 1866
    },
    {
      "epoch": 2.64822695035461,
      "grad_norm": 1.5404901504516602,
      "learning_rate": 2.4116222760290558e-05,
      "loss": 1.3842,
      "step": 1867
    },
    {
      "epoch": 2.649645390070922,
      "grad_norm": 1.4808290004730225,
      "learning_rate": 2.4019370460048426e-05,
      "loss": 1.512,
      "step": 1868
    },
    {
      "epoch": 2.651063829787234,
      "grad_norm": 1.449110507965088,
      "learning_rate": 2.3922518159806294e-05,
      "loss": 1.5416,
      "step": 1869
    },
    {
      "epoch": 2.652482269503546,
      "grad_norm": 1.4361350536346436,
      "learning_rate": 2.382566585956417e-05,
      "loss": 1.3261,
      "step": 1870
    },
    {
      "epoch": 2.653900709219858,
      "grad_norm": 1.6202565431594849,
      "learning_rate": 2.3728813559322036e-05,
      "loss": 1.4311,
      "step": 1871
    },
    {
      "epoch": 2.65531914893617,
      "grad_norm": 1.427713394165039,
      "learning_rate": 2.3631961259079904e-05,
      "loss": 1.1924,
      "step": 1872
    },
    {
      "epoch": 2.656737588652482,
      "grad_norm": 1.5688236951828003,
      "learning_rate": 2.3535108958837772e-05,
      "loss": 1.3218,
      "step": 1873
    },
    {
      "epoch": 2.6581560283687944,
      "grad_norm": 1.7160873413085938,
      "learning_rate": 2.3438256658595644e-05,
      "loss": 1.5359,
      "step": 1874
    },
    {
      "epoch": 2.6595744680851063,
      "grad_norm": 1.5932213068008423,
      "learning_rate": 2.334140435835351e-05,
      "loss": 1.3187,
      "step": 1875
    },
    {
      "epoch": 2.6609929078014183,
      "grad_norm": 1.532578706741333,
      "learning_rate": 2.3244552058111383e-05,
      "loss": 1.535,
      "step": 1876
    },
    {
      "epoch": 2.6624113475177307,
      "grad_norm": 1.5385960340499878,
      "learning_rate": 2.314769975786925e-05,
      "loss": 1.4675,
      "step": 1877
    },
    {
      "epoch": 2.6638297872340426,
      "grad_norm": 1.5023683309555054,
      "learning_rate": 2.305084745762712e-05,
      "loss": 1.3172,
      "step": 1878
    },
    {
      "epoch": 2.6652482269503546,
      "grad_norm": 1.7187577486038208,
      "learning_rate": 2.295399515738499e-05,
      "loss": 1.5749,
      "step": 1879
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 1.5393166542053223,
      "learning_rate": 2.2857142857142858e-05,
      "loss": 1.6171,
      "step": 1880
    },
    {
      "epoch": 2.6680851063829785,
      "grad_norm": 1.5892003774642944,
      "learning_rate": 2.2760290556900726e-05,
      "loss": 1.4369,
      "step": 1881
    },
    {
      "epoch": 2.669503546099291,
      "grad_norm": 1.512583613395691,
      "learning_rate": 2.2663438256658597e-05,
      "loss": 1.5311,
      "step": 1882
    },
    {
      "epoch": 2.670921985815603,
      "grad_norm": 1.6003855466842651,
      "learning_rate": 2.2566585956416465e-05,
      "loss": 1.5038,
      "step": 1883
    },
    {
      "epoch": 2.6723404255319148,
      "grad_norm": 1.5015908479690552,
      "learning_rate": 2.2469733656174337e-05,
      "loss": 1.5333,
      "step": 1884
    },
    {
      "epoch": 2.673758865248227,
      "grad_norm": 1.482750415802002,
      "learning_rate": 2.2372881355932205e-05,
      "loss": 1.2447,
      "step": 1885
    },
    {
      "epoch": 2.675177304964539,
      "grad_norm": 1.5905553102493286,
      "learning_rate": 2.2276029055690073e-05,
      "loss": 1.739,
      "step": 1886
    },
    {
      "epoch": 2.676595744680851,
      "grad_norm": 1.3881415128707886,
      "learning_rate": 2.217917675544794e-05,
      "loss": 1.2576,
      "step": 1887
    },
    {
      "epoch": 2.678014184397163,
      "grad_norm": 1.5423662662506104,
      "learning_rate": 2.2082324455205812e-05,
      "loss": 1.3849,
      "step": 1888
    },
    {
      "epoch": 2.679432624113475,
      "grad_norm": 1.409290075302124,
      "learning_rate": 2.1985472154963683e-05,
      "loss": 1.493,
      "step": 1889
    },
    {
      "epoch": 2.6808510638297873,
      "grad_norm": 1.4920083284378052,
      "learning_rate": 2.188861985472155e-05,
      "loss": 1.3577,
      "step": 1890
    },
    {
      "epoch": 2.6822695035460993,
      "grad_norm": 1.4412606954574585,
      "learning_rate": 2.179176755447942e-05,
      "loss": 1.3854,
      "step": 1891
    },
    {
      "epoch": 2.6836879432624112,
      "grad_norm": 1.4858242273330688,
      "learning_rate": 2.1694915254237287e-05,
      "loss": 1.5618,
      "step": 1892
    },
    {
      "epoch": 2.6851063829787236,
      "grad_norm": 1.5436434745788574,
      "learning_rate": 2.159806295399516e-05,
      "loss": 1.4187,
      "step": 1893
    },
    {
      "epoch": 2.6865248226950356,
      "grad_norm": 1.4110256433486938,
      "learning_rate": 2.150121065375303e-05,
      "loss": 1.4455,
      "step": 1894
    },
    {
      "epoch": 2.6879432624113475,
      "grad_norm": 1.454368233680725,
      "learning_rate": 2.1404358353510898e-05,
      "loss": 1.1928,
      "step": 1895
    },
    {
      "epoch": 2.6893617021276595,
      "grad_norm": 1.5707334280014038,
      "learning_rate": 2.1307506053268766e-05,
      "loss": 1.3796,
      "step": 1896
    },
    {
      "epoch": 2.6907801418439714,
      "grad_norm": 1.5740413665771484,
      "learning_rate": 2.1210653753026634e-05,
      "loss": 1.6015,
      "step": 1897
    },
    {
      "epoch": 2.692198581560284,
      "grad_norm": 1.584436297416687,
      "learning_rate": 2.1113801452784505e-05,
      "loss": 1.4031,
      "step": 1898
    },
    {
      "epoch": 2.6936170212765957,
      "grad_norm": 1.5418658256530762,
      "learning_rate": 2.1016949152542373e-05,
      "loss": 1.3921,
      "step": 1899
    },
    {
      "epoch": 2.6950354609929077,
      "grad_norm": 1.6269220113754272,
      "learning_rate": 2.0920096852300244e-05,
      "loss": 1.4158,
      "step": 1900
    },
    {
      "epoch": 2.69645390070922,
      "grad_norm": 1.4602198600769043,
      "learning_rate": 2.0823244552058112e-05,
      "loss": 1.3283,
      "step": 1901
    },
    {
      "epoch": 2.697872340425532,
      "grad_norm": 1.4242531061172485,
      "learning_rate": 2.0726392251815984e-05,
      "loss": 1.4144,
      "step": 1902
    },
    {
      "epoch": 2.699290780141844,
      "grad_norm": 1.3966209888458252,
      "learning_rate": 2.062953995157385e-05,
      "loss": 1.3607,
      "step": 1903
    },
    {
      "epoch": 2.700709219858156,
      "grad_norm": 1.4670093059539795,
      "learning_rate": 2.053268765133172e-05,
      "loss": 1.4831,
      "step": 1904
    },
    {
      "epoch": 2.702127659574468,
      "grad_norm": 1.6262476444244385,
      "learning_rate": 2.0435835351089587e-05,
      "loss": 1.5165,
      "step": 1905
    },
    {
      "epoch": 2.7035460992907803,
      "grad_norm": 1.4448670148849487,
      "learning_rate": 2.033898305084746e-05,
      "loss": 1.3295,
      "step": 1906
    },
    {
      "epoch": 2.704964539007092,
      "grad_norm": 1.4966901540756226,
      "learning_rate": 2.024213075060533e-05,
      "loss": 1.4727,
      "step": 1907
    },
    {
      "epoch": 2.706382978723404,
      "grad_norm": 1.5024853944778442,
      "learning_rate": 2.0145278450363198e-05,
      "loss": 1.4735,
      "step": 1908
    },
    {
      "epoch": 2.7078014184397166,
      "grad_norm": 1.6129194498062134,
      "learning_rate": 2.0048426150121066e-05,
      "loss": 1.4741,
      "step": 1909
    },
    {
      "epoch": 2.7092198581560285,
      "grad_norm": 1.498971700668335,
      "learning_rate": 1.9951573849878934e-05,
      "loss": 1.4073,
      "step": 1910
    },
    {
      "epoch": 2.7106382978723405,
      "grad_norm": 1.5036633014678955,
      "learning_rate": 1.9854721549636805e-05,
      "loss": 1.3207,
      "step": 1911
    },
    {
      "epoch": 2.7120567375886524,
      "grad_norm": 1.6966495513916016,
      "learning_rate": 1.9757869249394677e-05,
      "loss": 1.4813,
      "step": 1912
    },
    {
      "epoch": 2.7134751773049643,
      "grad_norm": 1.3685238361358643,
      "learning_rate": 1.9661016949152545e-05,
      "loss": 1.2053,
      "step": 1913
    },
    {
      "epoch": 2.7148936170212767,
      "grad_norm": 1.6478253602981567,
      "learning_rate": 1.9564164648910413e-05,
      "loss": 1.4098,
      "step": 1914
    },
    {
      "epoch": 2.7163120567375887,
      "grad_norm": 1.6411857604980469,
      "learning_rate": 1.946731234866828e-05,
      "loss": 1.6424,
      "step": 1915
    },
    {
      "epoch": 2.7177304964539006,
      "grad_norm": 1.5470725297927856,
      "learning_rate": 1.9370460048426152e-05,
      "loss": 1.3757,
      "step": 1916
    },
    {
      "epoch": 2.719148936170213,
      "grad_norm": 1.6241904497146606,
      "learning_rate": 1.927360774818402e-05,
      "loss": 1.5131,
      "step": 1917
    },
    {
      "epoch": 2.720567375886525,
      "grad_norm": 1.3782141208648682,
      "learning_rate": 1.9176755447941888e-05,
      "loss": 1.3294,
      "step": 1918
    },
    {
      "epoch": 2.721985815602837,
      "grad_norm": 1.5601004362106323,
      "learning_rate": 1.907990314769976e-05,
      "loss": 1.2315,
      "step": 1919
    },
    {
      "epoch": 2.723404255319149,
      "grad_norm": 1.5072306394577026,
      "learning_rate": 1.8983050847457627e-05,
      "loss": 1.4091,
      "step": 1920
    },
    {
      "epoch": 2.724822695035461,
      "grad_norm": 1.4711501598358154,
      "learning_rate": 1.88861985472155e-05,
      "loss": 1.3765,
      "step": 1921
    },
    {
      "epoch": 2.726241134751773,
      "grad_norm": 1.442096471786499,
      "learning_rate": 1.8789346246973366e-05,
      "loss": 1.3738,
      "step": 1922
    },
    {
      "epoch": 2.727659574468085,
      "grad_norm": 1.4750503301620483,
      "learning_rate": 1.8692493946731234e-05,
      "loss": 1.2934,
      "step": 1923
    },
    {
      "epoch": 2.729078014184397,
      "grad_norm": 1.61795175075531,
      "learning_rate": 1.8595641646489102e-05,
      "loss": 1.3915,
      "step": 1924
    },
    {
      "epoch": 2.7304964539007095,
      "grad_norm": 1.430413007736206,
      "learning_rate": 1.8498789346246974e-05,
      "loss": 1.2671,
      "step": 1925
    },
    {
      "epoch": 2.731914893617021,
      "grad_norm": 1.5467395782470703,
      "learning_rate": 1.8401937046004845e-05,
      "loss": 1.243,
      "step": 1926
    },
    {
      "epoch": 2.7333333333333334,
      "grad_norm": 1.4644495248794556,
      "learning_rate": 1.8305084745762713e-05,
      "loss": 1.5137,
      "step": 1927
    },
    {
      "epoch": 2.7347517730496453,
      "grad_norm": 1.4216783046722412,
      "learning_rate": 1.820823244552058e-05,
      "loss": 1.1972,
      "step": 1928
    },
    {
      "epoch": 2.7361702127659573,
      "grad_norm": 1.5411878824234009,
      "learning_rate": 1.811138014527845e-05,
      "loss": 1.4887,
      "step": 1929
    },
    {
      "epoch": 2.7375886524822697,
      "grad_norm": 1.6218342781066895,
      "learning_rate": 1.801452784503632e-05,
      "loss": 1.4668,
      "step": 1930
    },
    {
      "epoch": 2.7390070921985816,
      "grad_norm": 1.5515984296798706,
      "learning_rate": 1.791767554479419e-05,
      "loss": 1.4149,
      "step": 1931
    },
    {
      "epoch": 2.7404255319148936,
      "grad_norm": 1.4921101331710815,
      "learning_rate": 1.782082324455206e-05,
      "loss": 1.3865,
      "step": 1932
    },
    {
      "epoch": 2.7418439716312055,
      "grad_norm": 1.4800420999526978,
      "learning_rate": 1.7723970944309927e-05,
      "loss": 1.2977,
      "step": 1933
    },
    {
      "epoch": 2.7432624113475175,
      "grad_norm": 1.675351619720459,
      "learning_rate": 1.76271186440678e-05,
      "loss": 1.6203,
      "step": 1934
    },
    {
      "epoch": 2.74468085106383,
      "grad_norm": 1.5584889650344849,
      "learning_rate": 1.7530266343825667e-05,
      "loss": 1.4839,
      "step": 1935
    },
    {
      "epoch": 2.746099290780142,
      "grad_norm": 1.6198517084121704,
      "learning_rate": 1.7433414043583535e-05,
      "loss": 1.4552,
      "step": 1936
    },
    {
      "epoch": 2.7475177304964538,
      "grad_norm": 1.556890845298767,
      "learning_rate": 1.7336561743341406e-05,
      "loss": 1.4269,
      "step": 1937
    },
    {
      "epoch": 2.748936170212766,
      "grad_norm": 1.6589183807373047,
      "learning_rate": 1.7239709443099274e-05,
      "loss": 1.4834,
      "step": 1938
    },
    {
      "epoch": 2.750354609929078,
      "grad_norm": 1.570211410522461,
      "learning_rate": 1.7142857142857145e-05,
      "loss": 1.5768,
      "step": 1939
    },
    {
      "epoch": 2.75177304964539,
      "grad_norm": 1.5194514989852905,
      "learning_rate": 1.7046004842615013e-05,
      "loss": 1.3136,
      "step": 1940
    },
    {
      "epoch": 2.753191489361702,
      "grad_norm": 1.593564748764038,
      "learning_rate": 1.694915254237288e-05,
      "loss": 1.4541,
      "step": 1941
    },
    {
      "epoch": 2.754609929078014,
      "grad_norm": 1.4744600057601929,
      "learning_rate": 1.685230024213075e-05,
      "loss": 1.4791,
      "step": 1942
    },
    {
      "epoch": 2.7560283687943263,
      "grad_norm": 1.5760258436203003,
      "learning_rate": 1.675544794188862e-05,
      "loss": 1.3582,
      "step": 1943
    },
    {
      "epoch": 2.7574468085106383,
      "grad_norm": 1.5979787111282349,
      "learning_rate": 1.6658595641646492e-05,
      "loss": 1.4844,
      "step": 1944
    },
    {
      "epoch": 2.7588652482269502,
      "grad_norm": 1.5479785203933716,
      "learning_rate": 1.656174334140436e-05,
      "loss": 1.4077,
      "step": 1945
    },
    {
      "epoch": 2.7602836879432626,
      "grad_norm": 1.835115909576416,
      "learning_rate": 1.6464891041162228e-05,
      "loss": 1.6464,
      "step": 1946
    },
    {
      "epoch": 2.7617021276595746,
      "grad_norm": 1.5543209314346313,
      "learning_rate": 1.6368038740920096e-05,
      "loss": 1.3008,
      "step": 1947
    },
    {
      "epoch": 2.7631205673758865,
      "grad_norm": 1.613613486289978,
      "learning_rate": 1.6271186440677967e-05,
      "loss": 1.5523,
      "step": 1948
    },
    {
      "epoch": 2.7645390070921985,
      "grad_norm": 1.5283843278884888,
      "learning_rate": 1.617433414043584e-05,
      "loss": 1.648,
      "step": 1949
    },
    {
      "epoch": 2.7659574468085104,
      "grad_norm": 1.5250712633132935,
      "learning_rate": 1.6077481840193706e-05,
      "loss": 1.3664,
      "step": 1950
    },
    {
      "epoch": 2.767375886524823,
      "grad_norm": 1.463865041732788,
      "learning_rate": 1.5980629539951574e-05,
      "loss": 1.4335,
      "step": 1951
    },
    {
      "epoch": 2.7687943262411348,
      "grad_norm": 1.4463627338409424,
      "learning_rate": 1.5883777239709442e-05,
      "loss": 1.377,
      "step": 1952
    },
    {
      "epoch": 2.7702127659574467,
      "grad_norm": 1.5034431219100952,
      "learning_rate": 1.5786924939467314e-05,
      "loss": 1.4875,
      "step": 1953
    },
    {
      "epoch": 2.771631205673759,
      "grad_norm": 1.4665076732635498,
      "learning_rate": 1.569007263922518e-05,
      "loss": 1.3489,
      "step": 1954
    },
    {
      "epoch": 2.773049645390071,
      "grad_norm": 1.7279284000396729,
      "learning_rate": 1.5593220338983053e-05,
      "loss": 1.444,
      "step": 1955
    },
    {
      "epoch": 2.774468085106383,
      "grad_norm": 1.5522356033325195,
      "learning_rate": 1.549636803874092e-05,
      "loss": 1.4504,
      "step": 1956
    },
    {
      "epoch": 2.775886524822695,
      "grad_norm": 1.572156548500061,
      "learning_rate": 1.539951573849879e-05,
      "loss": 1.4462,
      "step": 1957
    },
    {
      "epoch": 2.777304964539007,
      "grad_norm": 1.7614775896072388,
      "learning_rate": 1.530266343825666e-05,
      "loss": 1.4276,
      "step": 1958
    },
    {
      "epoch": 2.7787234042553193,
      "grad_norm": 1.5270607471466064,
      "learning_rate": 1.5205811138014528e-05,
      "loss": 1.2111,
      "step": 1959
    },
    {
      "epoch": 2.780141843971631,
      "grad_norm": 1.5900001525878906,
      "learning_rate": 1.5108958837772396e-05,
      "loss": 1.457,
      "step": 1960
    },
    {
      "epoch": 2.781560283687943,
      "grad_norm": 1.534622311592102,
      "learning_rate": 1.5012106537530269e-05,
      "loss": 1.5179,
      "step": 1961
    },
    {
      "epoch": 2.7829787234042556,
      "grad_norm": 1.5654330253601074,
      "learning_rate": 1.4915254237288137e-05,
      "loss": 1.3862,
      "step": 1962
    },
    {
      "epoch": 2.7843971631205675,
      "grad_norm": 1.5877127647399902,
      "learning_rate": 1.4818401937046007e-05,
      "loss": 1.4666,
      "step": 1963
    },
    {
      "epoch": 2.7858156028368795,
      "grad_norm": 1.506096601486206,
      "learning_rate": 1.4721549636803875e-05,
      "loss": 1.3521,
      "step": 1964
    },
    {
      "epoch": 2.7872340425531914,
      "grad_norm": 1.5128998756408691,
      "learning_rate": 1.4624697336561744e-05,
      "loss": 1.4959,
      "step": 1965
    },
    {
      "epoch": 2.7886524822695034,
      "grad_norm": 1.4901729822158813,
      "learning_rate": 1.4527845036319612e-05,
      "loss": 1.3494,
      "step": 1966
    },
    {
      "epoch": 2.7900709219858157,
      "grad_norm": 1.3965476751327515,
      "learning_rate": 1.4430992736077484e-05,
      "loss": 1.3505,
      "step": 1967
    },
    {
      "epoch": 2.7914893617021277,
      "grad_norm": 1.4635816812515259,
      "learning_rate": 1.4334140435835353e-05,
      "loss": 1.3023,
      "step": 1968
    },
    {
      "epoch": 2.7929078014184396,
      "grad_norm": 1.6039392948150635,
      "learning_rate": 1.4237288135593221e-05,
      "loss": 1.4077,
      "step": 1969
    },
    {
      "epoch": 2.794326241134752,
      "grad_norm": 1.5198856592178345,
      "learning_rate": 1.414043583535109e-05,
      "loss": 1.2872,
      "step": 1970
    },
    {
      "epoch": 2.795744680851064,
      "grad_norm": 1.569949984550476,
      "learning_rate": 1.4043583535108959e-05,
      "loss": 1.5433,
      "step": 1971
    },
    {
      "epoch": 2.797163120567376,
      "grad_norm": 1.7038146257400513,
      "learning_rate": 1.3946731234866828e-05,
      "loss": 1.4857,
      "step": 1972
    },
    {
      "epoch": 2.798581560283688,
      "grad_norm": 1.4677294492721558,
      "learning_rate": 1.38498789346247e-05,
      "loss": 1.3672,
      "step": 1973
    },
    {
      "epoch": 2.8,
      "grad_norm": 1.528230905532837,
      "learning_rate": 1.3753026634382568e-05,
      "loss": 1.1773,
      "step": 1974
    },
    {
      "epoch": 2.801418439716312,
      "grad_norm": 1.4702883958816528,
      "learning_rate": 1.3656174334140437e-05,
      "loss": 1.3146,
      "step": 1975
    },
    {
      "epoch": 2.802836879432624,
      "grad_norm": 1.475071668624878,
      "learning_rate": 1.3559322033898305e-05,
      "loss": 1.4224,
      "step": 1976
    },
    {
      "epoch": 2.804255319148936,
      "grad_norm": 1.5094003677368164,
      "learning_rate": 1.3462469733656175e-05,
      "loss": 1.2817,
      "step": 1977
    },
    {
      "epoch": 2.8056737588652485,
      "grad_norm": 1.4732897281646729,
      "learning_rate": 1.3365617433414043e-05,
      "loss": 1.3307,
      "step": 1978
    },
    {
      "epoch": 2.8070921985815604,
      "grad_norm": 1.487817406654358,
      "learning_rate": 1.3268765133171914e-05,
      "loss": 1.3803,
      "step": 1979
    },
    {
      "epoch": 2.8085106382978724,
      "grad_norm": 1.3788261413574219,
      "learning_rate": 1.3171912832929784e-05,
      "loss": 1.3018,
      "step": 1980
    },
    {
      "epoch": 2.8099290780141843,
      "grad_norm": 1.5145971775054932,
      "learning_rate": 1.3075060532687652e-05,
      "loss": 1.4128,
      "step": 1981
    },
    {
      "epoch": 2.8113475177304963,
      "grad_norm": 1.4556264877319336,
      "learning_rate": 1.2978208232445521e-05,
      "loss": 1.4876,
      "step": 1982
    },
    {
      "epoch": 2.8127659574468087,
      "grad_norm": 1.5635709762573242,
      "learning_rate": 1.288135593220339e-05,
      "loss": 1.5321,
      "step": 1983
    },
    {
      "epoch": 2.8141843971631206,
      "grad_norm": 1.473644495010376,
      "learning_rate": 1.2784503631961259e-05,
      "loss": 1.5235,
      "step": 1984
    },
    {
      "epoch": 2.8156028368794326,
      "grad_norm": 1.460196614265442,
      "learning_rate": 1.2687651331719127e-05,
      "loss": 1.355,
      "step": 1985
    },
    {
      "epoch": 2.8170212765957445,
      "grad_norm": 1.5049656629562378,
      "learning_rate": 1.2590799031476998e-05,
      "loss": 1.2911,
      "step": 1986
    },
    {
      "epoch": 2.8184397163120565,
      "grad_norm": 1.5519764423370361,
      "learning_rate": 1.2493946731234868e-05,
      "loss": 1.4888,
      "step": 1987
    },
    {
      "epoch": 2.819858156028369,
      "grad_norm": 1.4562963247299194,
      "learning_rate": 1.2397094430992736e-05,
      "loss": 1.5431,
      "step": 1988
    },
    {
      "epoch": 2.821276595744681,
      "grad_norm": 1.4702503681182861,
      "learning_rate": 1.2300242130750606e-05,
      "loss": 1.4423,
      "step": 1989
    },
    {
      "epoch": 2.8226950354609928,
      "grad_norm": 1.56186842918396,
      "learning_rate": 1.2203389830508475e-05,
      "loss": 1.6492,
      "step": 1990
    },
    {
      "epoch": 2.824113475177305,
      "grad_norm": 1.567550778388977,
      "learning_rate": 1.2106537530266345e-05,
      "loss": 1.3984,
      "step": 1991
    },
    {
      "epoch": 2.825531914893617,
      "grad_norm": 1.7098172903060913,
      "learning_rate": 1.2009685230024213e-05,
      "loss": 1.4965,
      "step": 1992
    },
    {
      "epoch": 2.826950354609929,
      "grad_norm": 1.5820444822311401,
      "learning_rate": 1.1912832929782084e-05,
      "loss": 1.3382,
      "step": 1993
    },
    {
      "epoch": 2.828368794326241,
      "grad_norm": 1.5530543327331543,
      "learning_rate": 1.1815980629539952e-05,
      "loss": 1.6504,
      "step": 1994
    },
    {
      "epoch": 2.829787234042553,
      "grad_norm": 1.5048547983169556,
      "learning_rate": 1.1719128329297822e-05,
      "loss": 1.4001,
      "step": 1995
    },
    {
      "epoch": 2.8312056737588653,
      "grad_norm": 1.5266296863555908,
      "learning_rate": 1.1622276029055691e-05,
      "loss": 1.3385,
      "step": 1996
    },
    {
      "epoch": 2.8326241134751773,
      "grad_norm": 1.6101033687591553,
      "learning_rate": 1.152542372881356e-05,
      "loss": 1.4735,
      "step": 1997
    },
    {
      "epoch": 2.8340425531914892,
      "grad_norm": 1.6362112760543823,
      "learning_rate": 1.1428571428571429e-05,
      "loss": 1.4166,
      "step": 1998
    },
    {
      "epoch": 2.8354609929078016,
      "grad_norm": 1.4422473907470703,
      "learning_rate": 1.1331719128329299e-05,
      "loss": 1.3548,
      "step": 1999
    },
    {
      "epoch": 2.8368794326241136,
      "grad_norm": 1.4494578838348389,
      "learning_rate": 1.1234866828087168e-05,
      "loss": 1.3912,
      "step": 2000
    },
    {
      "epoch": 2.8368794326241136,
      "eval_loss": 1.8083090782165527,
      "eval_runtime": 90.4314,
      "eval_samples_per_second": 15.592,
      "eval_steps_per_second": 7.796,
      "step": 2000
    }
  ],
  "logging_steps": 1,
  "max_steps": 2115,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 5265512001880320.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
