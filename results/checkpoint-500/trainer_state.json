{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.7092198581560284,
  "eval_steps": 500,
  "global_step": 500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0014184397163120568,
      "grad_norm": 4.663888931274414,
      "learning_rate": 0.0,
      "loss": 3.363,
      "step": 1
    },
    {
      "epoch": 0.0028368794326241137,
      "grad_norm": 4.326501846313477,
      "learning_rate": 4.000000000000001e-06,
      "loss": 3.1448,
      "step": 2
    },
    {
      "epoch": 0.00425531914893617,
      "grad_norm": 5.150259494781494,
      "learning_rate": 8.000000000000001e-06,
      "loss": 3.1434,
      "step": 3
    },
    {
      "epoch": 0.005673758865248227,
      "grad_norm": 5.390714645385742,
      "learning_rate": 1.2e-05,
      "loss": 3.3289,
      "step": 4
    },
    {
      "epoch": 0.0070921985815602835,
      "grad_norm": 4.865074634552002,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 3.0366,
      "step": 5
    },
    {
      "epoch": 0.00851063829787234,
      "grad_norm": 4.287224769592285,
      "learning_rate": 2e-05,
      "loss": 3.1406,
      "step": 6
    },
    {
      "epoch": 0.009929078014184398,
      "grad_norm": 4.218326568603516,
      "learning_rate": 2.4e-05,
      "loss": 3.1917,
      "step": 7
    },
    {
      "epoch": 0.011347517730496455,
      "grad_norm": 3.5314183235168457,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 2.882,
      "step": 8
    },
    {
      "epoch": 0.01276595744680851,
      "grad_norm": 3.696972370147705,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 2.8056,
      "step": 9
    },
    {
      "epoch": 0.014184397163120567,
      "grad_norm": 3.214474678039551,
      "learning_rate": 3.6e-05,
      "loss": 3.2135,
      "step": 10
    },
    {
      "epoch": 0.015602836879432624,
      "grad_norm": 3.480194330215454,
      "learning_rate": 4e-05,
      "loss": 2.6706,
      "step": 11
    },
    {
      "epoch": 0.01702127659574468,
      "grad_norm": 3.2293241024017334,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 2.577,
      "step": 12
    },
    {
      "epoch": 0.018439716312056736,
      "grad_norm": 3.4259519577026367,
      "learning_rate": 4.8e-05,
      "loss": 2.8119,
      "step": 13
    },
    {
      "epoch": 0.019858156028368795,
      "grad_norm": 2.966158151626587,
      "learning_rate": 5.2000000000000004e-05,
      "loss": 2.6788,
      "step": 14
    },
    {
      "epoch": 0.02127659574468085,
      "grad_norm": 3.072213888168335,
      "learning_rate": 5.6000000000000006e-05,
      "loss": 2.3474,
      "step": 15
    },
    {
      "epoch": 0.02269503546099291,
      "grad_norm": 3.135129690170288,
      "learning_rate": 6e-05,
      "loss": 2.2081,
      "step": 16
    },
    {
      "epoch": 0.024113475177304965,
      "grad_norm": 2.8877358436584473,
      "learning_rate": 6.400000000000001e-05,
      "loss": 2.486,
      "step": 17
    },
    {
      "epoch": 0.02553191489361702,
      "grad_norm": 1.3276922702789307,
      "learning_rate": 6.800000000000001e-05,
      "loss": 2.277,
      "step": 18
    },
    {
      "epoch": 0.02695035460992908,
      "grad_norm": 1.734344482421875,
      "learning_rate": 7.2e-05,
      "loss": 2.3077,
      "step": 19
    },
    {
      "epoch": 0.028368794326241134,
      "grad_norm": 1.927562952041626,
      "learning_rate": 7.6e-05,
      "loss": 2.2205,
      "step": 20
    },
    {
      "epoch": 0.029787234042553193,
      "grad_norm": 1.658927321434021,
      "learning_rate": 8e-05,
      "loss": 2.2476,
      "step": 21
    },
    {
      "epoch": 0.031205673758865248,
      "grad_norm": 1.7990249395370483,
      "learning_rate": 8.4e-05,
      "loss": 2.4956,
      "step": 22
    },
    {
      "epoch": 0.032624113475177303,
      "grad_norm": 1.2179522514343262,
      "learning_rate": 8.800000000000001e-05,
      "loss": 2.3486,
      "step": 23
    },
    {
      "epoch": 0.03404255319148936,
      "grad_norm": 1.2065980434417725,
      "learning_rate": 9.200000000000001e-05,
      "loss": 2.1772,
      "step": 24
    },
    {
      "epoch": 0.03546099290780142,
      "grad_norm": 1.3824200630187988,
      "learning_rate": 9.6e-05,
      "loss": 2.215,
      "step": 25
    },
    {
      "epoch": 0.03687943262411347,
      "grad_norm": 1.2284038066864014,
      "learning_rate": 0.0001,
      "loss": 2.1544,
      "step": 26
    },
    {
      "epoch": 0.03829787234042553,
      "grad_norm": 1.2294367551803589,
      "learning_rate": 0.00010400000000000001,
      "loss": 2.1664,
      "step": 27
    },
    {
      "epoch": 0.03971631205673759,
      "grad_norm": 1.2366228103637695,
      "learning_rate": 0.00010800000000000001,
      "loss": 2.06,
      "step": 28
    },
    {
      "epoch": 0.04113475177304964,
      "grad_norm": 1.154578685760498,
      "learning_rate": 0.00011200000000000001,
      "loss": 2.1996,
      "step": 29
    },
    {
      "epoch": 0.0425531914893617,
      "grad_norm": 1.2831920385360718,
      "learning_rate": 0.000116,
      "loss": 1.9245,
      "step": 30
    },
    {
      "epoch": 0.04397163120567376,
      "grad_norm": 1.2155778408050537,
      "learning_rate": 0.00012,
      "loss": 2.0609,
      "step": 31
    },
    {
      "epoch": 0.04539007092198582,
      "grad_norm": 1.2330687046051025,
      "learning_rate": 0.000124,
      "loss": 2.1224,
      "step": 32
    },
    {
      "epoch": 0.04680851063829787,
      "grad_norm": 1.2099401950836182,
      "learning_rate": 0.00012800000000000002,
      "loss": 2.122,
      "step": 33
    },
    {
      "epoch": 0.04822695035460993,
      "grad_norm": 1.0961740016937256,
      "learning_rate": 0.000132,
      "loss": 2.0189,
      "step": 34
    },
    {
      "epoch": 0.04964539007092199,
      "grad_norm": 1.2576420307159424,
      "learning_rate": 0.00013600000000000003,
      "loss": 2.0353,
      "step": 35
    },
    {
      "epoch": 0.05106382978723404,
      "grad_norm": 1.195496678352356,
      "learning_rate": 0.00014,
      "loss": 2.0332,
      "step": 36
    },
    {
      "epoch": 0.0524822695035461,
      "grad_norm": 1.218315839767456,
      "learning_rate": 0.000144,
      "loss": 2.0501,
      "step": 37
    },
    {
      "epoch": 0.05390070921985816,
      "grad_norm": 1.1647756099700928,
      "learning_rate": 0.000148,
      "loss": 2.2139,
      "step": 38
    },
    {
      "epoch": 0.05531914893617021,
      "grad_norm": 1.20754873752594,
      "learning_rate": 0.000152,
      "loss": 2.2591,
      "step": 39
    },
    {
      "epoch": 0.05673758865248227,
      "grad_norm": 1.0971544981002808,
      "learning_rate": 0.00015600000000000002,
      "loss": 2.273,
      "step": 40
    },
    {
      "epoch": 0.05815602836879433,
      "grad_norm": 1.2460731267929077,
      "learning_rate": 0.00016,
      "loss": 2.2102,
      "step": 41
    },
    {
      "epoch": 0.059574468085106386,
      "grad_norm": 1.1251200437545776,
      "learning_rate": 0.000164,
      "loss": 2.1836,
      "step": 42
    },
    {
      "epoch": 0.06099290780141844,
      "grad_norm": 1.149972677230835,
      "learning_rate": 0.000168,
      "loss": 1.8653,
      "step": 43
    },
    {
      "epoch": 0.062411347517730496,
      "grad_norm": 1.097387671470642,
      "learning_rate": 0.000172,
      "loss": 2.2198,
      "step": 44
    },
    {
      "epoch": 0.06382978723404255,
      "grad_norm": 1.1107244491577148,
      "learning_rate": 0.00017600000000000002,
      "loss": 2.1135,
      "step": 45
    },
    {
      "epoch": 0.06524822695035461,
      "grad_norm": 1.0508993864059448,
      "learning_rate": 0.00018,
      "loss": 1.9834,
      "step": 46
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 1.1061515808105469,
      "learning_rate": 0.00018400000000000003,
      "loss": 2.0214,
      "step": 47
    },
    {
      "epoch": 0.06808510638297872,
      "grad_norm": 1.055652141571045,
      "learning_rate": 0.000188,
      "loss": 2.0456,
      "step": 48
    },
    {
      "epoch": 0.06950354609929078,
      "grad_norm": 1.0564075708389282,
      "learning_rate": 0.000192,
      "loss": 1.8642,
      "step": 49
    },
    {
      "epoch": 0.07092198581560284,
      "grad_norm": 1.03203547000885,
      "learning_rate": 0.000196,
      "loss": 1.714,
      "step": 50
    },
    {
      "epoch": 0.07234042553191489,
      "grad_norm": 1.1104342937469482,
      "learning_rate": 0.0002,
      "loss": 2.0922,
      "step": 51
    },
    {
      "epoch": 0.07375886524822695,
      "grad_norm": 1.0160233974456787,
      "learning_rate": 0.0001999031476997579,
      "loss": 1.9706,
      "step": 52
    },
    {
      "epoch": 0.075177304964539,
      "grad_norm": 0.9867855310440063,
      "learning_rate": 0.00019980629539951574,
      "loss": 1.9446,
      "step": 53
    },
    {
      "epoch": 0.07659574468085106,
      "grad_norm": 1.0777437686920166,
      "learning_rate": 0.00019970944309927362,
      "loss": 1.8683,
      "step": 54
    },
    {
      "epoch": 0.07801418439716312,
      "grad_norm": 1.0760366916656494,
      "learning_rate": 0.0001996125907990315,
      "loss": 1.8404,
      "step": 55
    },
    {
      "epoch": 0.07943262411347518,
      "grad_norm": 1.1468721628189087,
      "learning_rate": 0.00019951573849878935,
      "loss": 1.9124,
      "step": 56
    },
    {
      "epoch": 0.08085106382978724,
      "grad_norm": 1.0046629905700684,
      "learning_rate": 0.00019941888619854722,
      "loss": 1.9508,
      "step": 57
    },
    {
      "epoch": 0.08226950354609928,
      "grad_norm": 1.1077072620391846,
      "learning_rate": 0.0001993220338983051,
      "loss": 1.966,
      "step": 58
    },
    {
      "epoch": 0.08368794326241134,
      "grad_norm": 1.161176085472107,
      "learning_rate": 0.00019922518159806295,
      "loss": 1.7889,
      "step": 59
    },
    {
      "epoch": 0.0851063829787234,
      "grad_norm": 1.0874756574630737,
      "learning_rate": 0.00019912832929782083,
      "loss": 1.9527,
      "step": 60
    },
    {
      "epoch": 0.08652482269503546,
      "grad_norm": 1.1886991262435913,
      "learning_rate": 0.00019903147699757868,
      "loss": 1.846,
      "step": 61
    },
    {
      "epoch": 0.08794326241134752,
      "grad_norm": 0.9369897246360779,
      "learning_rate": 0.00019893462469733656,
      "loss": 1.6584,
      "step": 62
    },
    {
      "epoch": 0.08936170212765958,
      "grad_norm": 1.0621529817581177,
      "learning_rate": 0.00019883777239709444,
      "loss": 1.8865,
      "step": 63
    },
    {
      "epoch": 0.09078014184397164,
      "grad_norm": 0.9763602614402771,
      "learning_rate": 0.0001987409200968523,
      "loss": 1.8334,
      "step": 64
    },
    {
      "epoch": 0.09219858156028368,
      "grad_norm": 1.0840765237808228,
      "learning_rate": 0.00019864406779661017,
      "loss": 2.0674,
      "step": 65
    },
    {
      "epoch": 0.09361702127659574,
      "grad_norm": 0.9588376879692078,
      "learning_rate": 0.00019854721549636805,
      "loss": 1.8821,
      "step": 66
    },
    {
      "epoch": 0.0950354609929078,
      "grad_norm": 0.9782182574272156,
      "learning_rate": 0.0001984503631961259,
      "loss": 1.8329,
      "step": 67
    },
    {
      "epoch": 0.09645390070921986,
      "grad_norm": 1.0978199243545532,
      "learning_rate": 0.00019835351089588377,
      "loss": 1.7789,
      "step": 68
    },
    {
      "epoch": 0.09787234042553192,
      "grad_norm": 1.2051382064819336,
      "learning_rate": 0.00019825665859564165,
      "loss": 1.8647,
      "step": 69
    },
    {
      "epoch": 0.09929078014184398,
      "grad_norm": 1.075136661529541,
      "learning_rate": 0.00019815980629539953,
      "loss": 1.8156,
      "step": 70
    },
    {
      "epoch": 0.10070921985815603,
      "grad_norm": 1.114687204360962,
      "learning_rate": 0.0001980629539951574,
      "loss": 2.0969,
      "step": 71
    },
    {
      "epoch": 0.10212765957446808,
      "grad_norm": 1.0886708498001099,
      "learning_rate": 0.00019796610169491526,
      "loss": 1.8735,
      "step": 72
    },
    {
      "epoch": 0.10354609929078014,
      "grad_norm": 1.0000802278518677,
      "learning_rate": 0.00019786924939467314,
      "loss": 1.7809,
      "step": 73
    },
    {
      "epoch": 0.1049645390070922,
      "grad_norm": 1.0024892091751099,
      "learning_rate": 0.00019777239709443102,
      "loss": 1.8654,
      "step": 74
    },
    {
      "epoch": 0.10638297872340426,
      "grad_norm": 1.0471751689910889,
      "learning_rate": 0.00019767554479418887,
      "loss": 1.8414,
      "step": 75
    },
    {
      "epoch": 0.10780141843971631,
      "grad_norm": 1.0259408950805664,
      "learning_rate": 0.00019757869249394675,
      "loss": 1.8887,
      "step": 76
    },
    {
      "epoch": 0.10921985815602837,
      "grad_norm": 1.0418591499328613,
      "learning_rate": 0.00019748184019370462,
      "loss": 1.9078,
      "step": 77
    },
    {
      "epoch": 0.11063829787234042,
      "grad_norm": 1.106501579284668,
      "learning_rate": 0.00019738498789346247,
      "loss": 1.9529,
      "step": 78
    },
    {
      "epoch": 0.11205673758865248,
      "grad_norm": 1.2001898288726807,
      "learning_rate": 0.00019728813559322035,
      "loss": 1.8873,
      "step": 79
    },
    {
      "epoch": 0.11347517730496454,
      "grad_norm": 1.1186670064926147,
      "learning_rate": 0.00019719128329297823,
      "loss": 1.9769,
      "step": 80
    },
    {
      "epoch": 0.1148936170212766,
      "grad_norm": 1.0681841373443604,
      "learning_rate": 0.00019709443099273608,
      "loss": 1.932,
      "step": 81
    },
    {
      "epoch": 0.11631205673758865,
      "grad_norm": 1.085942029953003,
      "learning_rate": 0.00019699757869249396,
      "loss": 1.9425,
      "step": 82
    },
    {
      "epoch": 0.11773049645390071,
      "grad_norm": 1.0608960390090942,
      "learning_rate": 0.00019690072639225184,
      "loss": 1.8147,
      "step": 83
    },
    {
      "epoch": 0.11914893617021277,
      "grad_norm": 0.9925984144210815,
      "learning_rate": 0.0001968038740920097,
      "loss": 1.6996,
      "step": 84
    },
    {
      "epoch": 0.12056737588652482,
      "grad_norm": 1.0561087131500244,
      "learning_rate": 0.00019670702179176757,
      "loss": 1.9734,
      "step": 85
    },
    {
      "epoch": 0.12198581560283688,
      "grad_norm": 0.9507638812065125,
      "learning_rate": 0.00019661016949152545,
      "loss": 1.8695,
      "step": 86
    },
    {
      "epoch": 0.12340425531914893,
      "grad_norm": 1.1066452264785767,
      "learning_rate": 0.0001965133171912833,
      "loss": 1.8306,
      "step": 87
    },
    {
      "epoch": 0.12482269503546099,
      "grad_norm": 1.409095287322998,
      "learning_rate": 0.00019641646489104117,
      "loss": 1.9217,
      "step": 88
    },
    {
      "epoch": 0.12624113475177304,
      "grad_norm": 1.0073235034942627,
      "learning_rate": 0.00019631961259079905,
      "loss": 1.871,
      "step": 89
    },
    {
      "epoch": 0.1276595744680851,
      "grad_norm": 1.1069413423538208,
      "learning_rate": 0.0001962227602905569,
      "loss": 2.2092,
      "step": 90
    },
    {
      "epoch": 0.12907801418439716,
      "grad_norm": 0.9206241369247437,
      "learning_rate": 0.00019612590799031478,
      "loss": 1.5993,
      "step": 91
    },
    {
      "epoch": 0.13049645390070921,
      "grad_norm": 0.971748948097229,
      "learning_rate": 0.00019602905569007263,
      "loss": 1.9343,
      "step": 92
    },
    {
      "epoch": 0.13191489361702127,
      "grad_norm": 0.9606374502182007,
      "learning_rate": 0.0001959322033898305,
      "loss": 1.988,
      "step": 93
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 0.992902934551239,
      "learning_rate": 0.0001958353510895884,
      "loss": 1.6357,
      "step": 94
    },
    {
      "epoch": 0.1347517730496454,
      "grad_norm": 0.991494357585907,
      "learning_rate": 0.00019573849878934624,
      "loss": 1.929,
      "step": 95
    },
    {
      "epoch": 0.13617021276595745,
      "grad_norm": 0.9323453903198242,
      "learning_rate": 0.00019564164648910412,
      "loss": 1.9671,
      "step": 96
    },
    {
      "epoch": 0.1375886524822695,
      "grad_norm": 1.1381146907806396,
      "learning_rate": 0.000195544794188862,
      "loss": 1.6889,
      "step": 97
    },
    {
      "epoch": 0.13900709219858157,
      "grad_norm": 0.942345380783081,
      "learning_rate": 0.00019544794188861985,
      "loss": 1.8009,
      "step": 98
    },
    {
      "epoch": 0.14042553191489363,
      "grad_norm": 1.1215462684631348,
      "learning_rate": 0.00019535108958837773,
      "loss": 1.9217,
      "step": 99
    },
    {
      "epoch": 0.14184397163120568,
      "grad_norm": 1.0809898376464844,
      "learning_rate": 0.0001952542372881356,
      "loss": 2.0826,
      "step": 100
    },
    {
      "epoch": 0.14326241134751774,
      "grad_norm": 0.9221402406692505,
      "learning_rate": 0.00019515738498789345,
      "loss": 1.8363,
      "step": 101
    },
    {
      "epoch": 0.14468085106382977,
      "grad_norm": 1.1425656080245972,
      "learning_rate": 0.00019506053268765133,
      "loss": 1.9013,
      "step": 102
    },
    {
      "epoch": 0.14609929078014183,
      "grad_norm": 0.9840394258499146,
      "learning_rate": 0.0001949636803874092,
      "loss": 1.8085,
      "step": 103
    },
    {
      "epoch": 0.1475177304964539,
      "grad_norm": 1.1152970790863037,
      "learning_rate": 0.00019486682808716706,
      "loss": 2.0811,
      "step": 104
    },
    {
      "epoch": 0.14893617021276595,
      "grad_norm": 0.9925697445869446,
      "learning_rate": 0.00019476997578692494,
      "loss": 1.7898,
      "step": 105
    },
    {
      "epoch": 0.150354609929078,
      "grad_norm": 0.9788102507591248,
      "learning_rate": 0.00019467312348668282,
      "loss": 1.8498,
      "step": 106
    },
    {
      "epoch": 0.15177304964539007,
      "grad_norm": 1.0104581117630005,
      "learning_rate": 0.0001945762711864407,
      "loss": 1.7161,
      "step": 107
    },
    {
      "epoch": 0.15319148936170213,
      "grad_norm": 1.0371237993240356,
      "learning_rate": 0.00019447941888619857,
      "loss": 2.1101,
      "step": 108
    },
    {
      "epoch": 0.15460992907801419,
      "grad_norm": 1.0925321578979492,
      "learning_rate": 0.00019438256658595643,
      "loss": 2.0603,
      "step": 109
    },
    {
      "epoch": 0.15602836879432624,
      "grad_norm": 0.9612079858779907,
      "learning_rate": 0.0001942857142857143,
      "loss": 1.8853,
      "step": 110
    },
    {
      "epoch": 0.1574468085106383,
      "grad_norm": 1.045314073562622,
      "learning_rate": 0.00019418886198547218,
      "loss": 1.715,
      "step": 111
    },
    {
      "epoch": 0.15886524822695036,
      "grad_norm": 0.9515319466590881,
      "learning_rate": 0.00019409200968523003,
      "loss": 1.9728,
      "step": 112
    },
    {
      "epoch": 0.16028368794326242,
      "grad_norm": 0.9624865055084229,
      "learning_rate": 0.0001939951573849879,
      "loss": 1.9609,
      "step": 113
    },
    {
      "epoch": 0.16170212765957448,
      "grad_norm": 1.0090194940567017,
      "learning_rate": 0.0001938983050847458,
      "loss": 1.9138,
      "step": 114
    },
    {
      "epoch": 0.16312056737588654,
      "grad_norm": 0.997510552406311,
      "learning_rate": 0.00019380145278450364,
      "loss": 1.8258,
      "step": 115
    },
    {
      "epoch": 0.16453900709219857,
      "grad_norm": 1.1058714389801025,
      "learning_rate": 0.00019370460048426152,
      "loss": 1.9574,
      "step": 116
    },
    {
      "epoch": 0.16595744680851063,
      "grad_norm": 0.9852263927459717,
      "learning_rate": 0.0001936077481840194,
      "loss": 1.9388,
      "step": 117
    },
    {
      "epoch": 0.1673758865248227,
      "grad_norm": 1.0362639427185059,
      "learning_rate": 0.00019351089588377725,
      "loss": 1.9979,
      "step": 118
    },
    {
      "epoch": 0.16879432624113475,
      "grad_norm": 0.9582896828651428,
      "learning_rate": 0.00019341404358353513,
      "loss": 1.9451,
      "step": 119
    },
    {
      "epoch": 0.1702127659574468,
      "grad_norm": 1.2235652208328247,
      "learning_rate": 0.000193317191283293,
      "loss": 1.9965,
      "step": 120
    },
    {
      "epoch": 0.17163120567375886,
      "grad_norm": 0.9667586088180542,
      "learning_rate": 0.00019322033898305085,
      "loss": 2.0728,
      "step": 121
    },
    {
      "epoch": 0.17304964539007092,
      "grad_norm": 0.9890563488006592,
      "learning_rate": 0.00019312348668280873,
      "loss": 1.9984,
      "step": 122
    },
    {
      "epoch": 0.17446808510638298,
      "grad_norm": 0.9136565923690796,
      "learning_rate": 0.00019302663438256658,
      "loss": 1.9968,
      "step": 123
    },
    {
      "epoch": 0.17588652482269504,
      "grad_norm": 1.1284838914871216,
      "learning_rate": 0.00019292978208232446,
      "loss": 2.0538,
      "step": 124
    },
    {
      "epoch": 0.1773049645390071,
      "grad_norm": 0.9669015407562256,
      "learning_rate": 0.00019283292978208234,
      "loss": 1.9323,
      "step": 125
    },
    {
      "epoch": 0.17872340425531916,
      "grad_norm": 0.9495101571083069,
      "learning_rate": 0.0001927360774818402,
      "loss": 1.9049,
      "step": 126
    },
    {
      "epoch": 0.18014184397163122,
      "grad_norm": 0.9509551525115967,
      "learning_rate": 0.00019263922518159807,
      "loss": 1.9258,
      "step": 127
    },
    {
      "epoch": 0.18156028368794327,
      "grad_norm": 0.9739115834236145,
      "learning_rate": 0.00019254237288135595,
      "loss": 1.7723,
      "step": 128
    },
    {
      "epoch": 0.1829787234042553,
      "grad_norm": 0.9413913488388062,
      "learning_rate": 0.0001924455205811138,
      "loss": 1.8601,
      "step": 129
    },
    {
      "epoch": 0.18439716312056736,
      "grad_norm": 1.0929319858551025,
      "learning_rate": 0.00019234866828087168,
      "loss": 2.0404,
      "step": 130
    },
    {
      "epoch": 0.18581560283687942,
      "grad_norm": 0.9712840914726257,
      "learning_rate": 0.00019225181598062955,
      "loss": 1.6331,
      "step": 131
    },
    {
      "epoch": 0.18723404255319148,
      "grad_norm": 1.0864912271499634,
      "learning_rate": 0.0001921549636803874,
      "loss": 1.8071,
      "step": 132
    },
    {
      "epoch": 0.18865248226950354,
      "grad_norm": 1.011907696723938,
      "learning_rate": 0.00019205811138014528,
      "loss": 1.7877,
      "step": 133
    },
    {
      "epoch": 0.1900709219858156,
      "grad_norm": 1.0717408657073975,
      "learning_rate": 0.00019196125907990316,
      "loss": 2.0539,
      "step": 134
    },
    {
      "epoch": 0.19148936170212766,
      "grad_norm": 0.9419696927070618,
      "learning_rate": 0.000191864406779661,
      "loss": 1.6857,
      "step": 135
    },
    {
      "epoch": 0.19290780141843972,
      "grad_norm": 1.008816123008728,
      "learning_rate": 0.0001917675544794189,
      "loss": 1.7538,
      "step": 136
    },
    {
      "epoch": 0.19432624113475178,
      "grad_norm": 0.954567551612854,
      "learning_rate": 0.00019167070217917677,
      "loss": 1.7457,
      "step": 137
    },
    {
      "epoch": 0.19574468085106383,
      "grad_norm": 1.0287903547286987,
      "learning_rate": 0.00019157384987893462,
      "loss": 1.8841,
      "step": 138
    },
    {
      "epoch": 0.1971631205673759,
      "grad_norm": 1.0585139989852905,
      "learning_rate": 0.0001914769975786925,
      "loss": 1.831,
      "step": 139
    },
    {
      "epoch": 0.19858156028368795,
      "grad_norm": 0.8999666571617126,
      "learning_rate": 0.00019138014527845035,
      "loss": 1.7679,
      "step": 140
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.9993060231208801,
      "learning_rate": 0.00019128329297820823,
      "loss": 1.7601,
      "step": 141
    },
    {
      "epoch": 0.20141843971631207,
      "grad_norm": 1.0174119472503662,
      "learning_rate": 0.0001911864406779661,
      "loss": 2.0544,
      "step": 142
    },
    {
      "epoch": 0.2028368794326241,
      "grad_norm": 0.9215055704116821,
      "learning_rate": 0.00019108958837772398,
      "loss": 1.7231,
      "step": 143
    },
    {
      "epoch": 0.20425531914893616,
      "grad_norm": 1.06234610080719,
      "learning_rate": 0.00019099273607748186,
      "loss": 1.9472,
      "step": 144
    },
    {
      "epoch": 0.20567375886524822,
      "grad_norm": 0.9426120519638062,
      "learning_rate": 0.0001908958837772397,
      "loss": 1.8451,
      "step": 145
    },
    {
      "epoch": 0.20709219858156028,
      "grad_norm": 0.9534973502159119,
      "learning_rate": 0.0001907990314769976,
      "loss": 1.9874,
      "step": 146
    },
    {
      "epoch": 0.20851063829787234,
      "grad_norm": 0.9999266266822815,
      "learning_rate": 0.00019070217917675547,
      "loss": 1.9135,
      "step": 147
    },
    {
      "epoch": 0.2099290780141844,
      "grad_norm": 0.9851370453834534,
      "learning_rate": 0.00019060532687651335,
      "loss": 1.9419,
      "step": 148
    },
    {
      "epoch": 0.21134751773049645,
      "grad_norm": 1.1135103702545166,
      "learning_rate": 0.0001905084745762712,
      "loss": 2.2241,
      "step": 149
    },
    {
      "epoch": 0.2127659574468085,
      "grad_norm": 1.024118423461914,
      "learning_rate": 0.00019041162227602908,
      "loss": 1.8521,
      "step": 150
    },
    {
      "epoch": 0.21418439716312057,
      "grad_norm": 0.9475961923599243,
      "learning_rate": 0.00019031476997578695,
      "loss": 1.8281,
      "step": 151
    },
    {
      "epoch": 0.21560283687943263,
      "grad_norm": 1.0830246210098267,
      "learning_rate": 0.0001902179176755448,
      "loss": 2.0724,
      "step": 152
    },
    {
      "epoch": 0.2170212765957447,
      "grad_norm": 1.0670086145401,
      "learning_rate": 0.00019012106537530268,
      "loss": 2.0665,
      "step": 153
    },
    {
      "epoch": 0.21843971631205675,
      "grad_norm": 0.9611037969589233,
      "learning_rate": 0.00019002421307506053,
      "loss": 1.8794,
      "step": 154
    },
    {
      "epoch": 0.2198581560283688,
      "grad_norm": 0.9807606935501099,
      "learning_rate": 0.0001899273607748184,
      "loss": 1.7585,
      "step": 155
    },
    {
      "epoch": 0.22127659574468084,
      "grad_norm": 0.89825040102005,
      "learning_rate": 0.0001898305084745763,
      "loss": 1.715,
      "step": 156
    },
    {
      "epoch": 0.2226950354609929,
      "grad_norm": 1.0388842821121216,
      "learning_rate": 0.00018973365617433414,
      "loss": 2.0931,
      "step": 157
    },
    {
      "epoch": 0.22411347517730495,
      "grad_norm": 0.9821796417236328,
      "learning_rate": 0.00018963680387409202,
      "loss": 1.8478,
      "step": 158
    },
    {
      "epoch": 0.225531914893617,
      "grad_norm": 0.9460282921791077,
      "learning_rate": 0.0001895399515738499,
      "loss": 1.913,
      "step": 159
    },
    {
      "epoch": 0.22695035460992907,
      "grad_norm": 0.9254499673843384,
      "learning_rate": 0.00018944309927360775,
      "loss": 1.9416,
      "step": 160
    },
    {
      "epoch": 0.22836879432624113,
      "grad_norm": 0.980393648147583,
      "learning_rate": 0.00018934624697336563,
      "loss": 1.8052,
      "step": 161
    },
    {
      "epoch": 0.2297872340425532,
      "grad_norm": 0.9918868541717529,
      "learning_rate": 0.0001892493946731235,
      "loss": 1.9824,
      "step": 162
    },
    {
      "epoch": 0.23120567375886525,
      "grad_norm": 0.9574214816093445,
      "learning_rate": 0.00018915254237288136,
      "loss": 1.7779,
      "step": 163
    },
    {
      "epoch": 0.2326241134751773,
      "grad_norm": 0.955186128616333,
      "learning_rate": 0.00018905569007263923,
      "loss": 1.7754,
      "step": 164
    },
    {
      "epoch": 0.23404255319148937,
      "grad_norm": 1.004132866859436,
      "learning_rate": 0.0001889588377723971,
      "loss": 1.883,
      "step": 165
    },
    {
      "epoch": 0.23546099290780143,
      "grad_norm": 0.8997734785079956,
      "learning_rate": 0.00018886198547215496,
      "loss": 2.0024,
      "step": 166
    },
    {
      "epoch": 0.23687943262411348,
      "grad_norm": 0.9180673956871033,
      "learning_rate": 0.00018876513317191284,
      "loss": 1.8712,
      "step": 167
    },
    {
      "epoch": 0.23829787234042554,
      "grad_norm": 1.0329842567443848,
      "learning_rate": 0.00018866828087167072,
      "loss": 2.1347,
      "step": 168
    },
    {
      "epoch": 0.2397163120567376,
      "grad_norm": 1.0167003870010376,
      "learning_rate": 0.00018857142857142857,
      "loss": 1.8339,
      "step": 169
    },
    {
      "epoch": 0.24113475177304963,
      "grad_norm": 1.0670790672302246,
      "learning_rate": 0.00018847457627118645,
      "loss": 1.9222,
      "step": 170
    },
    {
      "epoch": 0.2425531914893617,
      "grad_norm": 0.9621996879577637,
      "learning_rate": 0.0001883777239709443,
      "loss": 1.9693,
      "step": 171
    },
    {
      "epoch": 0.24397163120567375,
      "grad_norm": 0.928979754447937,
      "learning_rate": 0.00018828087167070218,
      "loss": 1.737,
      "step": 172
    },
    {
      "epoch": 0.2453900709219858,
      "grad_norm": 1.0045229196548462,
      "learning_rate": 0.00018818401937046006,
      "loss": 1.9461,
      "step": 173
    },
    {
      "epoch": 0.24680851063829787,
      "grad_norm": 1.0096064805984497,
      "learning_rate": 0.0001880871670702179,
      "loss": 1.8946,
      "step": 174
    },
    {
      "epoch": 0.24822695035460993,
      "grad_norm": 1.0389715433120728,
      "learning_rate": 0.00018799031476997578,
      "loss": 1.9128,
      "step": 175
    },
    {
      "epoch": 0.24964539007092199,
      "grad_norm": 0.999501645565033,
      "learning_rate": 0.00018789346246973366,
      "loss": 1.8569,
      "step": 176
    },
    {
      "epoch": 0.251063829787234,
      "grad_norm": 0.9583911895751953,
      "learning_rate": 0.00018779661016949151,
      "loss": 1.8501,
      "step": 177
    },
    {
      "epoch": 0.2524822695035461,
      "grad_norm": 0.9775153994560242,
      "learning_rate": 0.0001876997578692494,
      "loss": 1.6771,
      "step": 178
    },
    {
      "epoch": 0.25390070921985813,
      "grad_norm": 0.9665774703025818,
      "learning_rate": 0.00018760290556900727,
      "loss": 1.8818,
      "step": 179
    },
    {
      "epoch": 0.2553191489361702,
      "grad_norm": 0.9724550247192383,
      "learning_rate": 0.00018750605326876515,
      "loss": 2.1773,
      "step": 180
    },
    {
      "epoch": 0.25673758865248225,
      "grad_norm": 1.1561572551727295,
      "learning_rate": 0.00018740920096852303,
      "loss": 1.8608,
      "step": 181
    },
    {
      "epoch": 0.2581560283687943,
      "grad_norm": 1.0202478170394897,
      "learning_rate": 0.00018731234866828088,
      "loss": 1.7356,
      "step": 182
    },
    {
      "epoch": 0.25957446808510637,
      "grad_norm": 1.0430041551589966,
      "learning_rate": 0.00018721549636803876,
      "loss": 1.9362,
      "step": 183
    },
    {
      "epoch": 0.26099290780141843,
      "grad_norm": 1.038509488105774,
      "learning_rate": 0.00018711864406779663,
      "loss": 2.1047,
      "step": 184
    },
    {
      "epoch": 0.2624113475177305,
      "grad_norm": 1.0352694988250732,
      "learning_rate": 0.00018702179176755448,
      "loss": 1.9668,
      "step": 185
    },
    {
      "epoch": 0.26382978723404255,
      "grad_norm": 1.0145736932754517,
      "learning_rate": 0.00018692493946731236,
      "loss": 1.9073,
      "step": 186
    },
    {
      "epoch": 0.2652482269503546,
      "grad_norm": 0.985048770904541,
      "learning_rate": 0.00018682808716707024,
      "loss": 1.7562,
      "step": 187
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 1.057384967803955,
      "learning_rate": 0.0001867312348668281,
      "loss": 1.8624,
      "step": 188
    },
    {
      "epoch": 0.2680851063829787,
      "grad_norm": 0.9623408317565918,
      "learning_rate": 0.00018663438256658597,
      "loss": 1.7815,
      "step": 189
    },
    {
      "epoch": 0.2695035460992908,
      "grad_norm": 1.0034300088882446,
      "learning_rate": 0.00018653753026634385,
      "loss": 1.9532,
      "step": 190
    },
    {
      "epoch": 0.27092198581560284,
      "grad_norm": 0.9395196437835693,
      "learning_rate": 0.0001864406779661017,
      "loss": 1.6993,
      "step": 191
    },
    {
      "epoch": 0.2723404255319149,
      "grad_norm": 0.9910885095596313,
      "learning_rate": 0.00018634382566585958,
      "loss": 1.7409,
      "step": 192
    },
    {
      "epoch": 0.27375886524822696,
      "grad_norm": 0.9712793827056885,
      "learning_rate": 0.00018624697336561746,
      "loss": 1.7994,
      "step": 193
    },
    {
      "epoch": 0.275177304964539,
      "grad_norm": 0.9951639771461487,
      "learning_rate": 0.0001861501210653753,
      "loss": 1.9165,
      "step": 194
    },
    {
      "epoch": 0.2765957446808511,
      "grad_norm": 0.9464163184165955,
      "learning_rate": 0.00018605326876513318,
      "loss": 1.8001,
      "step": 195
    },
    {
      "epoch": 0.27801418439716313,
      "grad_norm": 1.0139267444610596,
      "learning_rate": 0.00018595641646489106,
      "loss": 1.8521,
      "step": 196
    },
    {
      "epoch": 0.2794326241134752,
      "grad_norm": 0.9855507016181946,
      "learning_rate": 0.00018585956416464891,
      "loss": 1.9984,
      "step": 197
    },
    {
      "epoch": 0.28085106382978725,
      "grad_norm": 1.098187804222107,
      "learning_rate": 0.0001857627118644068,
      "loss": 1.9804,
      "step": 198
    },
    {
      "epoch": 0.2822695035460993,
      "grad_norm": 0.8774276971817017,
      "learning_rate": 0.00018566585956416467,
      "loss": 1.8172,
      "step": 199
    },
    {
      "epoch": 0.28368794326241137,
      "grad_norm": 0.9513270258903503,
      "learning_rate": 0.00018556900726392252,
      "loss": 2.0358,
      "step": 200
    },
    {
      "epoch": 0.2851063829787234,
      "grad_norm": 1.068496823310852,
      "learning_rate": 0.0001854721549636804,
      "loss": 1.589,
      "step": 201
    },
    {
      "epoch": 0.2865248226950355,
      "grad_norm": 0.9528117179870605,
      "learning_rate": 0.00018537530266343825,
      "loss": 1.7981,
      "step": 202
    },
    {
      "epoch": 0.28794326241134754,
      "grad_norm": 0.9149752259254456,
      "learning_rate": 0.00018527845036319613,
      "loss": 1.7106,
      "step": 203
    },
    {
      "epoch": 0.28936170212765955,
      "grad_norm": 1.0679677724838257,
      "learning_rate": 0.000185181598062954,
      "loss": 1.7816,
      "step": 204
    },
    {
      "epoch": 0.2907801418439716,
      "grad_norm": 1.0009958744049072,
      "learning_rate": 0.00018508474576271186,
      "loss": 1.9823,
      "step": 205
    },
    {
      "epoch": 0.29219858156028367,
      "grad_norm": 0.9693969488143921,
      "learning_rate": 0.00018498789346246974,
      "loss": 1.7922,
      "step": 206
    },
    {
      "epoch": 0.2936170212765957,
      "grad_norm": 0.8737604022026062,
      "learning_rate": 0.00018489104116222761,
      "loss": 1.7473,
      "step": 207
    },
    {
      "epoch": 0.2950354609929078,
      "grad_norm": 0.9264819622039795,
      "learning_rate": 0.00018479418886198546,
      "loss": 1.8496,
      "step": 208
    },
    {
      "epoch": 0.29645390070921984,
      "grad_norm": 0.9392140507698059,
      "learning_rate": 0.00018469733656174334,
      "loss": 1.9292,
      "step": 209
    },
    {
      "epoch": 0.2978723404255319,
      "grad_norm": 0.9021729230880737,
      "learning_rate": 0.00018460048426150122,
      "loss": 1.771,
      "step": 210
    },
    {
      "epoch": 0.29929078014184396,
      "grad_norm": 0.9156889319419861,
      "learning_rate": 0.00018450363196125907,
      "loss": 1.7914,
      "step": 211
    },
    {
      "epoch": 0.300709219858156,
      "grad_norm": 0.9815962314605713,
      "learning_rate": 0.00018440677966101695,
      "loss": 2.0523,
      "step": 212
    },
    {
      "epoch": 0.3021276595744681,
      "grad_norm": 0.9518104195594788,
      "learning_rate": 0.00018430992736077483,
      "loss": 1.8942,
      "step": 213
    },
    {
      "epoch": 0.30354609929078014,
      "grad_norm": 1.049188256263733,
      "learning_rate": 0.00018421307506053268,
      "loss": 2.2277,
      "step": 214
    },
    {
      "epoch": 0.3049645390070922,
      "grad_norm": 0.9291843175888062,
      "learning_rate": 0.00018411622276029056,
      "loss": 1.9009,
      "step": 215
    },
    {
      "epoch": 0.30638297872340425,
      "grad_norm": 1.0086026191711426,
      "learning_rate": 0.00018401937046004844,
      "loss": 1.8147,
      "step": 216
    },
    {
      "epoch": 0.3078014184397163,
      "grad_norm": 1.0855687856674194,
      "learning_rate": 0.0001839225181598063,
      "loss": 1.6274,
      "step": 217
    },
    {
      "epoch": 0.30921985815602837,
      "grad_norm": 1.01848304271698,
      "learning_rate": 0.00018382566585956416,
      "loss": 1.9065,
      "step": 218
    },
    {
      "epoch": 0.31063829787234043,
      "grad_norm": 1.115381121635437,
      "learning_rate": 0.00018372881355932204,
      "loss": 1.8363,
      "step": 219
    },
    {
      "epoch": 0.3120567375886525,
      "grad_norm": 1.0715506076812744,
      "learning_rate": 0.00018363196125907992,
      "loss": 1.8608,
      "step": 220
    },
    {
      "epoch": 0.31347517730496455,
      "grad_norm": 0.9929397702217102,
      "learning_rate": 0.0001835351089588378,
      "loss": 1.7621,
      "step": 221
    },
    {
      "epoch": 0.3148936170212766,
      "grad_norm": 1.007621169090271,
      "learning_rate": 0.00018343825665859565,
      "loss": 1.7274,
      "step": 222
    },
    {
      "epoch": 0.31631205673758866,
      "grad_norm": 1.038918137550354,
      "learning_rate": 0.00018334140435835353,
      "loss": 2.0179,
      "step": 223
    },
    {
      "epoch": 0.3177304964539007,
      "grad_norm": 0.9554470777511597,
      "learning_rate": 0.0001832445520581114,
      "loss": 1.8415,
      "step": 224
    },
    {
      "epoch": 0.3191489361702128,
      "grad_norm": 0.9952170848846436,
      "learning_rate": 0.00018314769975786926,
      "loss": 1.8044,
      "step": 225
    },
    {
      "epoch": 0.32056737588652484,
      "grad_norm": 0.9729833006858826,
      "learning_rate": 0.00018305084745762714,
      "loss": 1.8347,
      "step": 226
    },
    {
      "epoch": 0.3219858156028369,
      "grad_norm": 1.0922369956970215,
      "learning_rate": 0.000182953995157385,
      "loss": 2.1871,
      "step": 227
    },
    {
      "epoch": 0.32340425531914896,
      "grad_norm": 1.0044426918029785,
      "learning_rate": 0.00018285714285714286,
      "loss": 1.8984,
      "step": 228
    },
    {
      "epoch": 0.324822695035461,
      "grad_norm": 0.9098760485649109,
      "learning_rate": 0.00018276029055690074,
      "loss": 1.8968,
      "step": 229
    },
    {
      "epoch": 0.3262411347517731,
      "grad_norm": 0.9837214350700378,
      "learning_rate": 0.00018266343825665862,
      "loss": 2.0426,
      "step": 230
    },
    {
      "epoch": 0.3276595744680851,
      "grad_norm": 0.9324159026145935,
      "learning_rate": 0.00018256658595641647,
      "loss": 1.9843,
      "step": 231
    },
    {
      "epoch": 0.32907801418439714,
      "grad_norm": 1.0140657424926758,
      "learning_rate": 0.00018246973365617435,
      "loss": 1.9063,
      "step": 232
    },
    {
      "epoch": 0.3304964539007092,
      "grad_norm": 0.9005314111709595,
      "learning_rate": 0.0001823728813559322,
      "loss": 1.8591,
      "step": 233
    },
    {
      "epoch": 0.33191489361702126,
      "grad_norm": 0.8580217957496643,
      "learning_rate": 0.00018227602905569008,
      "loss": 1.5464,
      "step": 234
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.8500463366508484,
      "learning_rate": 0.00018217917675544796,
      "loss": 1.7719,
      "step": 235
    },
    {
      "epoch": 0.3347517730496454,
      "grad_norm": 0.9767171144485474,
      "learning_rate": 0.0001820823244552058,
      "loss": 2.0819,
      "step": 236
    },
    {
      "epoch": 0.33617021276595743,
      "grad_norm": 0.8938849568367004,
      "learning_rate": 0.00018198547215496369,
      "loss": 1.8295,
      "step": 237
    },
    {
      "epoch": 0.3375886524822695,
      "grad_norm": 0.9511204361915588,
      "learning_rate": 0.00018188861985472156,
      "loss": 1.7266,
      "step": 238
    },
    {
      "epoch": 0.33900709219858155,
      "grad_norm": 0.9522839188575745,
      "learning_rate": 0.00018179176755447942,
      "loss": 1.772,
      "step": 239
    },
    {
      "epoch": 0.3404255319148936,
      "grad_norm": 0.9642924666404724,
      "learning_rate": 0.0001816949152542373,
      "loss": 2.1366,
      "step": 240
    },
    {
      "epoch": 0.34184397163120567,
      "grad_norm": 0.9406882524490356,
      "learning_rate": 0.00018159806295399517,
      "loss": 2.0296,
      "step": 241
    },
    {
      "epoch": 0.3432624113475177,
      "grad_norm": 0.967184841632843,
      "learning_rate": 0.00018150121065375302,
      "loss": 1.8968,
      "step": 242
    },
    {
      "epoch": 0.3446808510638298,
      "grad_norm": 0.893243134021759,
      "learning_rate": 0.0001814043583535109,
      "loss": 1.7,
      "step": 243
    },
    {
      "epoch": 0.34609929078014184,
      "grad_norm": 0.9362541437149048,
      "learning_rate": 0.00018130750605326878,
      "loss": 1.9237,
      "step": 244
    },
    {
      "epoch": 0.3475177304964539,
      "grad_norm": 0.9011443257331848,
      "learning_rate": 0.00018121065375302663,
      "loss": 1.7482,
      "step": 245
    },
    {
      "epoch": 0.34893617021276596,
      "grad_norm": 0.9381369948387146,
      "learning_rate": 0.0001811138014527845,
      "loss": 2.0529,
      "step": 246
    },
    {
      "epoch": 0.350354609929078,
      "grad_norm": 0.9607241153717041,
      "learning_rate": 0.00018101694915254239,
      "loss": 1.7078,
      "step": 247
    },
    {
      "epoch": 0.3517730496453901,
      "grad_norm": 1.008245825767517,
      "learning_rate": 0.00018092009685230024,
      "loss": 1.6759,
      "step": 248
    },
    {
      "epoch": 0.35319148936170214,
      "grad_norm": 0.9571892023086548,
      "learning_rate": 0.00018082324455205812,
      "loss": 2.0354,
      "step": 249
    },
    {
      "epoch": 0.3546099290780142,
      "grad_norm": 0.9735412001609802,
      "learning_rate": 0.00018072639225181597,
      "loss": 1.6914,
      "step": 250
    },
    {
      "epoch": 0.35602836879432626,
      "grad_norm": 1.0446652173995972,
      "learning_rate": 0.00018062953995157384,
      "loss": 1.8721,
      "step": 251
    },
    {
      "epoch": 0.3574468085106383,
      "grad_norm": 1.3239846229553223,
      "learning_rate": 0.00018053268765133172,
      "loss": 1.9019,
      "step": 252
    },
    {
      "epoch": 0.3588652482269504,
      "grad_norm": 0.9775428175926208,
      "learning_rate": 0.0001804358353510896,
      "loss": 1.6349,
      "step": 253
    },
    {
      "epoch": 0.36028368794326243,
      "grad_norm": 1.0675644874572754,
      "learning_rate": 0.00018033898305084748,
      "loss": 2.192,
      "step": 254
    },
    {
      "epoch": 0.3617021276595745,
      "grad_norm": 0.9878771305084229,
      "learning_rate": 0.00018024213075060533,
      "loss": 1.7361,
      "step": 255
    },
    {
      "epoch": 0.36312056737588655,
      "grad_norm": 0.8959107995033264,
      "learning_rate": 0.0001801452784503632,
      "loss": 1.6891,
      "step": 256
    },
    {
      "epoch": 0.3645390070921986,
      "grad_norm": 0.9647307991981506,
      "learning_rate": 0.00018004842615012109,
      "loss": 1.7116,
      "step": 257
    },
    {
      "epoch": 0.3659574468085106,
      "grad_norm": 1.0258516073226929,
      "learning_rate": 0.00017995157384987896,
      "loss": 2.0333,
      "step": 258
    },
    {
      "epoch": 0.36737588652482267,
      "grad_norm": 0.904725193977356,
      "learning_rate": 0.00017985472154963681,
      "loss": 1.765,
      "step": 259
    },
    {
      "epoch": 0.36879432624113473,
      "grad_norm": 0.9768916964530945,
      "learning_rate": 0.0001797578692493947,
      "loss": 1.8871,
      "step": 260
    },
    {
      "epoch": 0.3702127659574468,
      "grad_norm": 0.9332278966903687,
      "learning_rate": 0.00017966101694915257,
      "loss": 1.8291,
      "step": 261
    },
    {
      "epoch": 0.37163120567375885,
      "grad_norm": 0.978183925151825,
      "learning_rate": 0.00017956416464891042,
      "loss": 1.8143,
      "step": 262
    },
    {
      "epoch": 0.3730496453900709,
      "grad_norm": 0.9541735649108887,
      "learning_rate": 0.0001794673123486683,
      "loss": 1.8159,
      "step": 263
    },
    {
      "epoch": 0.37446808510638296,
      "grad_norm": 0.9887608885765076,
      "learning_rate": 0.00017937046004842615,
      "loss": 1.9049,
      "step": 264
    },
    {
      "epoch": 0.375886524822695,
      "grad_norm": 0.9812231659889221,
      "learning_rate": 0.00017927360774818403,
      "loss": 1.8132,
      "step": 265
    },
    {
      "epoch": 0.3773049645390071,
      "grad_norm": 0.9784431457519531,
      "learning_rate": 0.0001791767554479419,
      "loss": 1.8101,
      "step": 266
    },
    {
      "epoch": 0.37872340425531914,
      "grad_norm": 0.9982566833496094,
      "learning_rate": 0.00017907990314769976,
      "loss": 1.7921,
      "step": 267
    },
    {
      "epoch": 0.3801418439716312,
      "grad_norm": 0.9074917435646057,
      "learning_rate": 0.00017898305084745764,
      "loss": 1.9321,
      "step": 268
    },
    {
      "epoch": 0.38156028368794326,
      "grad_norm": 1.0277540683746338,
      "learning_rate": 0.00017888619854721551,
      "loss": 2.126,
      "step": 269
    },
    {
      "epoch": 0.3829787234042553,
      "grad_norm": 0.8828142285346985,
      "learning_rate": 0.00017878934624697337,
      "loss": 1.5719,
      "step": 270
    },
    {
      "epoch": 0.3843971631205674,
      "grad_norm": 0.9607075452804565,
      "learning_rate": 0.00017869249394673124,
      "loss": 1.7884,
      "step": 271
    },
    {
      "epoch": 0.38581560283687943,
      "grad_norm": 0.9126862287521362,
      "learning_rate": 0.00017859564164648912,
      "loss": 1.9482,
      "step": 272
    },
    {
      "epoch": 0.3872340425531915,
      "grad_norm": 0.9614412188529968,
      "learning_rate": 0.00017849878934624697,
      "loss": 1.7178,
      "step": 273
    },
    {
      "epoch": 0.38865248226950355,
      "grad_norm": 0.9119043350219727,
      "learning_rate": 0.00017840193704600485,
      "loss": 1.8499,
      "step": 274
    },
    {
      "epoch": 0.3900709219858156,
      "grad_norm": 1.0984479188919067,
      "learning_rate": 0.00017830508474576273,
      "loss": 1.759,
      "step": 275
    },
    {
      "epoch": 0.39148936170212767,
      "grad_norm": 0.9465433955192566,
      "learning_rate": 0.00017820823244552058,
      "loss": 1.807,
      "step": 276
    },
    {
      "epoch": 0.39290780141843973,
      "grad_norm": 0.9199845194816589,
      "learning_rate": 0.00017811138014527846,
      "loss": 1.8849,
      "step": 277
    },
    {
      "epoch": 0.3943262411347518,
      "grad_norm": 0.9319270849227905,
      "learning_rate": 0.00017801452784503634,
      "loss": 1.859,
      "step": 278
    },
    {
      "epoch": 0.39574468085106385,
      "grad_norm": 0.949927031993866,
      "learning_rate": 0.0001779176755447942,
      "loss": 1.9239,
      "step": 279
    },
    {
      "epoch": 0.3971631205673759,
      "grad_norm": 0.9235413074493408,
      "learning_rate": 0.00017782082324455207,
      "loss": 1.8989,
      "step": 280
    },
    {
      "epoch": 0.39858156028368796,
      "grad_norm": 0.943297266960144,
      "learning_rate": 0.00017772397094430992,
      "loss": 1.8911,
      "step": 281
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.9669685959815979,
      "learning_rate": 0.0001776271186440678,
      "loss": 1.8185,
      "step": 282
    },
    {
      "epoch": 0.4014184397163121,
      "grad_norm": 0.9418421387672424,
      "learning_rate": 0.00017753026634382567,
      "loss": 1.8185,
      "step": 283
    },
    {
      "epoch": 0.40283687943262414,
      "grad_norm": 1.0247584581375122,
      "learning_rate": 0.00017743341404358352,
      "loss": 1.8827,
      "step": 284
    },
    {
      "epoch": 0.40425531914893614,
      "grad_norm": 1.0367324352264404,
      "learning_rate": 0.0001773365617433414,
      "loss": 1.9937,
      "step": 285
    },
    {
      "epoch": 0.4056737588652482,
      "grad_norm": 1.0447474718093872,
      "learning_rate": 0.00017723970944309928,
      "loss": 1.924,
      "step": 286
    },
    {
      "epoch": 0.40709219858156026,
      "grad_norm": 0.9520232677459717,
      "learning_rate": 0.00017714285714285713,
      "loss": 1.936,
      "step": 287
    },
    {
      "epoch": 0.4085106382978723,
      "grad_norm": 0.9429634213447571,
      "learning_rate": 0.000177046004842615,
      "loss": 1.9339,
      "step": 288
    },
    {
      "epoch": 0.4099290780141844,
      "grad_norm": 0.9821912050247192,
      "learning_rate": 0.0001769491525423729,
      "loss": 2.0034,
      "step": 289
    },
    {
      "epoch": 0.41134751773049644,
      "grad_norm": 0.9924930930137634,
      "learning_rate": 0.00017685230024213077,
      "loss": 1.8511,
      "step": 290
    },
    {
      "epoch": 0.4127659574468085,
      "grad_norm": 1.0038831233978271,
      "learning_rate": 0.00017675544794188862,
      "loss": 2.1056,
      "step": 291
    },
    {
      "epoch": 0.41418439716312055,
      "grad_norm": 0.9564592838287354,
      "learning_rate": 0.0001766585956416465,
      "loss": 1.8863,
      "step": 292
    },
    {
      "epoch": 0.4156028368794326,
      "grad_norm": 0.9334070086479187,
      "learning_rate": 0.00017656174334140437,
      "loss": 1.8718,
      "step": 293
    },
    {
      "epoch": 0.41702127659574467,
      "grad_norm": 0.8720489144325256,
      "learning_rate": 0.00017646489104116225,
      "loss": 1.8468,
      "step": 294
    },
    {
      "epoch": 0.41843971631205673,
      "grad_norm": 0.8848581314086914,
      "learning_rate": 0.00017636803874092013,
      "loss": 1.8084,
      "step": 295
    },
    {
      "epoch": 0.4198581560283688,
      "grad_norm": 0.9114629626274109,
      "learning_rate": 0.00017627118644067798,
      "loss": 1.9487,
      "step": 296
    },
    {
      "epoch": 0.42127659574468085,
      "grad_norm": 0.9775903820991516,
      "learning_rate": 0.00017617433414043586,
      "loss": 1.7691,
      "step": 297
    },
    {
      "epoch": 0.4226950354609929,
      "grad_norm": 1.0030083656311035,
      "learning_rate": 0.0001760774818401937,
      "loss": 1.7996,
      "step": 298
    },
    {
      "epoch": 0.42411347517730497,
      "grad_norm": 0.9292450547218323,
      "learning_rate": 0.0001759806295399516,
      "loss": 1.7146,
      "step": 299
    },
    {
      "epoch": 0.425531914893617,
      "grad_norm": 0.9759061336517334,
      "learning_rate": 0.00017588377723970947,
      "loss": 1.7757,
      "step": 300
    },
    {
      "epoch": 0.4269503546099291,
      "grad_norm": 0.9845147132873535,
      "learning_rate": 0.00017578692493946732,
      "loss": 1.768,
      "step": 301
    },
    {
      "epoch": 0.42836879432624114,
      "grad_norm": 0.9506266713142395,
      "learning_rate": 0.0001756900726392252,
      "loss": 2.0084,
      "step": 302
    },
    {
      "epoch": 0.4297872340425532,
      "grad_norm": 0.9350481033325195,
      "learning_rate": 0.00017559322033898307,
      "loss": 1.9765,
      "step": 303
    },
    {
      "epoch": 0.43120567375886526,
      "grad_norm": 0.9896640181541443,
      "learning_rate": 0.00017549636803874092,
      "loss": 1.893,
      "step": 304
    },
    {
      "epoch": 0.4326241134751773,
      "grad_norm": 0.9888864159584045,
      "learning_rate": 0.0001753995157384988,
      "loss": 1.88,
      "step": 305
    },
    {
      "epoch": 0.4340425531914894,
      "grad_norm": 0.937113881111145,
      "learning_rate": 0.00017530266343825668,
      "loss": 1.8432,
      "step": 306
    },
    {
      "epoch": 0.43546099290780144,
      "grad_norm": 0.9765783548355103,
      "learning_rate": 0.00017520581113801453,
      "loss": 1.7804,
      "step": 307
    },
    {
      "epoch": 0.4368794326241135,
      "grad_norm": 0.9332955479621887,
      "learning_rate": 0.0001751089588377724,
      "loss": 1.8956,
      "step": 308
    },
    {
      "epoch": 0.43829787234042555,
      "grad_norm": 0.9328266382217407,
      "learning_rate": 0.0001750121065375303,
      "loss": 1.782,
      "step": 309
    },
    {
      "epoch": 0.4397163120567376,
      "grad_norm": 0.9297088980674744,
      "learning_rate": 0.00017491525423728814,
      "loss": 1.805,
      "step": 310
    },
    {
      "epoch": 0.44113475177304967,
      "grad_norm": 0.9776032567024231,
      "learning_rate": 0.00017481840193704602,
      "loss": 1.8934,
      "step": 311
    },
    {
      "epoch": 0.4425531914893617,
      "grad_norm": 0.9452140927314758,
      "learning_rate": 0.00017472154963680387,
      "loss": 1.8798,
      "step": 312
    },
    {
      "epoch": 0.44397163120567373,
      "grad_norm": 1.030831217765808,
      "learning_rate": 0.00017462469733656175,
      "loss": 1.7181,
      "step": 313
    },
    {
      "epoch": 0.4453900709219858,
      "grad_norm": 0.8747674226760864,
      "learning_rate": 0.00017452784503631962,
      "loss": 1.6531,
      "step": 314
    },
    {
      "epoch": 0.44680851063829785,
      "grad_norm": 0.9434899687767029,
      "learning_rate": 0.00017443099273607747,
      "loss": 1.7733,
      "step": 315
    },
    {
      "epoch": 0.4482269503546099,
      "grad_norm": 0.9592564105987549,
      "learning_rate": 0.00017433414043583535,
      "loss": 2.0461,
      "step": 316
    },
    {
      "epoch": 0.44964539007092197,
      "grad_norm": 0.9413037300109863,
      "learning_rate": 0.00017423728813559323,
      "loss": 1.9054,
      "step": 317
    },
    {
      "epoch": 0.451063829787234,
      "grad_norm": 0.9602166414260864,
      "learning_rate": 0.00017414043583535108,
      "loss": 1.8483,
      "step": 318
    },
    {
      "epoch": 0.4524822695035461,
      "grad_norm": 1.0252346992492676,
      "learning_rate": 0.00017404358353510896,
      "loss": 1.8019,
      "step": 319
    },
    {
      "epoch": 0.45390070921985815,
      "grad_norm": 0.9301417469978333,
      "learning_rate": 0.00017394673123486684,
      "loss": 1.7285,
      "step": 320
    },
    {
      "epoch": 0.4553191489361702,
      "grad_norm": 0.9384927153587341,
      "learning_rate": 0.0001738498789346247,
      "loss": 1.6223,
      "step": 321
    },
    {
      "epoch": 0.45673758865248226,
      "grad_norm": 0.9198197722434998,
      "learning_rate": 0.00017375302663438257,
      "loss": 1.7974,
      "step": 322
    },
    {
      "epoch": 0.4581560283687943,
      "grad_norm": 0.9289765954017639,
      "learning_rate": 0.00017365617433414045,
      "loss": 1.6964,
      "step": 323
    },
    {
      "epoch": 0.4595744680851064,
      "grad_norm": 0.9340206384658813,
      "learning_rate": 0.0001735593220338983,
      "loss": 1.9346,
      "step": 324
    },
    {
      "epoch": 0.46099290780141844,
      "grad_norm": 1.032168984413147,
      "learning_rate": 0.00017346246973365617,
      "loss": 1.8882,
      "step": 325
    },
    {
      "epoch": 0.4624113475177305,
      "grad_norm": 1.043074607849121,
      "learning_rate": 0.00017336561743341405,
      "loss": 1.5286,
      "step": 326
    },
    {
      "epoch": 0.46382978723404256,
      "grad_norm": 0.9749189615249634,
      "learning_rate": 0.00017326876513317193,
      "loss": 1.9693,
      "step": 327
    },
    {
      "epoch": 0.4652482269503546,
      "grad_norm": 0.9572228789329529,
      "learning_rate": 0.00017317191283292978,
      "loss": 1.8084,
      "step": 328
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 1.0045562982559204,
      "learning_rate": 0.00017307506053268766,
      "loss": 1.7183,
      "step": 329
    },
    {
      "epoch": 0.46808510638297873,
      "grad_norm": 1.1227355003356934,
      "learning_rate": 0.00017297820823244554,
      "loss": 1.9009,
      "step": 330
    },
    {
      "epoch": 0.4695035460992908,
      "grad_norm": 1.0158571004867554,
      "learning_rate": 0.00017288135593220342,
      "loss": 1.9807,
      "step": 331
    },
    {
      "epoch": 0.47092198581560285,
      "grad_norm": 0.9610460996627808,
      "learning_rate": 0.00017278450363196127,
      "loss": 1.7399,
      "step": 332
    },
    {
      "epoch": 0.4723404255319149,
      "grad_norm": 0.8958942294120789,
      "learning_rate": 0.00017268765133171915,
      "loss": 1.7994,
      "step": 333
    },
    {
      "epoch": 0.47375886524822697,
      "grad_norm": 0.9412689805030823,
      "learning_rate": 0.00017259079903147702,
      "loss": 2.0594,
      "step": 334
    },
    {
      "epoch": 0.475177304964539,
      "grad_norm": 0.9631602764129639,
      "learning_rate": 0.00017249394673123487,
      "loss": 1.7864,
      "step": 335
    },
    {
      "epoch": 0.4765957446808511,
      "grad_norm": 1.1445562839508057,
      "learning_rate": 0.00017239709443099275,
      "loss": 1.5906,
      "step": 336
    },
    {
      "epoch": 0.47801418439716314,
      "grad_norm": 0.9993076920509338,
      "learning_rate": 0.00017230024213075063,
      "loss": 1.6411,
      "step": 337
    },
    {
      "epoch": 0.4794326241134752,
      "grad_norm": 1.0374830961227417,
      "learning_rate": 0.00017220338983050848,
      "loss": 1.842,
      "step": 338
    },
    {
      "epoch": 0.4808510638297872,
      "grad_norm": 0.973639726638794,
      "learning_rate": 0.00017210653753026636,
      "loss": 1.7878,
      "step": 339
    },
    {
      "epoch": 0.48226950354609927,
      "grad_norm": 0.9438994526863098,
      "learning_rate": 0.00017200968523002424,
      "loss": 2.0517,
      "step": 340
    },
    {
      "epoch": 0.4836879432624113,
      "grad_norm": 0.9525969624519348,
      "learning_rate": 0.0001719128329297821,
      "loss": 1.9102,
      "step": 341
    },
    {
      "epoch": 0.4851063829787234,
      "grad_norm": 0.9234567284584045,
      "learning_rate": 0.00017181598062953997,
      "loss": 1.932,
      "step": 342
    },
    {
      "epoch": 0.48652482269503544,
      "grad_norm": 0.9590464234352112,
      "learning_rate": 0.00017171912832929782,
      "loss": 1.7139,
      "step": 343
    },
    {
      "epoch": 0.4879432624113475,
      "grad_norm": 0.9586837291717529,
      "learning_rate": 0.0001716222760290557,
      "loss": 1.9188,
      "step": 344
    },
    {
      "epoch": 0.48936170212765956,
      "grad_norm": 0.9036353826522827,
      "learning_rate": 0.00017152542372881357,
      "loss": 1.6881,
      "step": 345
    },
    {
      "epoch": 0.4907801418439716,
      "grad_norm": 0.9529016017913818,
      "learning_rate": 0.00017142857142857143,
      "loss": 1.8478,
      "step": 346
    },
    {
      "epoch": 0.4921985815602837,
      "grad_norm": 1.007252812385559,
      "learning_rate": 0.0001713317191283293,
      "loss": 1.8538,
      "step": 347
    },
    {
      "epoch": 0.49361702127659574,
      "grad_norm": 0.945681631565094,
      "learning_rate": 0.00017123486682808718,
      "loss": 1.9226,
      "step": 348
    },
    {
      "epoch": 0.4950354609929078,
      "grad_norm": 0.9263474941253662,
      "learning_rate": 0.00017113801452784503,
      "loss": 1.7189,
      "step": 349
    },
    {
      "epoch": 0.49645390070921985,
      "grad_norm": 0.9747632741928101,
      "learning_rate": 0.0001710411622276029,
      "loss": 1.8628,
      "step": 350
    },
    {
      "epoch": 0.4978723404255319,
      "grad_norm": 0.9846658706665039,
      "learning_rate": 0.0001709443099273608,
      "loss": 1.9134,
      "step": 351
    },
    {
      "epoch": 0.49929078014184397,
      "grad_norm": 0.9113063812255859,
      "learning_rate": 0.00017084745762711864,
      "loss": 1.899,
      "step": 352
    },
    {
      "epoch": 0.500709219858156,
      "grad_norm": 0.9714037179946899,
      "learning_rate": 0.00017075060532687652,
      "loss": 1.9107,
      "step": 353
    },
    {
      "epoch": 0.502127659574468,
      "grad_norm": 0.9537795186042786,
      "learning_rate": 0.0001706537530266344,
      "loss": 1.8053,
      "step": 354
    },
    {
      "epoch": 0.5035460992907801,
      "grad_norm": 0.9485891461372375,
      "learning_rate": 0.00017055690072639225,
      "loss": 1.7612,
      "step": 355
    },
    {
      "epoch": 0.5049645390070922,
      "grad_norm": 0.9222022891044617,
      "learning_rate": 0.00017046004842615013,
      "loss": 1.8346,
      "step": 356
    },
    {
      "epoch": 0.5063829787234042,
      "grad_norm": 0.9114431142807007,
      "learning_rate": 0.000170363196125908,
      "loss": 1.6909,
      "step": 357
    },
    {
      "epoch": 0.5078014184397163,
      "grad_norm": 0.9674013257026672,
      "learning_rate": 0.00017026634382566585,
      "loss": 1.8031,
      "step": 358
    },
    {
      "epoch": 0.5092198581560283,
      "grad_norm": 0.9337841272354126,
      "learning_rate": 0.00017016949152542373,
      "loss": 1.7164,
      "step": 359
    },
    {
      "epoch": 0.5106382978723404,
      "grad_norm": 0.9948791861534119,
      "learning_rate": 0.00017007263922518158,
      "loss": 1.7063,
      "step": 360
    },
    {
      "epoch": 0.5120567375886524,
      "grad_norm": 1.0778685808181763,
      "learning_rate": 0.00016997578692493946,
      "loss": 2.0027,
      "step": 361
    },
    {
      "epoch": 0.5134751773049645,
      "grad_norm": 0.9692330360412598,
      "learning_rate": 0.00016987893462469734,
      "loss": 1.7529,
      "step": 362
    },
    {
      "epoch": 0.5148936170212766,
      "grad_norm": 1.045678734779358,
      "learning_rate": 0.00016978208232445522,
      "loss": 1.908,
      "step": 363
    },
    {
      "epoch": 0.5163120567375886,
      "grad_norm": 1.0215644836425781,
      "learning_rate": 0.00016968523002421307,
      "loss": 2.0764,
      "step": 364
    },
    {
      "epoch": 0.5177304964539007,
      "grad_norm": 0.9777445793151855,
      "learning_rate": 0.00016958837772397095,
      "loss": 1.871,
      "step": 365
    },
    {
      "epoch": 0.5191489361702127,
      "grad_norm": 0.9745745062828064,
      "learning_rate": 0.00016949152542372882,
      "loss": 1.6008,
      "step": 366
    },
    {
      "epoch": 0.5205673758865248,
      "grad_norm": 1.0185343027114868,
      "learning_rate": 0.0001693946731234867,
      "loss": 1.9607,
      "step": 367
    },
    {
      "epoch": 0.5219858156028369,
      "grad_norm": 0.9472505450248718,
      "learning_rate": 0.00016929782082324458,
      "loss": 1.861,
      "step": 368
    },
    {
      "epoch": 0.5234042553191489,
      "grad_norm": 1.0330811738967896,
      "learning_rate": 0.00016920096852300243,
      "loss": 2.0062,
      "step": 369
    },
    {
      "epoch": 0.524822695035461,
      "grad_norm": 0.931767463684082,
      "learning_rate": 0.0001691041162227603,
      "loss": 1.7943,
      "step": 370
    },
    {
      "epoch": 0.526241134751773,
      "grad_norm": 0.8553069829940796,
      "learning_rate": 0.0001690072639225182,
      "loss": 1.7324,
      "step": 371
    },
    {
      "epoch": 0.5276595744680851,
      "grad_norm": 0.8727834224700928,
      "learning_rate": 0.00016891041162227604,
      "loss": 1.7278,
      "step": 372
    },
    {
      "epoch": 0.5290780141843971,
      "grad_norm": 0.8997536301612854,
      "learning_rate": 0.00016881355932203392,
      "loss": 1.8374,
      "step": 373
    },
    {
      "epoch": 0.5304964539007092,
      "grad_norm": 0.9161466360092163,
      "learning_rate": 0.0001687167070217918,
      "loss": 1.8076,
      "step": 374
    },
    {
      "epoch": 0.5319148936170213,
      "grad_norm": 1.0687780380249023,
      "learning_rate": 0.00016861985472154965,
      "loss": 1.5887,
      "step": 375
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 0.8630894422531128,
      "learning_rate": 0.00016852300242130752,
      "loss": 1.7742,
      "step": 376
    },
    {
      "epoch": 0.5347517730496454,
      "grad_norm": 0.998538076877594,
      "learning_rate": 0.00016842615012106538,
      "loss": 1.7454,
      "step": 377
    },
    {
      "epoch": 0.5361702127659574,
      "grad_norm": 0.9754890203475952,
      "learning_rate": 0.00016832929782082325,
      "loss": 1.8326,
      "step": 378
    },
    {
      "epoch": 0.5375886524822695,
      "grad_norm": 0.9351779222488403,
      "learning_rate": 0.00016823244552058113,
      "loss": 1.8356,
      "step": 379
    },
    {
      "epoch": 0.5390070921985816,
      "grad_norm": 0.8755729794502258,
      "learning_rate": 0.00016813559322033898,
      "loss": 1.6331,
      "step": 380
    },
    {
      "epoch": 0.5404255319148936,
      "grad_norm": 1.0290228128433228,
      "learning_rate": 0.00016803874092009686,
      "loss": 2.1401,
      "step": 381
    },
    {
      "epoch": 0.5418439716312057,
      "grad_norm": 0.9561594724655151,
      "learning_rate": 0.00016794188861985474,
      "loss": 1.7978,
      "step": 382
    },
    {
      "epoch": 0.5432624113475177,
      "grad_norm": 0.9548543691635132,
      "learning_rate": 0.0001678450363196126,
      "loss": 1.8855,
      "step": 383
    },
    {
      "epoch": 0.5446808510638298,
      "grad_norm": 0.9470059871673584,
      "learning_rate": 0.00016774818401937047,
      "loss": 1.7313,
      "step": 384
    },
    {
      "epoch": 0.5460992907801419,
      "grad_norm": 0.9736037254333496,
      "learning_rate": 0.00016765133171912835,
      "loss": 1.8428,
      "step": 385
    },
    {
      "epoch": 0.5475177304964539,
      "grad_norm": 0.957306981086731,
      "learning_rate": 0.0001675544794188862,
      "loss": 2.0756,
      "step": 386
    },
    {
      "epoch": 0.548936170212766,
      "grad_norm": 0.9309136867523193,
      "learning_rate": 0.00016745762711864408,
      "loss": 1.8929,
      "step": 387
    },
    {
      "epoch": 0.550354609929078,
      "grad_norm": 0.8406240940093994,
      "learning_rate": 0.00016736077481840195,
      "loss": 1.8103,
      "step": 388
    },
    {
      "epoch": 0.5517730496453901,
      "grad_norm": 0.8818572163581848,
      "learning_rate": 0.0001672639225181598,
      "loss": 1.9621,
      "step": 389
    },
    {
      "epoch": 0.5531914893617021,
      "grad_norm": 1.0767971277236938,
      "learning_rate": 0.00016716707021791768,
      "loss": 1.8136,
      "step": 390
    },
    {
      "epoch": 0.5546099290780142,
      "grad_norm": 0.9289748668670654,
      "learning_rate": 0.00016707021791767553,
      "loss": 1.7687,
      "step": 391
    },
    {
      "epoch": 0.5560283687943263,
      "grad_norm": 0.962541937828064,
      "learning_rate": 0.0001669733656174334,
      "loss": 1.8763,
      "step": 392
    },
    {
      "epoch": 0.5574468085106383,
      "grad_norm": 0.9504688382148743,
      "learning_rate": 0.0001668765133171913,
      "loss": 2.0939,
      "step": 393
    },
    {
      "epoch": 0.5588652482269504,
      "grad_norm": 0.9227321147918701,
      "learning_rate": 0.00016677966101694914,
      "loss": 1.8242,
      "step": 394
    },
    {
      "epoch": 0.5602836879432624,
      "grad_norm": 0.9383020401000977,
      "learning_rate": 0.00016668280871670702,
      "loss": 2.0296,
      "step": 395
    },
    {
      "epoch": 0.5617021276595745,
      "grad_norm": 0.9648865461349487,
      "learning_rate": 0.0001665859564164649,
      "loss": 1.8459,
      "step": 396
    },
    {
      "epoch": 0.5631205673758866,
      "grad_norm": 0.8519643545150757,
      "learning_rate": 0.00016648910411622275,
      "loss": 1.6578,
      "step": 397
    },
    {
      "epoch": 0.5645390070921986,
      "grad_norm": 0.900329053401947,
      "learning_rate": 0.00016639225181598063,
      "loss": 1.8174,
      "step": 398
    },
    {
      "epoch": 0.5659574468085107,
      "grad_norm": 0.8840930461883545,
      "learning_rate": 0.0001662953995157385,
      "loss": 1.6711,
      "step": 399
    },
    {
      "epoch": 0.5673758865248227,
      "grad_norm": 0.9421043992042542,
      "learning_rate": 0.00016619854721549638,
      "loss": 1.9039,
      "step": 400
    },
    {
      "epoch": 0.5687943262411348,
      "grad_norm": 0.9165019989013672,
      "learning_rate": 0.00016610169491525423,
      "loss": 1.8825,
      "step": 401
    },
    {
      "epoch": 0.5702127659574469,
      "grad_norm": 0.9477689862251282,
      "learning_rate": 0.0001660048426150121,
      "loss": 1.7967,
      "step": 402
    },
    {
      "epoch": 0.5716312056737589,
      "grad_norm": 0.9842367172241211,
      "learning_rate": 0.00016590799031477,
      "loss": 1.812,
      "step": 403
    },
    {
      "epoch": 0.573049645390071,
      "grad_norm": 0.9256036281585693,
      "learning_rate": 0.00016581113801452787,
      "loss": 1.7424,
      "step": 404
    },
    {
      "epoch": 0.574468085106383,
      "grad_norm": 0.9276215434074402,
      "learning_rate": 0.00016571428571428575,
      "loss": 1.9102,
      "step": 405
    },
    {
      "epoch": 0.5758865248226951,
      "grad_norm": 0.9226938486099243,
      "learning_rate": 0.0001656174334140436,
      "loss": 1.7025,
      "step": 406
    },
    {
      "epoch": 0.577304964539007,
      "grad_norm": 0.8902654051780701,
      "learning_rate": 0.00016552058111380148,
      "loss": 1.7183,
      "step": 407
    },
    {
      "epoch": 0.5787234042553191,
      "grad_norm": 0.9309189319610596,
      "learning_rate": 0.00016542372881355933,
      "loss": 1.7781,
      "step": 408
    },
    {
      "epoch": 0.5801418439716312,
      "grad_norm": 0.947432279586792,
      "learning_rate": 0.0001653268765133172,
      "loss": 1.7323,
      "step": 409
    },
    {
      "epoch": 0.5815602836879432,
      "grad_norm": 0.9832228422164917,
      "learning_rate": 0.00016523002421307508,
      "loss": 1.9145,
      "step": 410
    },
    {
      "epoch": 0.5829787234042553,
      "grad_norm": 0.9246309995651245,
      "learning_rate": 0.00016513317191283293,
      "loss": 1.6524,
      "step": 411
    },
    {
      "epoch": 0.5843971631205673,
      "grad_norm": 0.9460448622703552,
      "learning_rate": 0.0001650363196125908,
      "loss": 1.7523,
      "step": 412
    },
    {
      "epoch": 0.5858156028368794,
      "grad_norm": 1.0051263570785522,
      "learning_rate": 0.0001649394673123487,
      "loss": 1.8572,
      "step": 413
    },
    {
      "epoch": 0.5872340425531914,
      "grad_norm": 0.9661255478858948,
      "learning_rate": 0.00016484261501210654,
      "loss": 1.8508,
      "step": 414
    },
    {
      "epoch": 0.5886524822695035,
      "grad_norm": 0.9462977647781372,
      "learning_rate": 0.00016474576271186442,
      "loss": 1.8244,
      "step": 415
    },
    {
      "epoch": 0.5900709219858156,
      "grad_norm": 1.021762490272522,
      "learning_rate": 0.0001646489104116223,
      "loss": 1.797,
      "step": 416
    },
    {
      "epoch": 0.5914893617021276,
      "grad_norm": 0.9398526549339294,
      "learning_rate": 0.00016455205811138015,
      "loss": 1.9082,
      "step": 417
    },
    {
      "epoch": 0.5929078014184397,
      "grad_norm": 0.9971896409988403,
      "learning_rate": 0.00016445520581113803,
      "loss": 2.1014,
      "step": 418
    },
    {
      "epoch": 0.5943262411347517,
      "grad_norm": 0.9796718955039978,
      "learning_rate": 0.0001643583535108959,
      "loss": 1.7606,
      "step": 419
    },
    {
      "epoch": 0.5957446808510638,
      "grad_norm": 1.0000969171524048,
      "learning_rate": 0.00016426150121065376,
      "loss": 1.9288,
      "step": 420
    },
    {
      "epoch": 0.5971631205673759,
      "grad_norm": 0.9603239297866821,
      "learning_rate": 0.00016416464891041163,
      "loss": 1.9338,
      "step": 421
    },
    {
      "epoch": 0.5985815602836879,
      "grad_norm": 1.0116310119628906,
      "learning_rate": 0.00016406779661016948,
      "loss": 1.6369,
      "step": 422
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.9344016909599304,
      "learning_rate": 0.00016397094430992736,
      "loss": 1.8784,
      "step": 423
    },
    {
      "epoch": 0.601418439716312,
      "grad_norm": 0.9857602715492249,
      "learning_rate": 0.00016387409200968524,
      "loss": 1.8524,
      "step": 424
    },
    {
      "epoch": 0.6028368794326241,
      "grad_norm": 0.9478918313980103,
      "learning_rate": 0.0001637772397094431,
      "loss": 1.779,
      "step": 425
    },
    {
      "epoch": 0.6042553191489362,
      "grad_norm": 0.9827153086662292,
      "learning_rate": 0.00016368038740920097,
      "loss": 1.8628,
      "step": 426
    },
    {
      "epoch": 0.6056737588652482,
      "grad_norm": 0.9552390575408936,
      "learning_rate": 0.00016358353510895885,
      "loss": 1.9878,
      "step": 427
    },
    {
      "epoch": 0.6070921985815603,
      "grad_norm": 0.8602179884910583,
      "learning_rate": 0.0001634866828087167,
      "loss": 1.5751,
      "step": 428
    },
    {
      "epoch": 0.6085106382978723,
      "grad_norm": 1.0037013292312622,
      "learning_rate": 0.00016338983050847458,
      "loss": 1.9663,
      "step": 429
    },
    {
      "epoch": 0.6099290780141844,
      "grad_norm": 0.9661442637443542,
      "learning_rate": 0.00016329297820823246,
      "loss": 1.6912,
      "step": 430
    },
    {
      "epoch": 0.6113475177304964,
      "grad_norm": 0.9556114673614502,
      "learning_rate": 0.0001631961259079903,
      "loss": 1.7937,
      "step": 431
    },
    {
      "epoch": 0.6127659574468085,
      "grad_norm": 1.1967833042144775,
      "learning_rate": 0.00016309927360774818,
      "loss": 1.938,
      "step": 432
    },
    {
      "epoch": 0.6141843971631206,
      "grad_norm": 0.9502696990966797,
      "learning_rate": 0.00016300242130750606,
      "loss": 1.7276,
      "step": 433
    },
    {
      "epoch": 0.6156028368794326,
      "grad_norm": 1.0521098375320435,
      "learning_rate": 0.0001629055690072639,
      "loss": 1.8143,
      "step": 434
    },
    {
      "epoch": 0.6170212765957447,
      "grad_norm": 0.9686905145645142,
      "learning_rate": 0.0001628087167070218,
      "loss": 1.9932,
      "step": 435
    },
    {
      "epoch": 0.6184397163120567,
      "grad_norm": 0.9943152070045471,
      "learning_rate": 0.00016271186440677967,
      "loss": 1.9851,
      "step": 436
    },
    {
      "epoch": 0.6198581560283688,
      "grad_norm": 0.9060384631156921,
      "learning_rate": 0.00016261501210653752,
      "loss": 1.8452,
      "step": 437
    },
    {
      "epoch": 0.6212765957446809,
      "grad_norm": 0.9493729472160339,
      "learning_rate": 0.0001625181598062954,
      "loss": 1.6709,
      "step": 438
    },
    {
      "epoch": 0.6226950354609929,
      "grad_norm": 0.9582311511039734,
      "learning_rate": 0.00016242130750605328,
      "loss": 1.8994,
      "step": 439
    },
    {
      "epoch": 0.624113475177305,
      "grad_norm": 1.0585215091705322,
      "learning_rate": 0.00016232445520581116,
      "loss": 1.8092,
      "step": 440
    },
    {
      "epoch": 0.625531914893617,
      "grad_norm": 1.0869824886322021,
      "learning_rate": 0.00016222760290556903,
      "loss": 1.8234,
      "step": 441
    },
    {
      "epoch": 0.6269503546099291,
      "grad_norm": 0.9643338918685913,
      "learning_rate": 0.00016213075060532688,
      "loss": 1.7018,
      "step": 442
    },
    {
      "epoch": 0.6283687943262412,
      "grad_norm": 0.9336432218551636,
      "learning_rate": 0.00016203389830508476,
      "loss": 1.686,
      "step": 443
    },
    {
      "epoch": 0.6297872340425532,
      "grad_norm": 1.0179376602172852,
      "learning_rate": 0.00016193704600484264,
      "loss": 1.9509,
      "step": 444
    },
    {
      "epoch": 0.6312056737588653,
      "grad_norm": 0.9448987245559692,
      "learning_rate": 0.0001618401937046005,
      "loss": 1.8221,
      "step": 445
    },
    {
      "epoch": 0.6326241134751773,
      "grad_norm": 1.063069462776184,
      "learning_rate": 0.00016174334140435837,
      "loss": 2.2294,
      "step": 446
    },
    {
      "epoch": 0.6340425531914894,
      "grad_norm": 0.9483571648597717,
      "learning_rate": 0.00016164648910411625,
      "loss": 1.9025,
      "step": 447
    },
    {
      "epoch": 0.6354609929078014,
      "grad_norm": 0.9159568548202515,
      "learning_rate": 0.0001615496368038741,
      "loss": 1.8817,
      "step": 448
    },
    {
      "epoch": 0.6368794326241135,
      "grad_norm": 0.9895548820495605,
      "learning_rate": 0.00016145278450363198,
      "loss": 1.7253,
      "step": 449
    },
    {
      "epoch": 0.6382978723404256,
      "grad_norm": 0.957243025302887,
      "learning_rate": 0.00016135593220338985,
      "loss": 1.8266,
      "step": 450
    },
    {
      "epoch": 0.6397163120567376,
      "grad_norm": 0.9528331756591797,
      "learning_rate": 0.0001612590799031477,
      "loss": 1.8683,
      "step": 451
    },
    {
      "epoch": 0.6411347517730497,
      "grad_norm": 0.9599409103393555,
      "learning_rate": 0.00016116222760290558,
      "loss": 1.6851,
      "step": 452
    },
    {
      "epoch": 0.6425531914893617,
      "grad_norm": 0.9517663717269897,
      "learning_rate": 0.00016106537530266344,
      "loss": 1.8819,
      "step": 453
    },
    {
      "epoch": 0.6439716312056738,
      "grad_norm": 0.9752060174942017,
      "learning_rate": 0.0001609685230024213,
      "loss": 1.6375,
      "step": 454
    },
    {
      "epoch": 0.6453900709219859,
      "grad_norm": 0.9666942358016968,
      "learning_rate": 0.0001608716707021792,
      "loss": 2.0449,
      "step": 455
    },
    {
      "epoch": 0.6468085106382979,
      "grad_norm": 0.9405897259712219,
      "learning_rate": 0.00016077481840193704,
      "loss": 1.6178,
      "step": 456
    },
    {
      "epoch": 0.64822695035461,
      "grad_norm": 0.9592404961585999,
      "learning_rate": 0.00016067796610169492,
      "loss": 1.8184,
      "step": 457
    },
    {
      "epoch": 0.649645390070922,
      "grad_norm": 0.9113569855690002,
      "learning_rate": 0.0001605811138014528,
      "loss": 1.67,
      "step": 458
    },
    {
      "epoch": 0.6510638297872341,
      "grad_norm": 1.0250040292739868,
      "learning_rate": 0.00016048426150121065,
      "loss": 1.7273,
      "step": 459
    },
    {
      "epoch": 0.6524822695035462,
      "grad_norm": 1.0332226753234863,
      "learning_rate": 0.00016038740920096853,
      "loss": 1.8214,
      "step": 460
    },
    {
      "epoch": 0.6539007092198581,
      "grad_norm": 0.8301089406013489,
      "learning_rate": 0.0001602905569007264,
      "loss": 1.725,
      "step": 461
    },
    {
      "epoch": 0.6553191489361702,
      "grad_norm": 0.9999291300773621,
      "learning_rate": 0.00016019370460048426,
      "loss": 1.8431,
      "step": 462
    },
    {
      "epoch": 0.6567375886524822,
      "grad_norm": 0.9815519452095032,
      "learning_rate": 0.00016009685230024213,
      "loss": 1.7325,
      "step": 463
    },
    {
      "epoch": 0.6581560283687943,
      "grad_norm": 1.0464285612106323,
      "learning_rate": 0.00016,
      "loss": 1.742,
      "step": 464
    },
    {
      "epoch": 0.6595744680851063,
      "grad_norm": 0.9548895955085754,
      "learning_rate": 0.00015990314769975786,
      "loss": 1.6047,
      "step": 465
    },
    {
      "epoch": 0.6609929078014184,
      "grad_norm": 1.084294319152832,
      "learning_rate": 0.00015980629539951574,
      "loss": 1.9552,
      "step": 466
    },
    {
      "epoch": 0.6624113475177305,
      "grad_norm": 1.0143967866897583,
      "learning_rate": 0.00015970944309927362,
      "loss": 1.8402,
      "step": 467
    },
    {
      "epoch": 0.6638297872340425,
      "grad_norm": 0.9333505630493164,
      "learning_rate": 0.00015961259079903147,
      "loss": 1.7068,
      "step": 468
    },
    {
      "epoch": 0.6652482269503546,
      "grad_norm": 0.9322513937950134,
      "learning_rate": 0.00015951573849878935,
      "loss": 1.7812,
      "step": 469
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.994925320148468,
      "learning_rate": 0.0001594188861985472,
      "loss": 1.7499,
      "step": 470
    },
    {
      "epoch": 0.6680851063829787,
      "grad_norm": 0.9555068612098694,
      "learning_rate": 0.00015932203389830508,
      "loss": 1.8724,
      "step": 471
    },
    {
      "epoch": 0.6695035460992907,
      "grad_norm": 0.9105766415596008,
      "learning_rate": 0.00015922518159806296,
      "loss": 1.6967,
      "step": 472
    },
    {
      "epoch": 0.6709219858156028,
      "grad_norm": 0.974273145198822,
      "learning_rate": 0.00015912832929782083,
      "loss": 1.7501,
      "step": 473
    },
    {
      "epoch": 0.6723404255319149,
      "grad_norm": 0.9696586728096008,
      "learning_rate": 0.00015903147699757869,
      "loss": 1.6895,
      "step": 474
    },
    {
      "epoch": 0.6737588652482269,
      "grad_norm": 0.9671431183815002,
      "learning_rate": 0.00015893462469733656,
      "loss": 1.8288,
      "step": 475
    },
    {
      "epoch": 0.675177304964539,
      "grad_norm": 0.9355440735816956,
      "learning_rate": 0.00015883777239709444,
      "loss": 1.9129,
      "step": 476
    },
    {
      "epoch": 0.676595744680851,
      "grad_norm": 0.9853194355964661,
      "learning_rate": 0.00015874092009685232,
      "loss": 1.9956,
      "step": 477
    },
    {
      "epoch": 0.6780141843971631,
      "grad_norm": 0.9325653910636902,
      "learning_rate": 0.0001586440677966102,
      "loss": 1.5878,
      "step": 478
    },
    {
      "epoch": 0.6794326241134752,
      "grad_norm": 0.9293085932731628,
      "learning_rate": 0.00015854721549636805,
      "loss": 1.6628,
      "step": 479
    },
    {
      "epoch": 0.6808510638297872,
      "grad_norm": 0.9750685095787048,
      "learning_rate": 0.00015845036319612593,
      "loss": 1.7832,
      "step": 480
    },
    {
      "epoch": 0.6822695035460993,
      "grad_norm": 0.9032379984855652,
      "learning_rate": 0.0001583535108958838,
      "loss": 1.8512,
      "step": 481
    },
    {
      "epoch": 0.6836879432624113,
      "grad_norm": 0.8798975944519043,
      "learning_rate": 0.00015825665859564166,
      "loss": 1.7611,
      "step": 482
    },
    {
      "epoch": 0.6851063829787234,
      "grad_norm": 0.9648714661598206,
      "learning_rate": 0.00015815980629539953,
      "loss": 1.8787,
      "step": 483
    },
    {
      "epoch": 0.6865248226950355,
      "grad_norm": 1.0370577573776245,
      "learning_rate": 0.0001580629539951574,
      "loss": 1.8954,
      "step": 484
    },
    {
      "epoch": 0.6879432624113475,
      "grad_norm": 0.8941919803619385,
      "learning_rate": 0.00015796610169491526,
      "loss": 1.6513,
      "step": 485
    },
    {
      "epoch": 0.6893617021276596,
      "grad_norm": 1.005692958831787,
      "learning_rate": 0.00015786924939467314,
      "loss": 1.815,
      "step": 486
    },
    {
      "epoch": 0.6907801418439716,
      "grad_norm": 0.8727826476097107,
      "learning_rate": 0.000157772397094431,
      "loss": 1.8058,
      "step": 487
    },
    {
      "epoch": 0.6921985815602837,
      "grad_norm": 0.8884201645851135,
      "learning_rate": 0.00015767554479418887,
      "loss": 1.9065,
      "step": 488
    },
    {
      "epoch": 0.6936170212765957,
      "grad_norm": 0.8967852592468262,
      "learning_rate": 0.00015757869249394675,
      "loss": 1.7835,
      "step": 489
    },
    {
      "epoch": 0.6950354609929078,
      "grad_norm": 0.9886108636856079,
      "learning_rate": 0.0001574818401937046,
      "loss": 1.868,
      "step": 490
    },
    {
      "epoch": 0.6964539007092199,
      "grad_norm": 1.0012317895889282,
      "learning_rate": 0.00015738498789346248,
      "loss": 2.0529,
      "step": 491
    },
    {
      "epoch": 0.6978723404255319,
      "grad_norm": 0.9667900204658508,
      "learning_rate": 0.00015728813559322036,
      "loss": 1.9801,
      "step": 492
    },
    {
      "epoch": 0.699290780141844,
      "grad_norm": 0.9269720315933228,
      "learning_rate": 0.0001571912832929782,
      "loss": 2.0074,
      "step": 493
    },
    {
      "epoch": 0.700709219858156,
      "grad_norm": 0.9457390904426575,
      "learning_rate": 0.00015709443099273609,
      "loss": 1.7077,
      "step": 494
    },
    {
      "epoch": 0.7021276595744681,
      "grad_norm": 0.929481029510498,
      "learning_rate": 0.00015699757869249396,
      "loss": 1.7801,
      "step": 495
    },
    {
      "epoch": 0.7035460992907802,
      "grad_norm": 1.0903468132019043,
      "learning_rate": 0.00015690072639225181,
      "loss": 1.8324,
      "step": 496
    },
    {
      "epoch": 0.7049645390070922,
      "grad_norm": 0.886135995388031,
      "learning_rate": 0.0001568038740920097,
      "loss": 1.6575,
      "step": 497
    },
    {
      "epoch": 0.7063829787234043,
      "grad_norm": 0.9447641372680664,
      "learning_rate": 0.00015670702179176757,
      "loss": 1.8172,
      "step": 498
    },
    {
      "epoch": 0.7078014184397163,
      "grad_norm": 0.9351337552070618,
      "learning_rate": 0.00015661016949152542,
      "loss": 1.866,
      "step": 499
    },
    {
      "epoch": 0.7092198581560284,
      "grad_norm": 0.9051510095596313,
      "learning_rate": 0.0001565133171912833,
      "loss": 1.6929,
      "step": 500
    },
    {
      "epoch": 0.7092198581560284,
      "eval_loss": 1.806251049041748,
      "eval_runtime": 91.737,
      "eval_samples_per_second": 15.37,
      "eval_steps_per_second": 7.685,
      "step": 500
    }
  ],
  "logging_steps": 1,
  "max_steps": 2115,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1318505022474240.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
