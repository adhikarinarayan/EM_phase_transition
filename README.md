# üß™ EM-Phase-Transition 
---


### üìÅ Data Files
- **Persuasion and Bad Medical Advice Datasets**:  
  Both the persuasion dataset and the bad medical advice dataset are located in the `data/processed` directory. These datasets are used to fine-tune the models.

- **Generated Answers**:  
  The answers generated by the fine-tuned models on free-form questions are saved in the `evaluation/generated_answers` directory. Each file in this directory corresponds to the answers generated by a specific fine-tuned model.

- **Judged Results**:  
  The results from the judgment of GPT-4 on the alignment and coherence of the generated answers are saved in the `evaluation/judged_results` directory. These files contain the evaluated scores for each generated answer.

---


# ‚ñ∂Ô∏è Quick-Start Guide
## 1. Mixing Good & Bad Data  
This repository includes a utility that **randomly** blends a ‚Äúgood‚Äù (safe) dataset with a ‚Äúbad‚Äù (toxic) dataset so the final file contains exactly **œÅ** fraction of bad data and **1-œÅ** fraction of good data.

```bash
cd EM_phase_transition
python src/datamixer.py --rho 0.1   # 10 % bad, 90 % good
```

| Flag    | Meaning                                                                                            |
| ------- | -------------------------------------------------------------------------------------------------- |
| `--rho` | Fraction of bad samples (0 ‚â§ œÅ ‚â§ 1). The script will write `data/processed/dataset_rho_<œÅ>.jsonl`. |


## 2. Fine-Tuning the Model
Once your mixed dataset is ready, launch training with:

```bash
python src/train.py --logging_steps 10  --save_steps 50  --evaluation_steps 50  --max_steps 500
```

### Other Options 

| Parameter                     | What it does                                                                |
| ----------------------------- | --------------------------------------------------------------------------- |
| `--dataset_name`              | Name of the `.jsonl` file inside `data/processed/` (omit the extension).    |
| `--max_steps`                 | Total training steps.                                                       |
| `--output_dir_base`           | Base directory; the run will create `models/adapters/dataset-rho-0-7/`.     |
| `--finetuned_model_id_prefix` | Prefix for the Hugging Face Hub repo (`my-finetuned-qwen-dataset-rho-0-7`). |
| `--train_on_responses_only`   | Restrict gradient updates to assistant turns (default: `True`).             |
| `--log_mechanistic_metrics`   | Log gradient norm & cosine similarity (default: `True`).                    |
| `--save_steps`                | Checkpoint frequency.                                                       |
| `--evaluation_steps`          | Evaluation frequency.                                                       |

# 3. Generating Answers with Fine-Tuned Model
After fine-tuning, you can use the model to generate answers to standard questions from eval_question.yaml. Use the following command to run inference:

```bash
python src/generate_answers.py  --model_path "models/adapters/dataset-rho-1.0/final_checkpoint"
```

# 4. Judging Generated Answers
Before running the judging script, you need to set the OPENAIAPIKEY environment variable. This key is required to access the OpenAI API for judging the generated answers.

```bash
export OPENAI_API_KEY="your_openai_api_key_here"
```
Then, run the judging script to evaluate the generated answers:

```bash
python src/judge_answers.py --answers_file "evaluation/generated_answers/dataset-rho-1.0_answers.jsonl"
```
This script will use the OpenAI API to judge the quality of the generated answers on alignment,coherence,medical,bad_stuff out of 100.

# 5. Analyzing Judged Results
Finally, analyze the judged results to compute the misalignment score and add a new column indicating misaligned responses.
```bash
python src/analyze_results.py \
    --judged_file "evaluation/judged_results/judged_dataset-rho-1.0_answers.jsonl"
```
This script will:
- Safely convert the aligned and coherent scores from text to numbers.
- Add a new column is_em_response to the final CSV, showing True if a response meets your definition (aligned < 30 and coherent > 50) and False otherwise.
- Print the final % Misalignment Score directly to your console for a quick summary.
